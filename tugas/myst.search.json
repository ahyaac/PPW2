{"version":"1","records":[{"hierarchy":{"lvl1":"Crawling Data X"},"type":"lvl1","url":"/crawldatax","position":0},{"hierarchy":{"lvl1":"Crawling Data X"},"content":"#@title Twitter Auth Token\n\ntwitter_auth_token = '963ab8acd52425ac0a41fb296d7f04bc1f68b521' # change this auth token\n\n\n\n# Import required Python package\n!pip install pandas\n\n# Install Node.js (because tweet-harvest built using Node.js)\n!sudo apt-get update\n!sudo apt-get install -y ca-certificates curl gnupg\n!sudo mkdir -p /etc/apt/keyrings\n!curl -fsSL https://deb.nodesource.com/gpgkey/nodesource-repo.gpg.key | sudo gpg --dearmor -o /etc/apt/keyrings/nodesource.gpg\n\n!NODE_MAJOR=20 && echo \"deb [signed-by=/etc/apt/keyrings/nodesource.gpg] https://deb.nodesource.com/node_$NODE_MAJOR.x nodistro main\" | sudo tee /etc/apt/sources.list.d/nodesource.list\n\n!sudo apt-get update\n!sudo apt-get install nodejs -y\n\n!node -v\n\n\n\n\n\n\n\n!npx playwright install\n\n\n\n\n\n\n\n\n\n\n# Crawl Data\n\nfilename = 'banjir.csv'\nsearch_keyword = 'banjir'\nlimit = 500\n\n!npx -y tweet-harvest@2.6.1 -o \"{filename}\" -s \"{search_keyword}\" --tab \"LATEST\" -l {limit} --token {twitter_auth_token}\n\n\n\nimport pandas as pd\n\n# Specify the path to your CSV file\nfile_path = f\"tweets-data/{filename}\"\n\n# Read the CSV file into a pandas DataFrame\ndf = pd.read_csv(file_path, delimiter=\",\")\n\n# Display the DataFrame\ndisplay(df)\n\n\n\n# Cek jumlah data yang didapatkan\n\nnum_tweets = len(df)\nprint(f\"Jumlah tweet dalam dataframe adalah {num_tweets}.\")\n\n","type":"content","url":"/crawldatax","position":1},{"hierarchy":{"lvl1":"Crawling Penambangan Web"},"type":"lvl1","url":"/crawlingppw","position":0},{"hierarchy":{"lvl1":"Crawling Penambangan Web"},"content":"pip install sprynger\n\n\n\nimport requests\n# Silahkan membuat api key dari https://dev.springernature.com/#api\napi_key = \"6812f052d58306c4316bac37cf837912\"\nkeyword = \"web mining\"\n\nurl = \"https://api.springernature.com/meta/v2/json\"\nparams = {\n    \"q\": f\"keyword:{keyword}\",\n    \"api_key\": api_key,\n    \"p\": 10\n}\n\nresponse = requests.get(url, params=params)\n\nif response.status_code == 200:\n    data = response.json()\n    print(f\"Total hasil: {data['result'][0]['total']}\\n\")\n    for record in data['records']:\n        doi = record.get('doi', 'N/A')\n        title = record.get('title', 'No title')\n        abstract = record.get('abstract', 'No abstract')\n        print(f\"DOI: {doi}\")\n        print(f\"Title: {title}\")\n        print(f\"Abstract: {abstract}\\n\")\nelse:\n    print(\"Error:\", response.status_code, response.text)\n\n\n\nimport requests\nimport csv\n\n# Silahkan membuat api key dari https://dev.springernature.com/#api\napi_key = \"6812f052d58306c4316bac37cf837912\"\nkeyword = \"web mining\"\n\nurl = \"https://api.springernature.com/meta/v2/json\"\nparams = {\n    \"q\": f\"keyword:{keyword}\",\n    \"api_key\": api_key,\n    \"p\": 10   # jumlah hasil per halaman\n}\n\nresponse = requests.get(url, params=params)\n\nif response.status_code == 200:\n    data = response.json()\n    total = data['result'][0]['total']\n    print(f\"Total hasil: {total}\\n\")\n\n    # Simpan ke CSV\n    with open(\"springer_results.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n        writer = csv.writer(file)\n        # Header kolom\n        writer.writerow([\"DOI\", \"Title\", \"Abstract\"])\n\n        # Data hasil crawling\n        for record in data['records']:\n            doi = record.get('doi', 'N/A')\n            title = record.get('title', 'No title')\n            abstract = record.get('abstract', 'No abstract')\n\n            # Tulis ke file CSV\n            writer.writerow([doi, title, abstract])\n\n            # Print ke console juga\n            print(f\"DOI: {doi}\")\n            print(f\"Title: {title}\")\n            print(f\"Abstract: {abstract}\\n\")\n\n    print(\"âœ… Data berhasil disimpan ke springer_results.csv\")\n\nelse:\n    print(\"Error:\", response.status_code, response.text)\n\n\n\nimport requests\n# Silahkan membuat api key dari https://dev.springernature.com/#api\napi_key = \"6812f052d58306c4316bac37cf837912\"\nkeyword = \"web usage mining\"\n\nurl = \"https://api.springernature.com/meta/v2/json\"\nparams = {\n    \"q\": f\"keyword:{keyword}\",\n    \"api_key\": api_key,\n    \"p\": 10\n}\n\nresponse = requests.get(url, params=params)\n\nif response.status_code == 200:\n    data = response.json()\n    print(f\"Total hasil: {data['result'][0]['total']}\\n\")\n    for record in data['records']:\n        doi = record.get('doi', 'N/A')\n        title = record.get('title', 'No title')\n        abstract = record.get('abstract', 'No abstract')\n        print(f\"DOI: {doi}\")\n        print(f\"Title: {title}\")\n        print(f\"Abstract: {abstract}\\n\")\nelse:\n    print(\"Error:\", response.status_code, response.text)\n\n\n\nimport requests\nimport csv\n\n# Silahkan membuat api key dari https://dev.springernature.com/#api\napi_key = \"6812f052d58306c4316bac37cf837912\"\nkeyword = \"web usage mining\"\n\nurl = \"https://api.springernature.com/meta/v2/json\"\nparams = {\n    \"q\": f\"keyword:{keyword}\",\n    \"api_key\": api_key,\n    \"p\": 10   # jumlah hasil per halaman\n}\n\nresponse = requests.get(url, params=params)\n\nif response.status_code == 200:\n    data = response.json()\n    total = data['result'][0]['total']\n    print(f\"Total hasil: {total}\\n\")\n\n    # Simpan ke CSV\n    with open(\"springer_results_web_usage_mining.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n        writer = csv.writer(file)\n        # Header kolom\n        writer.writerow([\"DOI\", \"Title\", \"Abstract\"])\n\n        # Data hasil crawling\n        for record in data['records']:\n            doi = record.get('doi', 'N/A')\n            title = record.get('title', 'No title')\n            abstract = record.get('abstract', 'No abstract')\n\n            # Tulis ke file CSV\n            writer.writerow([doi, title, abstract])\n\n            # Print ke console juga\n            print(f\"DOI: {doi}\")\n            print(f\"Title: {title}\")\n            print(f\"Abstract: {abstract}\\n\")\n\n    print(\" Data berhasil disimpan ke springer_results_web_usage_mining.csv\")\n\nelse:\n    print(\"Error:\", response.status_code, response.text)\n\n","type":"content","url":"/crawlingppw","position":1},{"hierarchy":{"lvl1":"Crawling PTA, Berita, dan Link Page Output"},"type":"lvl1","url":"/crawlingpta-berita","position":0},{"hierarchy":{"lvl1":"Crawling PTA, Berita, dan Link Page Output"},"content":"","type":"content","url":"/crawlingpta-berita","position":1},{"hierarchy":{"lvl1":"Crawling PTA, Berita, dan Link Page Output","lvl2":"1. Crawling pta.trunojoyo.ac.id"},"type":"lvl2","url":"/crawlingpta-berita#id-1-crawling-pta-trunojoyo-ac-id","position":2},{"hierarchy":{"lvl1":"Crawling PTA, Berita, dan Link Page Output","lvl2":"1. Crawling pta.trunojoyo.ac.id"},"content":"\n\nLibrary\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport re, sys, time\n\n\n\nBase url\n\nBase_Url = \"https://pta.trunojoyo.ac.id/c_search/byprod\"\n\n\n\nFunction\n\ndef get_max_page(prodi_id):\n    url = f\"{Base_Url}/{prodi_id}/1\"\n    r = requests.get(url)\n    soup = BeautifulSoup(r.content, \"html.parser\")\n\n    # Cari tombol >> (last page)\n    last_page = soup.select_one('ol.pagination a:contains(\"Â»\")')\n    if last_page and \"href\" in last_page.attrs:\n        href = last_page[\"href\"]\n        # Pecah URL -> ambil angka terakhir\n        max_page = int(href.split(\"/\")[-1])\n        return max_page\n\n    # fallback kalau pagination tidak ada\n    return 1\n\n\n\nprint(get_max_page(10))\n\n\n\ndef print_progress(prodi_id, prodi, current_page, total_pages):\n    percent = (current_page / total_pages) * 100\n    bar_length = 20\n    filled_length = int(bar_length * current_page // total_pages)\n    bar = 'â–ˆ' * filled_length + '-' * (bar_length - filled_length)\n    sys.stdout.write(f'\\r[{prodi_id}] {prodi} - Page {current_page}/{total_pages} [{bar}] {percent:.2f}%')\n    sys.stdout.flush()\n    if current_page == total_pages:\n        sys.stdout.write('\\n')\n\n\n\nFunction All Data\n\ndef pta_all():\n    start_time = time.time()\n\n    data = {\n        \"id\": [],\n        \"penulis\": [],\n        \"judul\": [],\n        \"abstrak_id\": [],\n        \"abstrak_en\": [],\n        \"pembimbing_pertama\": [],\n        \"pembimbing_kedua\": [],\n        \"prodi\": []\n    }\n\n    total_prodi = 1\n    total_pages = 0\n    max_pages_dict = {}\n\n    # hitung total halaman (untuk tiap prodi)\n    for i in range(1, total_prodi + 1):\n        max_page = get_max_page(i)\n        max_pages_dict[i] = max_page\n        total_pages += max_page\n\n    for i in range(1, total_prodi + 1):\n        max_page = max_pages_dict[i]\n        for j in range(1, max_page + 1):\n            url = f\"{Base_Url}/{i}/{j}\"\n            r = requests.get(url)\n            soup = BeautifulSoup(r.content, \"html.parser\")\n            jurnals = soup.select('li[data-cat=\"#luxury\"]')\n\n            isii = soup.select_one('div#begin')\n            if not isii:\n                continue\n            prodi_full = isii.select_one('h2').text.strip()\n            prodi = prodi_full.replace(\"Journal Jurusan \", \"\")\n\n            for jurnal in jurnals:\n                link_keluar = jurnal.select_one('a.gray.button')['href']\n\n                # ambil ID dari link PTA (angka terakhir di URL)\n                id_match = re.search(r\"/detail/(\\d+)\", link_keluar)\n                pta_id = id_match.group(1) if id_match else None\n\n                response = requests.get(link_keluar)\n                soup1 = BeautifulSoup(response.content, \"html.parser\")\n                isi = soup1.select_one('div#content_journal')\n\n                judul = isi.select_one('a.title').text.strip()\n                penulis = isi.select_one('span:contains(\"Penulis\")').text.split(' : ')[1]\n                pembimbing_pertama = isi.select_one('span:contains(\"Dosen Pembimbing I\")').text.split(' : ')[1]\n                pembimbing_kedua = isi.select_one('span:contains(\"Dosen Pembimbing II\")').text.split(' :')[1]\n\n                paragraf = isi.select('p[align=\"justify\"]')\n                abstrak_id = paragraf[0].get_text(strip=True) if len(paragraf) > 0 else \"N/A\"\n                abstrak_en = paragraf[1].get_text(strip=True) if len(paragraf) > 1 else \"N/A\"\n\n                data[\"id\"].append(pta_id)\n                data[\"penulis\"].append(penulis)\n                data[\"judul\"].append(judul)\n                data[\"abstrak_id\"].append(abstrak_id)\n                data[\"abstrak_en\"].append(abstrak_en)\n                data[\"pembimbing_pertama\"].append(pembimbing_pertama)\n                data[\"pembimbing_kedua\"].append(pembimbing_kedua)\n                data[\"prodi\"].append(prodi)\n\n            # update progress bar per prodi\n            print_progress(i, prodi, j, max_page)\n\n        sys.stdout.write(\"\\n\")  # pindah baris setelah 1 prodi selesai\n\n    # simpan ke CSV\n    df = pd.DataFrame(data)\n    df.to_csv(\"pta_all.csv\", index=False, encoding=\"utf-8-sig\")\n\n    # hitung durasi\n    end_time = time.time()\n    elapsed = int(end_time - start_time)\n    jam, sisa = divmod(elapsed, 3600)\n    menit, detik = divmod(sisa, 60)\n\n    # summary\n    print(\"\\nâœ… Seluruh data berhasil dikumpulkan!\")\n    print(f\"ðŸ“ˆ Total entri: {len(df)}\")\n    print(f\"â±ï¸ Waktu eksekusi: {jam} jam {menit} menit {detik} detik\")\n\n    return df\n\n\n\npta_all()\n\n\n\n\n\n\n\nFunction All Data 5 pages\n\ndef print_progress(prodi_id, prodi, current_page, total_pages):\n    percent = (current_page / total_pages) * 100\n    bar_length = 20\n    filled_length = int(bar_length * current_page // total_pages)\n    bar = 'â–ˆ' * filled_length + '-' * (bar_length - filled_length)\n    sys.stdout.write(f'\\r[{prodi_id}] {prodi} - Page {current_page}/{total_pages} [{bar}] {percent:.2f}%')\n    sys.stdout.flush()\n    if current_page == total_pages:\n        sys.stdout.write('\\n\\n')\n\ndef pta():\n    start_time = time.time()  # mulai hitung waktu\n\n    data = {\n        \"id\": [],\n        \"penulis\": [],\n        \"judul\": [],\n        \"abstrak id\": [],\n        \"abstrak en\": [],\n        \"pembimbing_pertama\": [],\n        \"pembimbing_kedua\": [],\n        \"prodi\": [],\n    }\n\n    for i in range(1, 42):  # jumlah prodi\n        total_pages = 5  # jumlah page\n        prodi_name = None\n\n        for j in range(1, total_pages + 1):  # loop page\n            url = f\"https://pta.trunojoyo.ac.id/c_search/byprod/{i}/{j}\"\n            r = requests.get(url)\n            soup = BeautifulSoup(r.content, \"html.parser\")\n            jurnals = soup.select('li[data-cat=\"#luxury\"]')\n\n            isii = soup.select_one('div#begin')\n            if not isii:\n                continue\n            prodi_full = isii.select_one('h2').text.strip()\n            prodi = prodi_full.replace(\"Journal Jurusan \", \"\")\n            if not prodi_name:\n                prodi_name = prodi\n\n            for jurnal in jurnals:\n                link = jurnal.select_one('a.gray.button')['href']\n\n                # ambil ID dari link PTA\n                id_match = re.search(r\"/detail/(\\d+)\", link)\n                pta_id = id_match.group(1) if id_match else None\n\n                response = requests.get(link)\n                soup1 = BeautifulSoup(response.content, \"html.parser\")\n                isi = soup1.select_one('div#content_journal')\n\n                # Judul\n                judul = isi.select_one('a.title').text\n\n                # Penulis\n                penulis = isi.select_one('span:contains(\"Penulis\")').text.split(' : ')[1]\n\n                # Pembimbing Pertama\n                pembimbing_pertama = isi.select_one('span:contains(\"Dosen Pembimbing I\")').text.split(' : ')[1]\n\n                # Pembimbing Kedua\n                pembimbing_kedua = isi.select_one('span:contains(\"Dosen Pembimbing II\")').text.split(' :')[1]\n\n                # Abstrak\n                paragraf = isi.select('p[align=\"justify\"]')\n                abstrak = paragraf[0].get_text(strip=True) if len(paragraf) > 0 else \"N/A\"\n                abstract = paragraf[1].get_text(strip=True) if len(paragraf) > 1 else \"N/A\"\n\n                # simpan data\n                data[\"id\"].append(pta_id)\n                data[\"penulis\"].append(penulis)\n                data[\"judul\"].append(judul)\n                data[\"pembimbing_pertama\"].append(pembimbing_pertama)\n                data[\"pembimbing_kedua\"].append(pembimbing_kedua)\n                data[\"abstrak id\"].append(abstrak)\n                data[\"abstrak en\"].append(abstract)\n                data[\"prodi\"].append(prodi)\n\n            # update progress bar\n            print_progress(i, prodi_name, j, total_pages)\n\n    df = pd.DataFrame(data)\n    df.to_csv(\"pta.csv\", index=False, encoding=\"utf-8-sig\")\n\n    end_time = time.time()\n    elapsed = int(end_time - start_time)\n    jam, sisa = divmod(elapsed, 3600)\n    menit, detik = divmod(sisa, 60)\n\n    # summary\n    print(\"\\nâœ… Seluruh data berhasil dikumpulkan!\")\n    print(f\"ðŸ“ˆ Total entri: {len(df)}\")\n    print(f\"â±ï¸ Waktu eksekusi: {jam} jam {menit} menit {detik} detik\")\n\n    return df\n\n\n\npta()\n\n\n\n\n\nPage & Output Link PTA\n\ndef print_progress(prodi_id, prodi, current_page, total_pages):\n    percent = (current_page / total_pages) * 100\n    bar_length = 20\n    filled_length = int(bar_length * current_page // total_pages)\n    bar = 'â–ˆ' * filled_length + '-' * (bar_length - filled_length)\n    sys.stdout.write(f'\\r[{prodi_id}] {prodi} - Page {current_page}/{total_pages} [{bar}] {percent:.2f}%')\n    sys.stdout.flush()\n    if current_page == total_pages:\n        sys.stdout.write('\\n\\n')\n\ndef pta_links():\n    start_time = time.time()  # mulai hitung waktu\n\n    data = {\n        \"no\": [],\n        \"page\": [],\n        \"link_keluar\": []\n    }\n\n    no = 1  # nomor urut\n\n    for i in range(1, 42):  # jumlah prodi\n        total_pages = 5  # jumlah page\n        prodi_name = None\n\n        for j in range(1, total_pages + 1):  # loop page\n            url = f\"https://pta.trunojoyo.ac.id/c_search/byprod/{i}/{j}\"\n            r = requests.get(url)\n            soup = BeautifulSoup(r.content, \"html.parser\")\n            jurnals = soup.select('li[data-cat=\"#luxury\"]')\n\n            isii = soup.select_one('div#begin')\n            if not isii:\n                continue\n            prodi_full = isii.select_one('h2').text.strip()\n            prodi = prodi_full.replace(\"Journal Jurusan \", \"\")\n            if not prodi_name:\n                prodi_name = prodi\n\n            for jurnal in jurnals:\n                link = jurnal.select_one('a.gray.button')['href']\n\n                data[\"no\"].append(no)\n                data[\"page\"].append(url)          # link page\n                data[\"link_keluar\"].append(link)  # link detail\n                no += 1\n\n            # update progress bar\n            print_progress(i, prodi_name, j, total_pages)\n\n    df = pd.DataFrame(data)\n    df.to_csv(\"pta_links.csv\", index=False)\n\n    end_time = time.time()\n    elapsed = int(end_time - start_time)\n    jam, sisa = divmod(elapsed, 3600)\n    menit, detik = divmod(sisa, 60)\n\n    # summary\n    print(\"\\nâœ… Seluruh link berhasil dikumpulkan!\")\n    print(f\"ðŸ“Š Total entri: {len(df)}\")\n    print(f\"â±ï¸ Waktu eksekusi: {jam} jam {menit} menit {detik} detik\")\n\n    return df\n\n\n\npta_links()\n\n\n\n\n\n","type":"content","url":"/crawlingpta-berita#id-1-crawling-pta-trunojoyo-ac-id","position":3},{"hierarchy":{"lvl1":"Crawling PTA, Berita, dan Link Page Output","lvl2":"2. Crawling Berita wwwâ€‹.cnnindonesiaâ€‹.com"},"type":"lvl2","url":"/crawlingpta-berita#id-2-crawling-berita-www-cnnindonesia-com","position":4},{"hierarchy":{"lvl1":"Crawling PTA, Berita, dan Link Page Output","lvl2":"2. Crawling Berita wwwâ€‹.cnnindonesiaâ€‹.com"},"content":"\n\nLibrary\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport time, re, sys\n\n\n\nFunction\n\n# Progress bar manual\ndef print_progress(current_page, total_pages):\n    percent = (current_page / total_pages) * 100\n    bar_length = 20\n    filled_length = int(bar_length * current_page // total_pages)\n    bar = 'â–ˆ' * filled_length + '-' * (bar_length - filled_length)\n    sys.stdout.write(f'\\rPage {current_page}/{total_pages} [{bar}] {percent:.2f}%')\n    sys.stdout.flush()\n    if current_page == total_pages:\n        sys.stdout.write('\\n\\n')\n\n\n\n# Ambil isi berita CNN\ndef get_article_content(url):\n    r = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n    soup = BeautifulSoup(r.text, \"html.parser\")\n\n    paragraphs = []\n    content_div = soup.find(\"div\", class_=\"detail-text\")\n    if content_div:\n        for p in content_div.find_all(\"p\"):\n            text = p.get_text(strip=True)\n            if text and not text.lower().startswith(\"baca juga\"):\n                paragraphs.append(text)\n    return \" \".join(paragraphs)\n\n\n\n# Scraping CNN Indonesia\ndef berita_cnn(pages=1):\n    start_time = time.time()\n\n    BASE_URL = \"https://www.cnnindonesia.com/indeks?page={}\"\n\n    data = {\n        \"id_berita\": [],\n        \"judul_berita\": [],\n        \"isi_berita\": [],\n        \"kategori_berita\": []\n    }\n\n    counter = 1  # mulai id dari 1\n\n    for page in range(1, pages+1):\n        url = BASE_URL.format(page)\n        r = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n        soup = BeautifulSoup(r.text, \"html.parser\")\n\n        articles = soup.select(\"article a\")\n        for a in articles:\n            link = a.get(\"href\")\n            title = a.get_text(strip=True)\n\n            if not link or not title:\n                continue\n            if not link.startswith(\"https://www.cnnindonesia.com/\"):\n                continue\n\n            # Ambil kategori dari URL\n            try:\n                kategori = link.split(\"/\")[3]\n            except:\n                kategori = \"unknown\"\n\n            try:\n                content = get_article_content(link)\n            except:\n                content = \"\"\n\n            data[\"id_berita\"].append(counter)\n            data[\"judul_berita\"].append(title)\n            data[\"isi_berita\"].append(content)\n            data[\"kategori_berita\"].append(kategori)\n\n            counter += 1  # naikkan id\n\n        print_progress(page, pages)\n\n    df = pd.DataFrame(data)\n    df.to_csv(\"cnnindonesia_berita4.csv\", index=False, encoding=\"utf-8-sig\")\n\n    end_time = time.time()\n    elapsed = int(end_time - start_time)\n    jam, sisa = divmod(elapsed, 3600)\n    menit, detik = divmod(sisa, 60)\n\n    print(\"\\nâœ… Seluruh data berhasil dikumpulkan!\")\n    print(f\"ðŸ“ˆ Total entri: {len(df)}\")\n    print(f\"â±ï¸ Waktu eksekusi: {jam} jam {menit} menit {detik} detik\")\n\n    return df\n\n# Contoh pemanggilan\ndf_cnn = berita_cnn(pages=100)\n\n\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport time, re, sys\n\n\n\nberita_path =\"/content/drive/MyDrive/PPW/output/cnnindonesia_berita4.csv\"\nberita = pd.read_csv(berita_path, index_col=\"id_berita\")\n\nberita\n\n\n\nPage & Link Output Berita\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport time, sys\n\n# progress bar\ndef print_progress(current_page, total_pages):\n    percent = (current_page / total_pages) * 100\n    bar_length = 20\n    filled_length = int(bar_length * current_page // total_pages)\n    bar = 'â–ˆ' * filled_length + '-' * (bar_length - filled_length)\n    sys.stdout.write(f'\\rPage {current_page}/{total_pages} [{bar}] {percent:.2f}%')\n    sys.stdout.flush()\n    if current_page == total_pages:\n        sys.stdout.write('\\n\\n')\n\n# fungsi untuk kumpulkan link berita CNN Indonesia\ndef berita_links(pages=1):\n    start_time = time.time()\n\n    BASE_URL = \"https://www.cnnindonesia.com/indeks?page={}\"\n\n    data = {\n        \"id_berita\": [],\n        \"page\": [],\n        \"link_keluar\": []\n    }\n\n    counter = 1\n    for page in range(1, pages+1):\n        url = BASE_URL.format(page)\n        r = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n        soup = BeautifulSoup(r.text, \"html.parser\")\n\n        articles = soup.select(\"article a\")\n        for a in articles:\n            link = a.get(\"href\")\n\n            if not link or not link.startswith(\"https://www.cnnindonesia.com/\"):\n                continue\n\n            data[\"id_berita\"].append(counter)\n            data[\"page\"].append(url)       # halaman indeks\n            data[\"link_keluar\"].append(link)  # link detail berita\n\n            counter += 1\n\n        print_progress(page, pages)\n\n    df = pd.DataFrame(data)\n    df.to_csv(\"cnnindonesia_links1.csv\", index=False, encoding=\"utf-8-sig\")\n\n    end_time = time.time()\n    elapsed = int(end_time - start_time)\n    jam, sisa = divmod(elapsed, 3600)\n    menit, detik = divmod(sisa, 60)\n\n    print(\"\\nâœ… Seluruh link berhasil dikumpulkan!\")\n    print(f\"ðŸ“ˆ Total entri: {len(df)}\")\n    print(f\"â±ï¸ Waktu eksekusi: {jam} jam {menit} menit {detik} detik\")\n\n    return df\n\n# contoh pemanggilan: ambil 100 halaman pertama\ndf_links = berita_links(pages=100)\n\n\n\n\nlink_path = \"/content/drive/MyDrive/PPW/output/cnnindonesia_links1.csv\"\nlink = pd.read_csv(link_path, index_col=\"id_berita\")\n\nlink\n\n","type":"content","url":"/crawlingpta-berita#id-2-crawling-berita-www-cnnindonesia-com","position":5},{"hierarchy":{"lvl1":"Crawling PTA Prodi Manajemen Abstrak Id."},"type":"lvl1","url":"/crawlingpta-manajemen","position":0},{"hierarchy":{"lvl1":"Crawling PTA Prodi Manajemen Abstrak Id."},"content":"","type":"content","url":"/crawlingpta-manajemen","position":1},{"hierarchy":{"lvl1":"Crawling PTA Prodi Manajemen Abstrak Id.","lvl2":"1. Crawling"},"type":"lvl2","url":"/crawlingpta-manajemen#id-1-crawling","position":2},{"hierarchy":{"lvl1":"Crawling PTA Prodi Manajemen Abstrak Id.","lvl2":"1. Crawling"},"content":"\n\nLibrary\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport re, sys, time\nfrom google.colab import drive\n\n\n\ndrive.mount('/content/drive')\n\n\n\nBase Url\n\nBase_Url = \"https://pta.trunojoyo.ac.id/c_search/byprod\"\n\n\n\nFunction\n\ndef get_max_page(prodi_id):\n    url = f\"{Base_Url}/{prodi_id}\"\n    r = requests.get(url)\n    soup = BeautifulSoup(r.content, \"html.parser\")\n\n    # cari tombol >> (last page)\n    last_page = soup.select_one('ol.pagination a:contains(\"Â»\")')\n    if last_page and \"href\" in last_page.attrs:\n        href = last_page[\"href\"]\n        # pecah URL -> ambil angka terakhir\n        max_page = int(href.split(\"/\")[-1])\n        return max_page\n\n    # fallback kalau pagination tidak ada\n    return 1\n\n\n\ndef print_progress(prodi_id, prodi, current_page, total_pages):\n    percent = (current_page / total_pages) * 100\n    bar_length = 20\n    filled_length = int(bar_length * current_page // total_pages)\n    bar = 'â–ˆ' * filled_length + '-' * (bar_length - filled_length)\n    sys.stdout.write(f'\\r[{prodi_id}] {prodi} - Page {current_page}/{total_pages} [{bar}] {percent:.2f}%')\n    sys.stdout.flush()\n    if current_page == total_pages:\n        sys.stdout.write('\\n')\n\n\n\nFunction All Data\n\ndef pta_manajemen():\n    start_time = time.time()\n    prodi_id = 7  # ID Prodi Manajemen\n    data = {\n        \"id\": [],\n        \"penulis\": [],\n        \"judul\": [],\n        \"abstrak_id\": [],\n        \"pembimbing_pertama\": [],\n        \"pembimbing_kedua\": [],\n        \"prodi\": [],\n    }\n\n    max_page = get_max_page(prodi_id)\n    print(f\"ðŸ” Total halaman Prodi Manajemen: {max_page}\")\n\n    for j in range(1, max_page + 1):\n        url = f\"{Base_Url}/{prodi_id}/{j}\"\n        r = requests.get(url)\n        soup = BeautifulSoup(r.content, \"html.parser\")\n        jurnals = soup.select('li[data-cat=\"#luxury\"]')\n\n        isii = soup.select_one('div#begin')\n        if not isii:\n            continue\n        prodi_full = isii.select_one('h2').text.strip()\n        prodi = prodi_full.replace(\"Journal Jurusan \", \"\")\n\n        for jurnal in jurnals:\n            link_keluar = jurnal.select_one('a.gray.button')['href']\n\n            # ambil ID dari link PTA\n            id_match = re.search(r\"/detail/(\\d+)\", link_keluar)\n            pta_id = id_match.group(1) if id_match else None\n            response = requests.get(link_keluar)\n            soup1 = BeautifulSoup(response.content, \"html.parser\")\n            isi = soup1.select_one('div#content_journal')\n\n            judul = isi.select_one('a.title').text.strip()\n\n            # Penulis\n            penulis = isi.select_one('span:contains(\"Penulis\")').text.split(' : ')[1]\n\n            # Pembimbing Pertama\n            pembimbing_pertama = isi.select_one('span:contains(\"Dosen Pembimbing I\")').text.split(' : ')[1]\n\n            # Pembimbing Kedua\n            pembimbing_kedua = isi.select_one('span:contains(\"Dosen Pembimbing II\")').text.split(' :')[1]\n\n            paragraf = isi.select('p[align=\"justify\"]')\n            abstrak_id = paragraf[0].get_text(strip=True) if len(paragraf) > 0 else \"N/A\"\n\n            data[\"id\"].append(pta_id)\n            data[\"penulis\"].append(penulis)\n            data[\"judul\"].append(judul)\n            data[\"pembimbing_pertama\"].append(pembimbing_pertama)\n            data[\"pembimbing_kedua\"].append(pembimbing_kedua)\n            data[\"abstrak_id\"].append(abstrak_id)\n            data[\"prodi\"].append(prodi)\n\n        # âœ… Perbaikan: panggil dengan 4 argumen\n        print_progress(prodi_id, prodi, j, max_page)\n\n    # Simpan ke CSV\n    df = pd.DataFrame(data)\n    output_path = \"/content/drive/MyDrive/PPW/output/pta_manajemen.csv\"\n    df.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n\n    # Hitung durasi\n    end_time = time.time()\n    elapsed = int(end_time - start_time)\n    jam, sisa = divmod(elapsed, 3600)\n    menit, detik = divmod(sisa, 60)\n\n    print(\"\\nâœ… Seluruh data Manajemen berhasil dikumpulkan!\")\n    print(f\"ðŸ“ˆ Total entri: {len(df)}\")\n    print(f\"â±ï¸ Waktu eksekusi: {jam} jam {menit} menit {detik} detik\")\n\n    return df\n\n\n\n# --- Eksekusi ---\nif __name__ == \"__main__\":\n    prodi_id = 7\n    pages = get_max_page(prodi_id)\n    confirm = input(\"Mau lanjut scraping semua data Manajemen? (y/n): \")\n    if confirm.lower() == \"y\":\n        df = pta_manajemen()\n    else:\n        print(\"âŒ Scraping dibatalkan.\")\n\n\n\nmanajemen_path = \"/content/drive/MyDrive/PPW/output/pta_manajemen.csv\"\nmanajemen = pd.read_csv(manajemen_path)\n\nmanajemen\n\n\n\nmanajemen.isna().sum().sum()\n\n\n\n# cari index (nomor baris) yang ada NaN\nnan_rows = manajemen[manajemen.isna().any(axis=1)].index\n\nprint(\"Baris yang mengandung NaN:\", nan_rows.tolist())\n\n\n\n\n","type":"content","url":"/crawlingpta-manajemen#id-1-crawling","position":3},{"hierarchy":{"lvl1":"Crawling PTA Prodi Manajemen Abstrak Id.","lvl2":"2. Pre-Processing"},"type":"lvl2","url":"/crawlingpta-manajemen#id-2-pre-processing","position":4},{"hierarchy":{"lvl1":"Crawling PTA Prodi Manajemen Abstrak Id.","lvl2":"2. Pre-Processing"},"content":"\n\n!pip install Sastrawi\n\n\n\nimport pandas as pd\nimport re\nfrom collections import Counter\nfrom Sastrawi.Stemmer.StemmerFactory import StemmerFactory\nfrom Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n\n\n\n##Sample Preprocessing\n\nsample_text = manajemen['abstrak_id'].dropna().iloc[0]\nprint(\"ðŸ”¹ TEKS ASLI:\\n\", sample_text, \"\\n\")\n\n\n\n# --- 1. Case Folding ---\ncasefolded = sample_text.lower()\nprint(\"ðŸ”¹ CASE FOLDING:\\n\", casefolded, \"\\n\")\n\n\n\n# --- 2. Cleaning (hapus angka, tanda baca, karakter khusus) ---\ncleaned = re.sub(r'[^a-zA-Z\\s]', '', casefolded)\nprint(\"ðŸ”¹ CLEANING:\\n\", cleaned, \"\\n\")\n\n\n\n# --- 3. Tokenisasi ---\ntokens = cleaned.split()\nprint(\"ðŸ”¹ TOKENISASI:\\n\", tokens, \"\\n\")\n\n\n\n# --- 4. Stopword Removal ---\nstop_factory = StopWordRemoverFactory()\nstopword = stop_factory.create_stop_word_remover()\nno_stop = stopword.remove(\" \".join(tokens))\nprint(\"ðŸ”¹ STOPWORD REMOVAL:\\n\", no_stop.split(), \"\\n\")\n\n\n\n# --- 5. Stemming ---\nstem_factory = StemmerFactory()\nstemmer = stem_factory.create_stemmer()\nstemmed = stemmer.stem(no_stop)\nprint(\"ðŸ”¹ STEMMING:\\n\", stemmed.split(), \"\\n\")\n\n\n\n##Preprocessing keseluruhan\n\n# === Ambil kolom abstrak ===\nabstrak_list = manajemen[\"abstrak_id\"].dropna().astype(str).tolist()\n\n\n\n# === 1. Siapkan stemmer & stopword ===\nfactory = StemmerFactory()\nstemmer = factory.create_stemmer()\n\nstop_factory = StopWordRemoverFactory()\nstopwords = set(stop_factory.get_stop_words())\n\n\n\n# === 2. Preprocessing & tokenisasi ===\nprocessed_words = []\nfor text in abstrak_list:   # abstrak_list sudah berisi daftar abstrak\n    text = text.lower()  # case folding\n    text = re.sub(r'[^a-z\\s]', ' ', text)  # hapus non-huruf\n    tokens = text.split()  # tokenisasi\n    tokens = [t for t in tokens if t not in stopwords]  # hapus stopword\n    stemmed = [stemmer.stem(t) for t in tokens]  # stemming\n    processed_words.extend(stemmed)  # gabungkan semua kata\n\n\n\n# === 3. Hitung frekuensi kata ===\ncounter = Counter(processed_words)\nsorted_freq = counter.most_common()  # otomatis urut descending\n\n\n\n# === 4. Simpan ke CSV dengan indeks ===\nfreq_df = pd.DataFrame(sorted_freq, columns=[\"kata\", \"frekuensi\"])\nfreq_df.index += 1  # mulai indeks dari 1\nfreq_df.to_csv(\"/content/drive/MyDrive/PPW/output/frekuensi_kata_indexed.csv\",\n               index=True, encoding=\"utf-8-sig\", index_label=\"indeks\")\n\nprint(\"âœ… Frekuensi kata berhasil disimpan dengan indeks.\")\nprint(freq_df.head(20))  # tampilkan 10 kata teratas\n\n\n\npreprocess_manajemen_path = \"/content/drive/MyDrive/PPW/output/frekuensi_kata_indexed.csv\"\npreprocess_manajemen = pd.read_csv(preprocess_manajemen_path)\n\npreprocess_manajemen\n\n\n","type":"content","url":"/crawlingpta-manajemen#id-2-pre-processing","position":5},{"hierarchy":{"lvl1":"Data Nasa"},"type":"lvl1","url":"/nasa","position":0},{"hierarchy":{"lvl1":"Data Nasa"},"content":"https://â€‹itaâ€‹.eeâ€‹.lblâ€‹.govâ€‹/htmlâ€‹/contribâ€‹/NASAâ€‹-HTTPâ€‹.html\n\n","type":"content","url":"/nasa","position":1},{"hierarchy":{"lvl1":"Data Nasa","lvl2":"Load log file"},"type":"lvl2","url":"/nasa#load-log-file","position":2},{"hierarchy":{"lvl1":"Data Nasa","lvl2":"Load log file"},"content":"","type":"content","url":"/nasa#load-log-file","position":3},{"hierarchy":{"lvl1":"Data Nasa","lvl3":"Subtask:","lvl2":"Load log file"},"type":"lvl3","url":"/nasa#subtask","position":4},{"hierarchy":{"lvl1":"Data Nasa","lvl3":"Subtask:","lvl2":"Load log file"},"content":"â€œMemuat file log ke pandas DataFrame. Karena file tidak memiliki ekstensi dan formatnya khusus, maka harus dibaca sebagai file teks biasa, lalu diparsing (dipecah) per baris.â€","type":"content","url":"/nasa#subtask","position":5},{"hierarchy":{"lvl1":"Data Nasa","lvl3":"Reasoning:","lvl2":"Load log file"},"type":"lvl3","url":"/nasa#reasoning","position":6},{"hierarchy":{"lvl1":"Data Nasa","lvl3":"Reasoning:","lvl2":"Load log file"},"content":"â€œBaca file log baris per baris dan simpan setiap barisnya ke dalam sebuah list.â€\n\nwith open(\"/content/drive/MyDrive/PPW/NASA/access_log_Jul95\", 'r', encoding='latin-1') as f:\n    log_lines = f.readlines()\n\n\n\n","type":"content","url":"/nasa#reasoning","position":7},{"hierarchy":{"lvl1":"Data Nasa","lvl3":"Reasoning:","lvl2":"Load log file"},"type":"lvl3","url":"/nasa#reasoning-1","position":8},{"hierarchy":{"lvl1":"Data Nasa","lvl3":"Reasoning:","lvl2":"Load log file"},"content":"â€œLangkah sebelumnya berhasil membaca berkas log ke dalam daftar string. Tugas saat ini adalah memuat berkas log ke dalam daftar string. Langkah berikutnya adalah memverifikasi isi berkas dan menyelesaikan tugas tersebut.â€\n\nprint(log_lines[:5])\n\n\n\n","type":"content","url":"/nasa#reasoning-1","position":9},{"hierarchy":{"lvl1":"Data Nasa","lvl3":"Parse the log data","lvl2":"Load log file"},"type":"lvl3","url":"/nasa#parse-the-log-data","position":10},{"hierarchy":{"lvl1":"Data Nasa","lvl3":"Parse the log data","lvl2":"Load log file"},"content":"","type":"content","url":"/nasa#parse-the-log-data","position":11},{"hierarchy":{"lvl1":"Data Nasa","lvl3":"Subtask:","lvl2":"Load log file"},"type":"lvl3","url":"/nasa#subtask-1","position":12},{"hierarchy":{"lvl1":"Data Nasa","lvl3":"Subtask:","lvl2":"Load log file"},"content":"Setelah berhasil memuat file log NASA ke dalam Python sebagai list berisi baris-baris log.\"\n\nimport re\n\nlog_pattern = re.compile(r'(\\S+) - - \\[(.*?)\\] \"(\\S+) (.*?) (\\S+)\" (\\S+) (\\S+)')\nparsed_data = []\n\nfor line in log_lines:\n    match = log_pattern.match(line)\n    if match:\n        hostname, timestamp, request_type, path, protocol, status_code, size = match.groups()\n        parsed_data.append([hostname, timestamp, request_type, path, status_code, size])\n    else:\n        # Optional: Log lines that don't match the pattern\n        # print(f\"Skipping line: {line.strip()}\")\n        pass\n\nprint(parsed_data[:5])\n\n\n\n","type":"content","url":"/nasa#subtask-1","position":13},{"hierarchy":{"lvl1":"Data Nasa","lvl3":"Structure the data","lvl2":"Load log file"},"type":"lvl3","url":"/nasa#structure-the-data","position":14},{"hierarchy":{"lvl1":"Data Nasa","lvl3":"Structure the data","lvl2":"Load log file"},"content":"","type":"content","url":"/nasa#structure-the-data","position":15},{"hierarchy":{"lvl1":"Data Nasa","lvl3":"Subtask:","lvl2":"Load log file"},"type":"lvl3","url":"/nasa#subtask-2","position":16},{"hierarchy":{"lvl1":"Data Nasa","lvl3":"Subtask:","lvl2":"Load log file"},"content":"Menyusun data hasil ekstraksi (hasil parsing log) ke dalam format yang terstruktur, misalnya menjadi pandas DataFrame, dan memberikan nama kolom yang sesuai.\"\n\nimport pandas as pd\n\ndf = pd.DataFrame(parsed_data, columns=['hostname', 'timestamp', 'request_type', 'path', 'status_code', 'size'])\ndisplay(df.head())\n\n\n\nlen(df.timestamp)\n\n\n\n# === Tampilkan jumlah duplikasi berdasarkan IP ===\ndupe_count = df[\"hostname\"].value_counts()\nprint(\"Jumlah kemunculan setiap IP:\")\ndisplay(dupe_count.head(10))  # tampilkan 10 IP teratas\n\n\n\n\n\ndf['timestamp'] = pd.to_datetime(df['timestamp'], format='%d/%b/%Y:%H:%M:%S %z')\ndf_sorted = df.sort_values(by=['hostname', 'timestamp'])\ndisplay(df_sorted.head())\n\n\n\nrequest_type_counts = df['request_type'].value_counts()\nprint(\"Nilai unik dalam kolom 'request_type' beserta jumlahnya:\")\ndisplay(request_type_counts)\n\n\n\n\n\nrequest_type_counts = df['status_code'].value_counts()\nprint(\"Nilai unik dalam kolom 'status_code' beserta jumlahnya:\")\ndisplay(request_type_counts)\n\n\n\n\n\n","type":"content","url":"/nasa#subtask-2","position":17},{"hierarchy":{"lvl1":"Data Nasa","lvl3":"Berarti filter hanya pada path html,status code unik, and request type get","lvl2":"Load log file"},"type":"lvl3","url":"/nasa#berarti-filter-hanya-pada-path-html-status-code-unik-and-request-type-get","position":18},{"hierarchy":{"lvl1":"Data Nasa","lvl3":"Berarti filter hanya pada path html,status code unik, and request type get","lvl2":"Load log file"},"content":"\n\npost_html_count = df[(df['request_type'] == 'POST') & (df['path'].str.endswith('.html'))].shape[0]\nprint(f\"Jumlah permintaan POST dengan path diakhiri '.html': {post_html_count}\")\n\n\n\npost_html_count = df[(df['request_type'] == 'GET') & (df['path'].str.endswith('.html'))].shape[0]\nprint(f\"Jumlah permintaan POST dengan path diakhiri '.html': {post_html_count}\")\n\n\n\n","type":"content","url":"/nasa#berarti-filter-hanya-pada-path-html-status-code-unik-and-request-type-get","position":19},{"hierarchy":{"lvl1":"Data Nasa","lvl3":"jadi filter hanya get nya aja","lvl2":"Load log file"},"type":"lvl3","url":"/nasa#jadi-filter-hanya-get-nya-aja","position":20},{"hierarchy":{"lvl1":"Data Nasa","lvl3":"jadi filter hanya get nya aja","lvl2":"Load log file"},"content":"\n\nfiltered_df = df[(df['request_type'] == 'GET') & (df['path'].str.endswith('.html')) & (df['status_code'] == '200')]\ndisplay(filtered_df.head())\n\n\n\n","type":"content","url":"/nasa#jadi-filter-hanya-get-nya-aja","position":21},{"hierarchy":{"lvl1":"Data Nasa","lvl3":"kemudian hostname yang sama di urutkan berdasarkan waktunya/ timestamp","lvl2":"Load log file"},"type":"lvl3","url":"/nasa#kemudian-hostname-yang-sama-di-urutkan-berdasarkan-waktunya-timestamp","position":22},{"hierarchy":{"lvl1":"Data Nasa","lvl3":"kemudian hostname yang sama di urutkan berdasarkan waktunya/ timestamp","lvl2":"Load log file"},"content":"\n\nfiltered_df_sorted = filtered_df.sort_values(by=['hostname', 'timestamp'])\ndisplay(filtered_df_sorted.head())\n\n\n\nfiltered_df_sorted.to_csv('/content/drive/MyDrive/PPW/NASA/filtered_sorted_log_data.csv', index=False)\nprint(\"DataFrame telah disimpan ke 'filtered_sorted_log_data.csv'\")\n\n\n\nnama_file = '/content/drive/MyDrive/PPW/NASA/filtered_sorted_log_data.csv'\ndf = pd.read_csv(nama_file)\n\n\n\ndf\n\n","type":"content","url":"/nasa#kemudian-hostname-yang-sama-di-urutkan-berdasarkan-waktunya-timestamp","position":23},{"hierarchy":{"lvl1":"Data Nasa New"},"type":"lvl1","url":"/nasa2","position":0},{"hierarchy":{"lvl1":"Data Nasa New"},"content":"https://â€‹itaâ€‹.eeâ€‹.lblâ€‹.govâ€‹/htmlâ€‹/contribâ€‹/NASAâ€‹-HTTPâ€‹.html\n\n","type":"content","url":"/nasa2","position":1},{"hierarchy":{"lvl1":"Data Nasa New","lvl2":"Load log file"},"type":"lvl2","url":"/nasa2#load-log-file","position":2},{"hierarchy":{"lvl1":"Data Nasa New","lvl2":"Load log file"},"content":"","type":"content","url":"/nasa2#load-log-file","position":3},{"hierarchy":{"lvl1":"Data Nasa New","lvl3":"Subtask:","lvl2":"Load log file"},"type":"lvl3","url":"/nasa2#subtask","position":4},{"hierarchy":{"lvl1":"Data Nasa New","lvl3":"Subtask:","lvl2":"Load log file"},"content":"â€œMemuat file log ke pandas DataFrame. Karena file tidak memiliki ekstensi dan formatnya khusus, maka harus dibaca sebagai file teks biasa, lalu diparsing (dipecah) per baris.â€","type":"content","url":"/nasa2#subtask","position":5},{"hierarchy":{"lvl1":"Data Nasa New","lvl3":"Reasoning:","lvl2":"Load log file"},"type":"lvl3","url":"/nasa2#reasoning","position":6},{"hierarchy":{"lvl1":"Data Nasa New","lvl3":"Reasoning:","lvl2":"Load log file"},"content":"â€œBaca file log baris per baris dan simpan setiap barisnya ke dalam sebuah list.â€\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n\n\nimport requests\n\nurl = \"https://ita.ee.lbl.gov/traces/NASA_access_log_Jul95.gz\"\nfilename = \"NASA_access_log_Jul95.gz\"\n\nprint(f\"Mengunduh {filename} ...\")\n\ntry:\n    response = requests.get(url, stream=True)\n\n    if response.status_code == 200:\n        with open(filename, \"wb\") as f:\n            for chunk in response.iter_content(chunk_size=1024):\n                if chunk:\n                    f.write(chunk)\n        print(\"Download selesai!\")\n    else:\n        print(f\"Gagal mengunduh. Status: {response.status_code}\")\n\nexcept Exception as e:\n    print(f\"Terjadi error: {e}\")\n\n\n\n\nimport gzip\n\nfilename = '/content/drive/MyDrive/PPW/COBA/NASA_access_log_Jul95.gz'\n\nprint(f\"--- Menampilkan 20 baris pertama dari file {filename} ---\\n\")\n\ntry:\n    # 'rt' artinya read text mode.\n    # encoding='latin-1' dipakai agar tidak error jika ada karakter aneh di log lama\n    with gzip.open(filename, 'rt', encoding='latin-1') as f:\n        for i, line in enumerate(f):\n            print(f\"Baris {i+1}: {line.strip()}\")\n\n            # Berhenti setelah 20 baris agar terminal tidak penuh\n            if i >= 19:\n                break\n\nexcept FileNotFoundError:\n    print(f\"Error: File '{filename}' tidak ditemukan di folder ini.\")\nexcept Exception as e:\n    print(f\"Terjadi kesalahan: {e}\")\n\n\n\n","type":"content","url":"/nasa2#reasoning","position":7},{"hierarchy":{"lvl1":"Data Nasa New","lvl3":"Reasoning:","lvl2":"Load log file"},"type":"lvl3","url":"/nasa2#reasoning-1","position":8},{"hierarchy":{"lvl1":"Data Nasa New","lvl3":"Reasoning:","lvl2":"Load log file"},"content":"â€œLangkah sebelumnya berhasil membaca berkas log ke dalam daftar string. Tugas saat ini adalah memuat berkas log ke dalam daftar string. Langkah berikutnya adalah memverifikasi isi berkas dan menyelesaikan tugas tersebut.â€\n\nimport gzip\nimport csv\nimport re\nfrom datetime import datetime\n\n# 1. Konfigurasi Nama File\ninput_file = '/content/drive/MyDrive/PPW/COBA/NASA_access_log_Jul95.gz'\noutput_file = '/content/drive/MyDrive/PPW/NASA_Jul95_cleaned.csv'\n\n# 2. Pola Regex untuk membaca format Log Apache/Nasa\n# Pola: Host - - [Tanggal] \"Request\" Status Bytes\nlog_pattern = re.compile(r'^(\\S+) \\S+ \\S+ \\[(.*?)\\] \"(.*?)\" (\\d{3}) (\\S+)')\n\ndef format_date(date_str):\n    \"\"\"\n    Mengubah format '01/Jul/1995:00:00:01 -0400'\n    menjadi format ISO '1995-07-01T00:00:01Z'\n    \"\"\"\n    try:\n        # Ambil bagian tanggal saja, abaikan zona waktu -0400 untuk penyederhanaan\n        clean_date = date_str.split(' ')[0]\n        # Parse dari format lama\n        dt_obj = datetime.strptime(clean_date, '%d/%b/%Y:%H:%M:%S')\n        # Ubah ke format baru (ISO 8601)\n        return dt_obj.strftime('%Y-%m-%dT%H:%M:%SZ')\n    except ValueError:\n        return date_str\n\nprint(\"Sedang memproses data... (Ini mungkin memakan waktu beberapa detik)\")\n\n# 3. Membuka file GZ dan File CSV Output\n# encoding='latin-1' digunakan karena log lama kadang punya karakter aneh di URL\nwith gzip.open(input_file, 'rt', encoding='latin-1') as f_in, \\\n     open(output_file, 'w', newline='', encoding='utf-8') as f_out:\n\n    # Siapkan penulis CSV\n    # quoting=csv.QUOTE_NONNUMERIC akan memberi tanda kutip pada String, tapi tidak pada Angka\n    writer = csv.writer(f_out, quoting=csv.QUOTE_NONNUMERIC)\n\n    # 4. Tulis Header\n    header = [\n        \"Remote host\", \"Remote logname\", \"Remote user\",\n        \"Request time\", \"Request method\", \"Request URI\",\n        \"Request Protocol\", \"Status\", \"Size of response (incl. headers)\"\n    ]\n    writer.writerow(header)\n\n    count = 0\n    errors = 0\n\n    # 5. Loop setiap baris log\n    for line in f_in:\n        match = log_pattern.match(line)\n        if match:\n            host, timestamp, request_full, status, size = match.groups()\n\n            # Proses Tanggal\n            time_iso = format_date(timestamp)\n\n            # Proses Request (Split Method, URI, Protocol)\n            # Contoh: \"GET /images/nasa-logo.gif HTTP/1.0\"\n            req_parts = request_full.split()\n            if len(req_parts) >= 3:\n                method = req_parts[0]\n                uri = req_parts[1]\n                protocol = req_parts[2]\n            elif len(req_parts) == 2: # Kadang protokol tidak ada\n                method = req_parts[0]\n                uri = req_parts[1]\n                protocol = \"HTTP/1.0\"\n            else:\n                method = \"UNKNOWN\"\n                uri = request_full\n                protocol = \"-\"\n\n            # Proses Size (Ubah '-' menjadi 0 dan pastikan integer)\n            if size == '-':\n                size_int = 0\n            else:\n                try:\n                    size_int = int(size)\n                except:\n                    size_int = 0\n\n            # Pastikan Status jadi integer\n            status_int = int(status)\n\n            # Tulis baris ke CSV\n            # Logname dan User diisi \"-\" karena di log NASA isinya memang kosong\n            writer.writerow([\n                host, \"-\", \"-\", time_iso,\n                method, uri, protocol,\n                status_int, size_int\n            ])\n\n            count += 1\n        else:\n            errors += 1\n\nprint(f\"Selesai! {count} baris berhasil diproses.\")\nif errors > 0:\n    print(f\"Ada {errors} baris yang formatnya aneh dan dilewati.\")\nprint(f\"File tersimpan sebagai: {output_file}\")\n\n\n\n","type":"content","url":"/nasa2#reasoning-1","position":9},{"hierarchy":{"lvl1":"Data Nasa New","lvl3":"Parse the log data","lvl2":"Load log file"},"type":"lvl3","url":"/nasa2#parse-the-log-data","position":10},{"hierarchy":{"lvl1":"Data Nasa New","lvl3":"Parse the log data","lvl2":"Load log file"},"content":"","type":"content","url":"/nasa2#parse-the-log-data","position":11},{"hierarchy":{"lvl1":"Data Nasa New","lvl3":"Subtask:","lvl2":"Load log file"},"type":"lvl3","url":"/nasa2#subtask-1","position":12},{"hierarchy":{"lvl1":"Data Nasa New","lvl3":"Subtask:","lvl2":"Load log file"},"content":"Setelah berhasil memuat file log NASA ke dalam Python sebagai list berisi baris-baris log.\"\n\nimport pandas as pd\n\n# 1. Load file CSV yang sudah dibersihkan\nfilename = '/content/drive/MyDrive/PPW/NASA_Jul95_cleaned.csv'\ndf = pd.read_csv(filename)\n\n# 2. Tampilkan info dasar (optional, untuk cek tipe data)\n# print(df.info())\n\n# 3. Tampilkan 10 baris pertama dalam bentuk tabel rapi\n# Jika di Jupyter Notebook, cukup ketik 'df.head(10)' di baris terakhir\nprint(df.head(50).to_markdown(index=False, numalign=\"left\", stralign=\"left\"))\n\n# Catatan: .to_markdown() membutuhkan library 'tabulate'\n# Install dulu jika error: pip install tabulate\n\n\n\n","type":"content","url":"/nasa2#subtask-1","position":13},{"hierarchy":{"lvl1":"Data Nasa New","lvl3":"Filterisasi:","lvl2":"Load log file"},"type":"lvl3","url":"/nasa2#filterisasi","position":14},{"hierarchy":{"lvl1":"Data Nasa New","lvl3":"Filterisasi:","lvl2":"Load log file"},"content":"\n\nimport pandas as pd\nfrom google.colab import files\n\ndf = pd.read_csv(filename, sep=None, engine=\"python\")\n\ndf.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\nprint(\"ðŸ” Kolom yang tersedia:\", df.columns.tolist())\n\nfiltered = df[\n    (df[\"request_method\"] == \"GET\") &\n    (df[\"request_uri\"].isin([\"/images/NASA-logosmall.gif\"])) &\n    (df[\"status\"] == 200)\n]\n\nprint(f\"Jumlah data hasil filter: {len(filtered)}\")\ndisplay(filtered.head())\n\n\n\n\n\n\n# simpan hasil\noutput_file = \"/content/drive/MyDrive/PPW/COBA/filtered_log_nasa.csv\"\nfiltered.to_csv(output_file, index=False)\nfiles.download(output_file)\n\n\n\n\n\n","type":"content","url":"/nasa2#filterisasi","position":15},{"hierarchy":{"lvl1":"Data Nasa New","lvl3":"Cek Duplikasi:","lvl2":"Load log file"},"type":"lvl3","url":"/nasa2#cek-duplikasi","position":16},{"hierarchy":{"lvl1":"Data Nasa New","lvl3":"Cek Duplikasi:","lvl2":"Load log file"},"content":"\n\n\ndf.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n\n# === Tampilkan jumlah duplikasi berdasarkan IP ===\ndupe_count = df[\"remote_host\"].value_counts()\nprint(\"Jumlah kemunculan setiap IP:\")\ndisplay(dupe_count.head(10))  # tampilkan 10 IP teratas\n\n# === Tampilkan hanya IP yang muncul lebih dari 1 kali ===\ndupe_ips = dupe_count[dupe_count > 1]\nprint(f\"Jumlah IP yang duplikat: {len(dupe_ips)}\")\ndisplay(dupe_ips)\n\n\n\n\n\n\n\n\n\n# === Konversi kolom waktu menjadi datetime agar bisa diurutkan ===\ndf[\"request_time\"] = pd.to_datetime(df[\"request_time\"], errors=\"coerce\")\n\n# === Urutkan berdasarkan IP dan waktu ===\ndf_sorted = df.sort_values(by=[\"remote_host\", \"request_time\"]).reset_index(drop=True)\n\n# === Filter sesuai kriteria ===\nfiltered = df_sorted[\n    (df_sorted[\"request_method\"] == \"GET\") &\n    (df_sorted[\"request_uri\"].isin([\"/images/NASA-logosmall.gif\"])) &\n    (df_sorted[\"status\"] == 200)\n]\n\n# === Urutkan hasil akhir berdasarkan IP (Remote Host) ===\nfiltered = filtered.sort_values(by=\"remote_host\").reset_index(drop=True)\n\n# === Tampilkan hasil ===\nprint(f\"Jumlah data hasil filter: {len(filtered)}\")\ndisplay(filtered.head(20))  # tampilkan 20 baris teratas\n\n# === Simpan hasil ke file baru dan download ===\noutput_file = \"/content/drive/MyDrive/PPW/COBA/filtered_log_sorted_by_ip.csv\"\nfiltered.to_csv(output_file, index=False)\n#files.download(output_file)\n\n\n\n\n","type":"content","url":"/nasa2#cek-duplikasi","position":17},{"hierarchy":{"lvl1":"PageRank (Link dlm Page)"},"type":"lvl1","url":"/pagerank-link-dlm-page","position":0},{"hierarchy":{"lvl1":"PageRank (Link dlm Page)"},"content":"import pandas as pd\nimport numpy as np\nimport networkx as nx\nimport matplotlib.pyplot as plt\nfrom IPython.display import display\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n\n\nnama_file = '/content/drive/MyDrive/PPW/output/cnnindonesia_links1.csv'\ndata_page = pd.read_csv(nama_file)\n\n\n\ndisplay(data_page)\n\n\n\n","type":"content","url":"/pagerank-link-dlm-page","position":1},{"hierarchy":{"lvl1":"PageRank (Link dlm Page)","lvl2":"Membangun Graph Menggunakan NetworkX"},"type":"lvl2","url":"/pagerank-link-dlm-page#membangun-graph-menggunakan-networkx","position":2},{"hierarchy":{"lvl1":"PageRank (Link dlm Page)","lvl2":"Membangun Graph Menggunakan NetworkX"},"content":"\n\ngraph = nx.DiGraph()\nedge = list(zip(data_page['page'], data_page['link_keluar']))\ngraph.add_edges_from(edge)\n\nprint(f\"Jumlah node (page unik): {graph.number_of_nodes()}\")\nprint(f\"Jumlah edge (tautan): {graph.number_of_edges()}\")\n\n\n\n# Visualisasi Graph\nplt.figure(figsize=(16, 12))\nplt.title(\"Visualisasi Graph (662 Node dengan 3.561 Edge)\", fontsize=14, fontweight='bold')\n\n# layout efisien untuk dataset besar\npos = nx.spring_layout(graph, seed=42, k=0.15, iterations=20)\n\n# gambar node dan edge\nnx.draw_networkx_nodes(graph, pos, node_color='skyblue', node_size=25, alpha=0.8)\n# nx.draw_networkx_edges(graph, pos, edge_color='black', arrows=False, alpha=0.3)\nnx.draw_networkx_edges(graph, pos, edge_color='black', arrowstyle='->', arrowsize=12, alpha=0.3)\n\n# label, bisa ditampilkan (resiko visualisasi ga jelas karena link panjang). aktifkan :\n# nx.draw_networkx_labels(graph, pos, font_size=6, font_color='black')\n\nplt.axis('off')\nplt.show()\n\n\n\n","type":"content","url":"/pagerank-link-dlm-page#membangun-graph-menggunakan-networkx","position":3},{"hierarchy":{"lvl1":"PageRank (Link dlm Page)","lvl2":"Membentuk Matriks Adjacency"},"type":"lvl2","url":"/pagerank-link-dlm-page#membentuk-matriks-adjacency","position":4},{"hierarchy":{"lvl1":"PageRank (Link dlm Page)","lvl2":"Membentuk Matriks Adjacency"},"content":"\n\n# note: all matriks berukuran 662 x 662, sangat besar untuk ditampilkan\n# jadi ditampilkan sebagian (misalnya 30x30)\npage = list(graph.nodes())[:30]\nA = nx.to_numpy_array(graph, nodelist=page, dtype=int)\n\ndata_page_A = pd.DataFrame(A, index=page, columns=page)\nprint(\"Matriks Adjacency (A) [30x30]:\")\ndisplay(data_page_A)\n\n\n\n\n\n","type":"content","url":"/pagerank-link-dlm-page#membentuk-matriks-adjacency","position":5},{"hierarchy":{"lvl1":"PageRank (Link dlm Page)","lvl2":"Membentuk Matriks Probabilitas Transisi Baris-Stokastik"},"type":"lvl2","url":"/pagerank-link-dlm-page#membentuk-matriks-probabilitas-transisi-baris-stokastik","position":6},{"hierarchy":{"lvl1":"PageRank (Link dlm Page)","lvl2":"Membentuk Matriks Probabilitas Transisi Baris-Stokastik"},"content":"\n\nadj_full = nx.to_numpy_array(graph, dtype=float)\nn = adj_full.shape[0]\n\n# tangani dangling node (page tanpa link_keluar)\nout_degree = adj_full.sum(axis=1)\nfor i in range(n):\n    if out_degree[i] == 0:\n        adj_full[i, :] = 1.0\n\n# matriks transisi baris-stokastik\nP = adj_full / adj_full.sum(axis=1, keepdims=True)\n\n# tampilan sebagian matriks P\ndata_page_P = pd.DataFrame(P[:30, :30], columns=range(30), index=range(30))\nprint(\"Matriks Transisi (P) [30x30]:\")\ndisplay(data_page_P)\n\n\n\n\n\n","type":"content","url":"/pagerank-link-dlm-page#membentuk-matriks-probabilitas-transisi-baris-stokastik","position":7},{"hierarchy":{"lvl1":"PageRank (Link dlm Page)","lvl2":"Membentuk Matriks Kolom-Stokastik"},"type":"lvl2","url":"/pagerank-link-dlm-page#membentuk-matriks-kolom-stokastik","position":8},{"hierarchy":{"lvl1":"PageRank (Link dlm Page)","lvl2":"Membentuk Matriks Kolom-Stokastik"},"content":"\n\n# M = Páµ€ (kolom-stokastik)\nM = P.T\ndata_page_M = pd.DataFrame(M[:30, :30], columns=range(30), index=range(30))\nprint(\"Matriks M = Páµ€ [30x30]:\")\ndisplay(data_page_M)\n\n\n\n\n\n","type":"content","url":"/pagerank-link-dlm-page#membentuk-matriks-kolom-stokastik","position":9},{"hierarchy":{"lvl1":"PageRank (Link dlm Page)","lvl2":"Menghitung PageRank Iteratif (Manual)"},"type":"lvl2","url":"/pagerank-link-dlm-page#menghitung-pagerank-iteratif-manual","position":10},{"hierarchy":{"lvl1":"PageRank (Link dlm Page)","lvl2":"Menghitung PageRank Iteratif (Manual)"},"content":"\n\ndef pagerank_iteratif(M, pagelist, d=0.85, max_iter=100, tol=1e-6):\n    n = M.shape[0]\n    r = np.ones(n) / n\n    teleport = (1 - d) / n\n\n    for i in range(max_iter):\n        r_new = d * M @ r + teleport\n        # debugging aja\n        indeks_top_page = np.argmax(r_new)  # indeks page dengan PageRank tertinggi\n        top_page = pagelist[indeks_top_page] # page dengan score pagerank tertinggi\n        print(f\"Iterasi {i+1}: Page {top_page} dengan PageRank {r_new[indeks_top_page]:.6f}\")\n        if np.linalg.norm(r_new - r, 1) < tol:\n            # debugging aja\n            print(f\"Konvergen setelah {i+1} iterasi\")\n            break\n        r = r_new\n    return r\n\nprint(\"Menghitung PageRank:\")\n\nurutan_page = list(graph.nodes())\nr = pagerank_iteratif(M, pagelist=urutan_page, d=0.85, max_iter=100, tol=1e-6)\n\n\n\n","type":"content","url":"/pagerank-link-dlm-page#menghitung-pagerank-iteratif-manual","position":11},{"hierarchy":{"lvl1":"PageRank (Link dlm Page)","lvl2":"Menampilkan dan Menyimpan Hasil PageRank (Semua Page)"},"type":"lvl2","url":"/pagerank-link-dlm-page#menampilkan-dan-menyimpan-hasil-pagerank-semua-page","position":12},{"hierarchy":{"lvl1":"PageRank (Link dlm Page)","lvl2":"Menampilkan dan Menyimpan Hasil PageRank (Semua Page)"},"content":"\n\nscore_pagerank = pd.DataFrame({\n    'page': list(graph.nodes()),\n    'pagerank': r\n})\n\n# mengurutkan nilai PageRank tertinggi ke terendah\ndata_pagerank_page = score_pagerank.sort_values(by='pagerank', ascending=False).reset_index(drop=True)\nprint(\"Data Page dan Score PageRank:\")\ndisplay(data_pagerank_page)\n\n\n\n\n\n","type":"content","url":"/pagerank-link-dlm-page#menampilkan-dan-menyimpan-hasil-pagerank-semua-page","position":13},{"hierarchy":{"lvl1":"PageRank (Link dlm Page)","lvl2":"5 Page"},"type":"lvl2","url":"/pagerank-link-dlm-page#id-5-page","position":14},{"hierarchy":{"lvl1":"PageRank (Link dlm Page)","lvl2":"5 Page"},"content":"\n\nlima_page_penting = data_pagerank_page.head(5)\nprint(\"5 Page dengan Nilai PageRank Tertinggi:\")\ndisplay(lima_page_penting)\n\n\n\n\n\n","type":"content","url":"/pagerank-link-dlm-page#id-5-page","position":15},{"hierarchy":{"lvl1":"PageRank (Link dlm Page)","lvl2":"Perbandingan: Menghitung dan Menampilkan Hasil PageRank Menggunakan NetworkX (Semua Page)"},"type":"lvl2","url":"/pagerank-link-dlm-page#perbandingan-menghitung-dan-menampilkan-hasil-pagerank-menggunakan-networkx-semua-page","position":16},{"hierarchy":{"lvl1":"PageRank (Link dlm Page)","lvl2":"Perbandingan: Menghitung dan Menampilkan Hasil PageRank Menggunakan NetworkX (Semua Page)"},"content":"\n\nscore_pagerank_networkx = nx.pagerank(graph, alpha=0.85)\ndata_pagerank_page_networkx = pd.DataFrame(list(score_pagerank_networkx.items()), columns=['page', 'pagerank'])\n\n# page score pagerank tertinggi ke terendah\ndata_page_pagerank_networkx_urut = data_pagerank_page_networkx.sort_values(by='pagerank', ascending=False).reset_index(drop=True)\nprint(\"Perbandingan Menggunakan NetworkX - Data Page dan Score PageRank:\")\ndisplay(data_page_pagerank_networkx_urut)\n\n\n\n\n\n# OPSIONAL\n# kalau mau simpan hasil data page dan score pagerank pembanding menggunakan NetworkX, aktifkan :\n# data_page_pagerank_networkx_urut.to_csv(\"PPW_Tugas6_LinkDalamPage_PageRank(NetworkX).csv\", index=False)\n\n\n\n","type":"content","url":"/pagerank-link-dlm-page#perbandingan-menghitung-dan-menampilkan-hasil-pagerank-menggunakan-networkx-semua-page","position":17},{"hierarchy":{"lvl1":"PageRank (Link dlm Page)","lvl2":"5 Page"},"type":"lvl2","url":"/pagerank-link-dlm-page#id-5-page-1","position":18},{"hierarchy":{"lvl1":"PageRank (Link dlm Page)","lvl2":"5 Page"},"content":"\n\nlima_page_penting_networkx = data_page_pagerank_networkx_urut.head(5)\n\nprint(\"5 Page dengan Nilai PageRank Tertinggi (NetworkX):\")\ndisplay(lima_page_penting_networkx)\n\n\n\n\n\n","type":"content","url":"/pagerank-link-dlm-page#id-5-page-1","position":19},{"hierarchy":{"lvl1":"PageRank (Link dlm Page)","lvl2":"Visualisasi Hubungan Page Penting dengan Page Terhubung"},"type":"lvl2","url":"/pagerank-link-dlm-page#visualisasi-hubungan-page-penting-dengan-page-terhubung","position":20},{"hierarchy":{"lvl1":"PageRank (Link dlm Page)","lvl2":"Visualisasi Hubungan Page Penting dengan Page Terhubung"},"content":"\n\npage_penting = lima_page_penting.iloc[0]['page']\nprint(f\"Page dengan nilai PageRank tertinggi: {page_penting}\")\n\nneighbors_out = list(graph.successors(page_penting))\nneighbors_in = list(graph.predecessors(page_penting))\npage_terhubung = set(neighbors_out + neighbors_in + [page_penting])\nsubgraph_page_terhubung = graph.subgraph(page_terhubung)\n\n\n\n# Visualisasi Graph\nplt.figure(figsize=(12, 9))\nplt.title(f\"Graph Keterhubungan Page Penting:\\n{page_penting}\", fontsize=14, fontweight='bold')\n\npos = nx.spring_layout(subgraph_page_terhubung, seed=42, k=0.3)\nwarna_page = ['red' if page == page_penting else 'skyblue' for page in subgraph_page_terhubung.nodes()]\n\nnx.draw_networkx_nodes(subgraph_page_terhubung, pos, node_color=warna_page, node_size=600, alpha=0.9)\nnx.draw_networkx_edges(subgraph_page_terhubung, pos, edge_color='black', arrowstyle='->', arrowsize=8, alpha=0.6)\nnx.draw_networkx_labels(subgraph_page_terhubung, pos, font_size=7, font_color='black')\n\nplt.axis('off')\nplt.show()\n\n\n\n# Data Keterhubungan Page Penting\nprint(f\"Jumlah total page yang terhubung dengan page penting: {len(page_terhubung)}\")\n\n# DataFrame untuk incoming dan outgoing edges\ndf_in = pd.DataFrame({\n    'page': neighbors_in,\n    'link_keluar': [page_penting] * len(neighbors_in)\n})\ndf_out = pd.DataFrame({\n    'page': [page_penting] * len(neighbors_out),\n    'link_keluar': neighbors_out\n})\n\n# gabungkan kedua arah hubungan\ndata_page_terhubung = pd.concat([df_in, df_out], ignore_index=True)\n\nprint(f\"Daftar Page yang Terhubung dengan Page Penting:\")\ndisplay(data_page_terhubung)\n\n\n\n","type":"content","url":"/pagerank-link-dlm-page#visualisasi-hubungan-page-penting-dengan-page-terhubung","position":21},{"hierarchy":{"lvl1":"Word Graph - UAS"},"type":"lvl1","url":"/uas-ppw-final","position":0},{"hierarchy":{"lvl1":"Word Graph - UAS"},"content":"","type":"content","url":"/uas-ppw-final","position":1},{"hierarchy":{"lvl1":"Word Graph - UAS","lvl2":"Nama : Ahya Cinta Azka M."},"type":"lvl2","url":"/uas-ppw-final#nama-ahya-cinta-azka-m","position":2},{"hierarchy":{"lvl1":"Word Graph - UAS","lvl2":"Nama : Ahya Cinta Azka M."},"content":"","type":"content","url":"/uas-ppw-final#nama-ahya-cinta-azka-m","position":3},{"hierarchy":{"lvl1":"Word Graph - UAS","lvl2":"NIM  : 220411100122"},"type":"lvl2","url":"/uas-ppw-final#nim-220411100122","position":4},{"hierarchy":{"lvl1":"Word Graph - UAS","lvl2":"NIM  : 220411100122"},"content":"","type":"content","url":"/uas-ppw-final#nim-220411100122","position":5},{"hierarchy":{"lvl1":"Word Graph - UAS","lvl2":"Mata Kuliah : Pencarian dan Penambangan Web"},"type":"lvl2","url":"/uas-ppw-final#mata-kuliah-pencarian-dan-penambangan-web","position":6},{"hierarchy":{"lvl1":"Word Graph - UAS","lvl2":"Mata Kuliah : Pencarian dan Penambangan Web"},"content":"\n\n#Import Library\n\npip install --upgrade pymupdf\n\n\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n\n\n%cd /content/drive/MyDrive/PPW/UAS/word_graph\n\n\n\nimport pymupdf\n\ndoc = pymupdf.open(\"/content/drive/MyDrive/PPW/UAS/word_graph/paper global warming.pdf\") # open a document\nout = open(\"/content/drive/MyDrive/PPW/UAS/word_graph/output.txt\", \"wb\") # create a text output\nfor page in doc: # iterate the document pages\n    text = page.get_text().encode(\"utf8\") # get plain text (is in UTF-8)\n    out.write(text) # write text of page\n    out.write(bytes((12,))) # write page delimiter (form feed 0x0C)\nout.close()\n\n\n\n%%capture\n!pip install nltk\n\n\n\nimport nltk\nnltk.download('punkt')  # hanya perlu sekali\nnltk.download('punkt_tab')  # opsional, untuk versi terbaru NLTK (â‰¥3.8.2)\n\n\n\n\n\nwith open('/content/drive/MyDrive/PPW/UAS/word_graph/output.txt', 'r', encoding='utf-8') as file:\n    teks = file.read()\n\nprint(teks[:200])  # tampilkan 200 karakter pertama\n\n\n\n# Install: pip install nltk\nimport nltk\n#text = \"Ini adalah kalimat pertama. Ini kalimat kedua? Ya!\"\nsentences = nltk.sent_tokenize(teks)\nprint(sentences)\n# Output: ['Ini adalah kalimat pertama.', 'Ini kalimat kedua?', 'Ya!']\n\n\n\nimport pandas as pd\ndf = pd.DataFrame(sentences, columns=['kalimat'])\nprint(df)\n\n\n\ndf.to_csv('kalimat.csv', index=False, encoding='utf-8')\n\n\n\nPembuatan word graph\ndengan menggunakan \n\nhttps://â€‹wwwâ€‹.geeksforgeeksâ€‹.orgâ€‹/nlpâ€‹/coâ€‹-occurenceâ€‹-matrixâ€‹-inâ€‹-nlp/\n\n","type":"content","url":"/uas-ppw-final#mata-kuliah-pencarian-dan-penambangan-web","position":7},{"hierarchy":{"lvl1":"Word Graph - UAS","lvl2":"Pre-pocessing"},"type":"lvl2","url":"/uas-ppw-final#pre-pocessing","position":8},{"hierarchy":{"lvl1":"Word Graph - UAS","lvl2":"Pre-pocessing"},"content":"\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom collections import defaultdict, Counter\nimport numpy as np\nimport pandas as pd\n\n# Download NLTK resources\nnltk.download('punkt')\nnltk.download('stopwords')\n\n# Load CSV (asumsi kolom berisi kalimat bernama 'kalimat')\ndf = pd.read_csv(\"/content/drive/MyDrive/PPW/UAS/word_graph/kalimat.csv\")\n\n# Gabungkan semua teks\ntext = \" \".join(df.iloc[:, 0].astype(str))\n\n# Preprocess the text\nstop_words = set(stopwords.words('indonesian'))\nwords = word_tokenize(text.lower())\nwords = [word for word in words if word.isalnum() and word not in stop_words]\n\n# Define the window size for co-occurrence\nwindow_size = 2\n\n# Create a list of co-occurring word pairs\nco_occurrences = defaultdict(Counter)\nfor i, word in enumerate(words):\n    for j in range(max(0, i - window_size), min(len(words), i + window_size + 1)):\n        if i != j:\n            co_occurrences[word][words[j]] += 1\n\n# Create a list of unique words\nunique_words = list(set(words))\n\n# Initialize the co-occurrence matrix\nco_matrix = np.zeros((len(unique_words), len(unique_words)), dtype=int)\n\n# Populate the co-occurrence matrix\nword_index = {word: idx for idx, word in enumerate(unique_words)}\nfor word, neighbors in co_occurrences.items():\n    for neighbor, count in neighbors.items():\n        co_matrix[word_index[word]][word_index[neighbor]] = count\n\n# Create DataFrame\nco_matrix_df = pd.DataFrame(co_matrix, index=unique_words, columns=unique_words)\n\nco_matrix_df\n\n\n\n\n\n\n","type":"content","url":"/uas-ppw-final#pre-pocessing","position":9},{"hierarchy":{"lvl1":"Word Graph - UAS","lvl2":"Networkx"},"type":"lvl2","url":"/uas-ppw-final#networkx","position":10},{"hierarchy":{"lvl1":"Word Graph - UAS","lvl2":"Networkx"},"content":"\n\n%%capture\npip install networkx\n\n\n\nimport networkx as nx\narr = co_matrix_df.to_numpy()\nG=nx.from_numpy_array(arr)\n\n\n\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(8, 6))\nnx.draw(G, with_labels=True, node_color='lightblue', node_size=2000, font_size=12)\nplt.title(\"Graf co-occurence (Dasar)\", fontsize=14)\nplt.show()\n\n\n\nlabels = {i: word for i, word in enumerate(co_matrix_df.index)}\n\n\n\nG = nx.relabel_nodes(G, labels)\n\n\n\nG.number_of_nodes()\n\n\n\nG.number_of_edges()\n\n\n\nlist(G.edges(data=True))\n\n\n\nlist(G.nodes())\n\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# Membuat graph kosong\nG = nx.Graph()\n\n# Menambahkan edge ke graph berdasarkan co-occurrence\nfor word, neighbors in co_occurrences.items():\n    for neighbor, count in neighbors.items():\n        # Tambahkan edge dengan bobot = frekuensi co-occurrence\n        if count > 0:\n            G.add_edge(word, neighbor, weight=count)\n\n# Membuat layout graph\npos = nx.spring_layout(G, k=0.5, seed=42)\n\n# Menggambar node dan edge\nplt.figure(figsize=(12, 10))\n\n# Edge dengan ketebalan sesuai bobot\nedges = G.edges(data=True)\nnx.draw_networkx_edges(\n    G, pos,\n    width=[d['weight'] for (_, _, d) in edges]\n)\n\n# Node dan label\nnx.draw_networkx_nodes(G, pos, node_size=500)\nnx.draw_networkx_labels(G, pos, font_size=10)\n\nplt.title(\"Word Graph Berdasarkan Co-occurrence\")\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n\n\n","type":"content","url":"/uas-ppw-final#networkx","position":11},{"hierarchy":{"lvl1":"Word Graph - UAS","lvl2":"Tugas UAS"},"type":"lvl2","url":"/uas-ppw-final#tugas-uas","position":12},{"hierarchy":{"lvl1":"Word Graph - UAS","lvl2":"Tugas UAS"},"content":"Mengekstrak dokumen pdf jurnal kemudian dinormalisasi angka, menghilangkan kata tidak jelas & kata karakter.\n\nMembuat matrik coocurance dan graph serta pagerank.\n\nOutput berupa graph & nilai pagerank dari hasil graph\n\n!pip install PyPDF2\n\n\n\n!pip install pdfplumber\n\n\n\n","type":"content","url":"/uas-ppw-final#tugas-uas","position":13},{"hierarchy":{"lvl1":"Word Graph - UAS","lvl2":"Ekstrak Paper"},"type":"lvl2","url":"/uas-ppw-final#ekstrak-paper","position":14},{"hierarchy":{"lvl1":"Word Graph - UAS","lvl2":"Ekstrak Paper"},"content":"\n\nimport nltk\nimport re\nimport pandas as pd\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport pdfplumber\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom collections import Counter, defaultdict\n\nnltk.download(\"punkt\")\n\ndef extract_pdf(path):\n    text = \"\"\n    with pdfplumber.open(path) as pdf:\n        for page in pdf.pages:\n            content = page.extract_text()\n            if content:\n                text += content + \" \"\n    return text.lower()\n\n# menggabungkan 2 kata kunci jd 1 token (regex)\ndef merge_multiword_keywords_regex(text, keywords):\n    for kw in keywords:\n        token = kw.replace(\" \", \"_\")\n        # pola regex: cocokkan semua variasi spasi\n        pattern = re.compile(kw.replace(\" \", r\"\\s+\"), re.IGNORECASE)\n\n        text = pattern.sub(token, text)\n    return text\n\n# normalisasi\ndef normalize_text(text):\n    text = re.sub(r\"\\d+\", \" \", text)\n    text = re.sub(r\"[^a-zA-Z_\\s]\", \" \", text)\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    text = re.sub(r\"\\b[a-zA-Z]\\b\", \" \", text)\n    return text\n\nfrom nltk.util import ngrams\nfrom collections import Counter\n\ndef extract_top_bigrams(text, top_n=30):\n    words = word_tokenize(text)\n\n    bigrams = list(ngrams(words, 2))\n\n    bigram_freq = Counter(bigrams)\n\n    return bigram_freq.most_common(top_n)\n\ntop_bigrams = extract_top_bigrams(text, top_n=30)\n\nprint(\"\\n=== TOP 2-KATA (BIGRAM) PALING SERING MUNCUL ===\")\nfor bg, freq in top_bigrams:\n    print(f\"{' '.join(bg):30s}  â†’  {freq}\")\n\n\n# load pdf\npdf_path = \"/content/drive/MyDrive/PPW/UAS/word_graph/paper global warming.pdf\"\nraw_text = extract_pdf(pdf_path)\n\n# kata kunci\nkeywords_multi = [\n    \"perubahan iklim\",\n    \"global warming\",\n    \"climate change\",\n    \"pemanasan global\",\n    \"curah hujan\",\n    \"gas rumah kaca\",\n    \"karbondioksida\",\n    \"keanekaragaman hayati\",\n    \"kenaikan permukaan laut\",\n    \"es mencair\",\n    \"ketahanan pangan\",\n    \"dampak lingkungan\",\n    \"pembangunan berkelanjutan\"\n\n]\n\n\n\n# menggabungkan multiword\nraw_text = merge_multiword_keywords_regex(raw_text, keywords_multi)\n\n# normalisasi\ntext = normalize_text(raw_text)\n\n# tokenisasi\nsentences = sent_tokenize(text)\ntokenized = [word_tokenize(s) for s in sentences]\n\n# coocurrance\ndef build_cooccurrence(tokenized_sentences, window_size=2):\n    cooc = defaultdict(Counter)\n    for sentence in tokenized_sentences:\n        for i, word in enumerate(sentence):\n            window = sentence[i+1:i+1+window_size]\n            for w in window:\n                if w != word:\n                    cooc[word][w] += 1\n                    cooc[w][word] += 1\n    return cooc\n\nco_matrix = build_cooccurrence(tokenized)\n\n# graph 1\ndef build_graph(cooc, min_weight=2):\n    G = nx.Graph()\n    for w, neigh in cooc.items():\n        for n, freq in neigh.items():\n            if freq >= min_weight:\n                G.add_edge(w, n, weight=freq)\n    return G\n\nG_single = build_graph(co_matrix)\n\n# graph 2 -> multiword\n# ubah keyword jadi token multiword (pakai underscore)\nkeyword_nodes = [kw.replace(\" \", \"_\") for kw in keywords_multi]\n\n# kumpulkan semua keyword yang muncul\nexisting_keywords = [n for n in G_single.nodes() if n in keyword_nodes]\n\n# kumpulkan tetangga keyword agar graph terhubung\nneighbors = set()\nfor kw in existing_keywords:\n    if kw in G_single:\n        neighbors.update(G_single.neighbors(kw))\n\n# graph 2 = keyword multiword + tetangganya\nsubgraph_nodes = set(existing_keywords) | neighbors\nG_multiword = G_single.subgraph(subgraph_nodes).copy()\n\nprint(\"Node Graph 2 =\", len(G_multiword.nodes()))\nprint(\"Edge Graph 2 =\", len(G_multiword.edges()))\n\n\n# visualisasi graph\ndef draw_graph(G, title):\n    plt.figure(figsize=(10, 8))\n\n    # posisi node\n    pos = nx.spring_layout(G, k=0.65, seed=42)\n\n    # ambil weight edge\n    weights = [G[u][v]['weight'] for u, v in G.edges()]\n\n    # node\n    nx.draw_networkx_nodes(\n        G, pos,\n        node_size=900,\n        node_color='pink',\n        edgecolors='black',\n        linewidths=1.2\n    )\n\n    # edge (garis antar node)\n    nx.draw_networkx_edges(\n        G, pos,\n        width=[w * 1.5 for w in weights],  # garis lebih tebal\n        alpha=0.8\n    )\n\n    # label node\n    nx.draw_networkx_labels(\n        G, pos,\n        font_size=9,\n        font_weight='bold'\n    )\n\n    # judul\n    plt.title(title, fontsize=14)\n    plt.axis(\"off\")\n    plt.show()\n\n\n# GRAPH KELUAR\ndraw_graph(G_single, \"Graph 1 Kata\")\ndraw_graph(G_multiword, \"Graph 2 Kata (Multiword digabung)\")\n\n# pagerank\npagerank_single = nx.pagerank(G_single)\npagerank_multi = nx.pagerank(G_multiword)\n\nprint(\"\\n=== PAGE RANK GRAPH 1 ===\")\nprint(sorted(pagerank_single.items(), key=lambda x: -x[1])[:20])\n\nprint(\"\\n=== PAGE RANK GRAPH 2 ===\")\nprint(sorted(pagerank_multi.items(), key=lambda x: -x[1]))\n\n\n\n\n\n\n\n\n\n\n","type":"content","url":"/uas-ppw-final#ekstrak-paper","position":15},{"hierarchy":{"lvl1":"Clustering Data Email"},"type":"lvl1","url":"/uts-ppw-clustering","position":0},{"hierarchy":{"lvl1":"Clustering Data Email"},"content":"","type":"content","url":"/uts-ppw-clustering","position":1},{"hierarchy":{"lvl1":"Clustering Data Email","lvl2":"Load Data"},"type":"lvl2","url":"/uts-ppw-clustering#load-data","position":2},{"hierarchy":{"lvl1":"Clustering Data Email","lvl2":"Load Data"},"content":"\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n\n\n\nfile_path = '/content/drive/MyDrive/PPW/UTS/spam.csv'\n\n\n\nimport pandas as pd\nimport re\nimport nltk\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\n\n\n\n# Baca file CSV\ndata = pd.read_csv(file_path, encoding='latin1')\ndata.head()\n\n\n\n# Ambil hanya kolom yang diperlukan\ndatanew = data[['id', 'Text']]\ndatanew\n\n\n\nprint(\"Jumlah data:\", len(datanew))\n\n\n\n","type":"content","url":"/uts-ppw-clustering#load-data","position":3},{"hierarchy":{"lvl1":"Clustering Data Email","lvl3":"Preprocessing","lvl2":"Load Data"},"type":"lvl3","url":"/uts-ppw-clustering#preprocessing","position":4},{"hierarchy":{"lvl1":"Clustering Data Email","lvl3":"Preprocessing","lvl2":"Load Data"},"content":"\n\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('punkt_tab')\n\n\n\n\n\nstemmer = PorterStemmer()\nstop_words = set(stopwords.words('english'))\n\ndef clean_and_stem(text):\n    text = str(text).lower()                       # ubah ke huruf kecil\n    text = re.sub(r'[^a-z\\s]', '', text)           # hapus karakter non-huruf\n    words = nltk.word_tokenize(text)               # tokenisasi\n    words = [stemmer.stem(w) for w in words if w not in stop_words]  # stemming + hapus stopwords\n    return ' '.join(words)\n\ndatanew['clean_text'] = datanew['Text'].apply(clean_and_stem)\ndatanew.head()\n\n\n\n\n\n","type":"content","url":"/uts-ppw-clustering#preprocessing","position":5},{"hierarchy":{"lvl1":"Clustering Data Email","lvl3":"TF-IDF dan K-means =3","lvl2":"Load Data"},"type":"lvl3","url":"/uts-ppw-clustering#tf-idf-dan-k-means-3","position":6},{"hierarchy":{"lvl1":"Clustering Data Email","lvl3":"TF-IDF dan K-means =3","lvl2":"Load Data"},"content":"\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\n\n# TF-IDF\nvectorizer = TfidfVectorizer(stop_words='english')\nX = vectorizer.fit_transform(datanew['clean_text'])\n\n# KMeans (contoh: 3 cluster)\nkmeans = KMeans(n_clusters=3, random_state=42)\ndatanew['cluster'] = kmeans.fit_predict(X)\n\n\n\n\n","type":"content","url":"/uts-ppw-clustering#tf-idf-dan-k-means-3","position":7},{"hierarchy":{"lvl1":"Clustering Data Email","lvl4":"Hasil","lvl3":"TF-IDF dan K-means =3","lvl2":"Load Data"},"type":"lvl4","url":"/uts-ppw-clustering#hasil","position":8},{"hierarchy":{"lvl1":"Clustering Data Email","lvl4":"Hasil","lvl3":"TF-IDF dan K-means =3","lvl2":"Load Data"},"content":"\n\nfor i in range(3):\n    print(f\"\\n=== Cluster {i} ===\")\n    print(datanew[datanew['cluster'] == i]['Text'].head(5).to_string(index=False))\n\n\n\n\n","type":"content","url":"/uts-ppw-clustering#hasil","position":9},{"hierarchy":{"lvl1":"Clustering Data Email","lvl4":"Visualisasi Hasil Clustering (PCA 2D)","lvl3":"TF-IDF dan K-means =3","lvl2":"Load Data"},"type":"lvl4","url":"/uts-ppw-clustering#visualisasi-hasil-clustering-pca-2d","position":10},{"hierarchy":{"lvl1":"Clustering Data Email","lvl4":"Visualisasi Hasil Clustering (PCA 2D)","lvl3":"TF-IDF dan K-means =3","lvl2":"Load Data"},"content":"\n\npca = PCA(n_components=2, random_state=42)\nreduced_X = pca.fit_transform(X.toarray())\n\nplt.figure(figsize=(8,6))\nplt.scatter(reduced_X[:,0], reduced_X[:,1], c=datanew['cluster'], cmap='viridis', s=40)\nplt.title('Visualisasi Clustering Email (TF-IDF + Stemming + KMeans)')\nplt.xlabel('Komponen 1')\nplt.ylabel('Komponen 2')\nplt.show()\n\n\n\n","type":"content","url":"/uts-ppw-clustering#visualisasi-hasil-clustering-pca-2d","position":11},{"hierarchy":{"lvl1":"Clustering Data Email","lvl3":"TF-IDF dan K-means =10","lvl2":"Load Data"},"type":"lvl3","url":"/uts-ppw-clustering#tf-idf-dan-k-means-10","position":12},{"hierarchy":{"lvl1":"Clustering Data Email","lvl3":"TF-IDF dan K-means =10","lvl2":"Load Data"},"content":"\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\n\n# TF-IDF\nvectorizer = TfidfVectorizer(stop_words='english')\nX = vectorizer.fit_transform(datanew['clean_text'])\n\n# KMeans (contoh: 10 cluster)\nkmeans = KMeans(n_clusters=10, random_state=42)\ndatanew['cluster'] = kmeans.fit_predict(X)\n\n\n\n\n","type":"content","url":"/uts-ppw-clustering#tf-idf-dan-k-means-10","position":13},{"hierarchy":{"lvl1":"Clustering Data Email","lvl4":"Hasil","lvl3":"TF-IDF dan K-means =10","lvl2":"Load Data"},"type":"lvl4","url":"/uts-ppw-clustering#hasil-1","position":14},{"hierarchy":{"lvl1":"Clustering Data Email","lvl4":"Hasil","lvl3":"TF-IDF dan K-means =10","lvl2":"Load Data"},"content":"\n\nfor i in range(10):\n    print(f\"\\n=== Cluster {i} ===\")\n    print(datanew[datanew['cluster'] == i]['Text'].head(5).to_string(index=False))\n\n\n\n\n","type":"content","url":"/uts-ppw-clustering#hasil-1","position":15},{"hierarchy":{"lvl1":"Clustering Data Email","lvl4":"Visualisasi Hasil Clustering (PCA 2D)","lvl3":"TF-IDF dan K-means =10","lvl2":"Load Data"},"type":"lvl4","url":"/uts-ppw-clustering#visualisasi-hasil-clustering-pca-2d-1","position":16},{"hierarchy":{"lvl1":"Clustering Data Email","lvl4":"Visualisasi Hasil Clustering (PCA 2D)","lvl3":"TF-IDF dan K-means =10","lvl2":"Load Data"},"content":"\n\npca = PCA(n_components=2, random_state=42)\nreduced_X = pca.fit_transform(X.toarray())\n\nplt.figure(figsize=(8,6))\nplt.scatter(reduced_X[:,0], reduced_X[:,1], c=datanew['cluster'], cmap='viridis', s=40)\nplt.title('Visualisasi Clustering Email (TF-IDF + Stemming + KMeans)')\nplt.xlabel('Komponen 1')\nplt.ylabel('Komponen 2')\nplt.show()\n\n","type":"content","url":"/uts-ppw-clustering#visualisasi-hasil-clustering-pca-2d-1","position":17},{"hierarchy":{"lvl1":"klasifikasi berita dengan ekstraksi fitur model topik modelling dengan classifier naÃ¯ve bayes dan SVM"},"type":"lvl1","url":"/uts-ppw-klasifikasi","position":0},{"hierarchy":{"lvl1":"klasifikasi berita dengan ekstraksi fitur model topik modelling dengan classifier naÃ¯ve bayes dan SVM"},"content":"!pip install gensim\n\n\n\n!pip install gensim matplotlib seaborn scikit-learn pandas numpy\n\n\n\n!pip install umap-learn\n\n\n\nimport pandas as pd\nimport numpy as np\nimport re\nimport nltk\nfrom gensim import corpora, models\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Download stopword bahasa Indonesia\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('indonesian')\n\n\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n\n\n# Baca data\ndf_path = \"/content/drive/MyDrive/PPW/UTS/Berita.csv\"\ndf = pd.read_csv(df_path)\n\n# Cek kolom\ndf.head(20)\n\n\n\ndf['text'] = df['judul'].astype(str) + \" \" + df['berita'].astype(str)\nprint(\"Contoh teks gabungan:\\n\")\nprint(df['text'].iloc[0])\n\n\n\nCleaning text\n\ndef clean_text(text):\n    # ubah ke huruf kecil\n    text = text.lower()\n    # hilangkan karakter selain huruf\n    text = re.sub(r'[^a-z\\s]', '', text)\n    # hapus stopword\n    text = ' '.join([word for word in text.split() if word not in stop_words])\n    # hapus spasi berlebih\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\n# Terapkan fungsi ke kolom text\ndf['clean_text'] = df['text'].apply(clean_text)\n\n# Tampilkan hasil\ndf[['judul', 'clean_text']].head(3)\n\n\n\nTokenisasi\n\ntexts = [row.split() for row in df['clean_text']]\ndictionary = corpora.Dictionary(texts)\ncorpus = [dictionary.doc2bow(text) for text in texts]\n\nprint(\"Jumlah dokumen:\", len(corpus))\nprint(\"Kata unik dalam kamus:\", len(dictionary))\n\n\n\nModel LDA\n\nnum_topics = 120  # jumlah topik, bisa disesuaikan\nlda_model = models.LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary, passes=10, random_state=42)\n\n# Tampilkan daftar topik\nfor idx, topic in lda_model.print_topics(-1):\n    print(f\"Topik {idx+1}:\\n{topic}\\n\")\n\n\n\nSetiap dokumen menjadi vektor topik\n\nnum_topics = 120\nlda_model = models.LdaModel(\n    corpus=corpus,\n    num_topics=num_topics,\n    id2word=dictionary,\n    passes=10,\n    random_state=42\n)\n\ndef get_topic_vector(lda_model, corpus):\n    topic_features = []\n    for doc in corpus:\n        topic_dist = [0] * lda_model.num_topics\n        for topic_id, prob in lda_model.get_document_topics(doc):\n            topic_dist[topic_id] = prob\n        topic_features.append(topic_dist)\n    return np.array(topic_features)\n\nX_topics = get_topic_vector(lda_model, corpus)\nprint(\"Bentuk matriks fitur:\", X_topics.shape)\n\ntopic_cols = [f\"Topik_{i+1}\" for i in range(num_topics)]\ndf_topics = pd.DataFrame(X_topics, columns=topic_cols)\ndf_final = pd.concat([df[['judul', 'kategori']], df_topics], axis=1)\n\nprint(df_final.head())\n\n\n\nnum_topics = 132\nlda_model = models.LdaModel(\n    corpus=corpus,\n    num_topics=num_topics,\n    id2word=dictionary,\n    passes=10,\n    random_state=42\n)\n\n\ndef get_topic_vector(lda_model, corpus):\n    topic_features = []\n    for doc in corpus:\n        topic_dist = [0] * lda_model.num_topics\n        for topic_id, prob in lda_model.get_document_topics(doc):\n            topic_dist[topic_id] = prob\n        topic_features.append(topic_dist)\n    return np.array(topic_features)\n\nX_topics = get_topic_vector(lda_model, corpus)\nprint(\"Bentuk matriks fitur:\", X_topics.shape)  # Harus (1500, 132)\n\ny = df['kategori']\n\nnb = MultinomialNB()\nnb.fit(X_topics, y)\ny_pred_nb = nb.predict(X_topics)\n\nprint(\"\\n=== HASIL KLASIFIKASI: NAÃVE BAYES ===\")\nprint(\"Akurasi NaÃ¯ve Bayes:\", round(accuracy_score(y, y_pred_nb) * 100, 2), \"%\")\nprint(classification_report(y, y_pred_nb))\n\n# ======================================================\n# 9. Klasifikasi SVM (Linear Support Vector Machine)\n# ======================================================\nsvm = LinearSVC(random_state=42)\nsvm.fit(X_topics, y)\ny_pred_svm = svm.predict(X_topics)\n\nprint(\"\\n=== HASIL KLASIFIKASI: SVM ===\")\nprint(\"Akurasi SVM:\", round(accuracy_score(y, y_pred_svm) * 100, 2), \"%\")\nprint(classification_report(y, y_pred_svm))\n\n# ======================================================\n# 10. Gabungkan ke DataFrame dan Simpan\n# ======================================================\ntopic_cols = [f\"Topik_{i+1}\" for i in range(num_topics)]\ndf_topics = pd.DataFrame(X_topics, columns=topic_cols)\n\ndf_final = pd.concat([df[['judul', 'kategori']], df_topics], axis=1)\ndf_final['Prediksi_NB'] = y_pred_nb\ndf_final['Prediksi_SVM'] = y_pred_svm\n\nprint(\"\\nJumlah data akhir:\", df_final.shape)\nprint(df_final.head())\n\n# Simpan ke Google Drive\ndf_final.to_csv('/content/drive/MyDrive/PPW/UTS/hasil_klasifikasi_132topik.csv', index=False)\nprint(\"\\nâœ… File 'hasil_klasifikasi_132topik.csv' berhasil disimpan di Google Drive.\")\n\n\n\nnum_topics = 148\nlda_model = models.LdaModel(\n    corpus=corpus,\n    num_topics=num_topics,\n    id2word=dictionary,\n    passes=10,\n    random_state=42\n)\n\n# ======================================================\n# 6. Ubah Dokumen ke Vektor Topik\n# ======================================================\ndef get_topic_vector(lda_model, corpus):\n    topic_features = []\n    for doc in corpus:\n        topic_dist = [0] * lda_model.num_topics\n        for topic_id, prob in lda_model.get_document_topics(doc):\n            topic_dist[topic_id] = prob\n        topic_features.append(topic_dist)\n    return np.array(topic_features)\n\nX_topics = get_topic_vector(lda_model, corpus)\nprint(\"Bentuk matriks fitur:\", X_topics.shape)  # Harus (1500, 148)\n\n# ======================================================\n# 7. Ambil Label Target\n# ======================================================\ny = df['kategori']\n\n# ======================================================\n# 8. Klasifikasi NaÃ¯ve Bayes\n# ======================================================\nnb = MultinomialNB()\nnb.fit(X_topics, y)\ny_pred_nb = nb.predict(X_topics)\n\nprint(\"\\n=== HASIL KLASIFIKASI: NAÃVE BAYES ===\")\nprint(\"Akurasi NaÃ¯ve Bayes:\", round(accuracy_score(y, y_pred_nb) * 100, 2), \"%\")\nprint(classification_report(y, y_pred_nb))\n\n# ======================================================\n# 9. Klasifikasi SVM (Linear Support Vector Machine)\n# ======================================================\nsvm = LinearSVC(random_state=42)\nsvm.fit(X_topics, y)\ny_pred_svm = svm.predict(X_topics)\n\nprint(\"\\n=== HASIL KLASIFIKASI: SVM ===\")\nprint(\"Akurasi SVM:\", round(accuracy_score(y, y_pred_svm) * 100, 2), \"%\")\nprint(classification_report(y, y_pred_svm))\n\n# ======================================================\n# 10. Gabungkan ke DataFrame dan Simpan\n# ======================================================\ntopic_cols = [f\"Topik_{i+1}\" for i in range(num_topics)]\ndf_topics = pd.DataFrame(X_topics, columns=topic_cols)\n\ndf_final = pd.concat([df[['judul', 'kategori']], df_topics], axis=1)\ndf_final['Prediksi_NB'] = y_pred_nb\ndf_final['Prediksi_SVM'] = y_pred_svm\n\nprint(\"\\nJumlah data akhir:\", df_final.shape)\nprint(df_final.head())\n\n# Simpan ke Google Drive\ndf_final.to_csv('/content/drive/MyDrive/PPW/UTS/hasil_klasifikasi_148topik.csv', index=False)\nprint(\"\\nâœ… File 'hasil_klasifikasi_148topik.csv' berhasil disimpan di Google Drive.\")\n\n\n\ndef get_topic_vector(lda_model, corpus):\n    topic_features = []\n    for doc in corpus:\n        topic_dist = [0] * lda_model.num_topics\n        for topic_id, prob in lda_model.get_document_topics(doc):\n            topic_dist[topic_id] = prob\n        topic_features.append(topic_dist)\n    return np.array(topic_features)\n\n\ntopic_nums = [120, 132, 148]\nnb_scores = []\nsvm_scores = []\n\nfor n_topics in topic_nums:\n    print(f\"\\n=== Jumlah topik: {n_topics} ===\")\n    lda_model = models.LdaModel(corpus, num_topics=n_topics, id2word=dictionary, passes=10, random_state=42)\n    X_topics = get_topic_vector(lda_model, corpus)\n    y = df['kategori']\n\n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(X_topics, y, test_size=0.2, random_state=42)\n\n    # Naive Bayes\n    nb = MultinomialNB()\n    nb.fit(X_train, y_train)\n    y_pred_nb = nb.predict(X_test)\n    acc_nb = accuracy_score(y_test, y_pred_nb)\n    nb_scores.append(acc_nb)\n\n    # SVM\n    svm = LinearSVC()\n    svm.fit(X_train, y_train)\n    y_pred_svm = svm.predict(X_test)\n    acc_svm = accuracy_score(y_test, y_pred_svm)\n    svm_scores.append(acc_svm)\n\n    print(f\"Akurasi NaÃ¯ve Bayes: {acc_nb:.4f}\")\n    print(f\"Akurasi SVM: {acc_svm:.4f}\")\n\nplt.figure(figsize=(8,5))\nplt.plot(topic_nums, nb_scores, marker='o', label='NaÃ¯ve Bayes')\nplt.plot(topic_nums, svm_scores, marker='s', label='SVM')\nplt.title('Perbandingan Akurasi Berdasarkan Jumlah Topik LDA')\nplt.xlabel('Jumlah Topik')\nplt.ylabel('Akurasi')\nplt.legend()\nplt.grid(True)\nplt.show()\n\nhasil = pd.DataFrame({\n    'Jumlah Topik': topic_nums,\n    'Akurasi NaÃ¯ve Bayes': nb_scores,\n    'Akurasi SVM': svm_scores\n})\n\nprint(\"\\n=== Ringkasan Akurasi ===\")\nprint(hasil)\n\n\n\n\n\n\n\nTarget(label berita)\n\nif 'kategori' not in df.columns:\n    raise ValueError(\"Kolom 'kategori' tidak ditemukan di dataset.\")\ny = df['kategori']\nprint(\"Contoh label:\", y.unique())\n\n\n\nKlasifikasi NB\n\nnb = MultinomialNB()\nnb.fit(X_topics, y)\ny_pred_nb = nb.predict(X_topics)\n\nprint(\"=== Hasil NaÃ¯ve Bayes ===\")\nprint(\"Akurasi:\", accuracy_score(y, y_pred_nb))\nprint(classification_report(y, y_pred_nb))\n\n\n\nKlasifikasi SVM\n\nsvm = LinearSVC()\nsvm.fit(X_topics, y)\ny_pred_svm = svm.predict(X_topics)\n\nprint(\"=== Hasil SVM ===\")\nprint(\"Akurasi:\", accuracy_score(y, y_pred_svm))\nprint(classification_report(y, y_pred_svm))\n\n\n\nhasil = pd.DataFrame({\n    'Judul': df['judul'],\n    'Kategori_Asli': y,\n    'Prediksi_NB': y_pred_nb,\n    'Prediksi_SVM': y_pred_svm\n})\n\nhasil.to_csv('/content/drive/MyDrive/PPW/UTS/hasil_klasifikasi2_1500.csv', index=False)\nprint(\"âœ… File hasil_klasifikasi_1500.csv berhasil disimpan di Google Drive.\")\n\n","type":"content","url":"/uts-ppw-klasifikasi","position":1},{"hierarchy":{"lvl1":"WebGoogle-10txt"},"type":"lvl1","url":"/webgoogle-10txt","position":0},{"hierarchy":{"lvl1":"WebGoogle-10txt"},"content":"import pandas as pd\nimport numpy as np\nimport networkx as nx\nimport matplotlib.pyplot as plt\nfrom IPython.display import display\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n\n\nnama_file = '/content/drive/MyDrive/PPW/web-Google_10k.txt'\n\n\n\ndata_node = pd.read_csv(\n    nama_file,\n    sep='\\t',          # data dipisahkan oleh tab\n    comment='#',       # abaikan baris yang diawali tanda #\n    names=['FromNodeId', 'ToNodeId'],  # nama kolom\n    # header=0           # tidak ada header di baris pertama setelah komentar\n)\n\n\n\ndisplay(data_node)\n\n\n\n","type":"content","url":"/webgoogle-10txt","position":1},{"hierarchy":{"lvl1":"WebGoogle-10txt","lvl2":"1. Membangun Graph Menggunakan NetworkX"},"type":"lvl2","url":"/webgoogle-10txt#id-1-membangun-graph-menggunakan-networkx","position":2},{"hierarchy":{"lvl1":"WebGoogle-10txt","lvl2":"1. Membangun Graph Menggunakan NetworkX"},"content":"\n\ngraph = nx.DiGraph()\nedge = list(zip(data_node['FromNodeId'], data_node['ToNodeId']))\ngraph.add_edges_from(edge)\n\nprint(f\"Jumlah node : {graph.number_of_nodes()}\")\nprint(f\"Jumlah edge : {graph.number_of_edges()}\")\n\n\n\n# Visualisasi Graph\nplt.figure(figsize=(16, 12))\nplt.title(\"Visualisasi Graph (10.000 Node dengan 78.323 Edge)\", fontsize=14, fontweight='bold')\n\n# layout efisien untuk dataset besar\npos = nx.spring_layout(graph, seed=42, k=0.15, iterations=20)\n\n# gambar node dan edge\nnx.draw_networkx_nodes(graph, pos, node_color='skyblue', node_size=10, alpha=0.8)\n# nx.draw_networkx_edges(graph, pos, edge_color='black', arrows=False, alpha=0.3)\nnx.draw_networkx_edges(graph, pos, edge_color='black', arrowstyle='->', arrowsize=4, alpha=0.3)\n\n# label, bisa ditampilkan (resiko crash atau hang karena 10.000 node). aktifkan :\n# nx.draw_networkx_labels(graph, pos, font_size=6, font_color='black')\n\nplt.axis('off')\nplt.show()\n\n\n\n","type":"content","url":"/webgoogle-10txt#id-1-membangun-graph-menggunakan-networkx","position":3},{"hierarchy":{"lvl1":"WebGoogle-10txt","lvl2":"2. Membentuk Matriks Adjacency"},"type":"lvl2","url":"/webgoogle-10txt#id-2-membentuk-matriks-adjacency","position":4},{"hierarchy":{"lvl1":"WebGoogle-10txt","lvl2":"2. Membentuk Matriks Adjacency"},"content":"\n\n# note: all matriks berukuran 10.000 x 10.000, sangat besar untuk ditampilkan\n# jadi ditampilkan sebagian (misalnya 30x30)\nnode = list(graph.nodes())[:30]\nA = nx.to_numpy_array(graph, nodelist=node, dtype=int)\n\ndata_node_A = pd.DataFrame(A, index=node, columns=node)\nprint(\"Matriks Adjacency (A) [30x30]:\")\ndisplay(data_node_A)\n\n\n\n\n\n","type":"content","url":"/webgoogle-10txt#id-2-membentuk-matriks-adjacency","position":5},{"hierarchy":{"lvl1":"WebGoogle-10txt","lvl2":"3. Membentuk Matriks Probabilitas Transisi Baris-Stokastik"},"type":"lvl2","url":"/webgoogle-10txt#id-3-membentuk-matriks-probabilitas-transisi-baris-stokastik","position":6},{"hierarchy":{"lvl1":"WebGoogle-10txt","lvl2":"3. Membentuk Matriks Probabilitas Transisi Baris-Stokastik"},"content":"\n\nadj_full = nx.to_numpy_array(graph, dtype=float)\nn = adj_full.shape[0]\n\n# tangani dangling node (node tanpa outlink)\nout_degree = adj_full.sum(axis=1)\nfor i in range(n):\n    if out_degree[i] == 0:\n        adj_full[i, :] = 1.0\n\n# matriks transisi baris-stokastik\nP = adj_full / adj_full.sum(axis=1, keepdims=True)\n\n# tampilan sebagian matriks P\ndata_node_P = pd.DataFrame(P[:30, :30], columns=range(30), index=range(30))\nprint(\"Matriks Transisi (P) [30x30]:\")\ndisplay(data_node_P)\n\n\n\n\n\n","type":"content","url":"/webgoogle-10txt#id-3-membentuk-matriks-probabilitas-transisi-baris-stokastik","position":7},{"hierarchy":{"lvl1":"WebGoogle-10txt","lvl2":"4. Membentuk Matriks Kolom-Stokastik"},"type":"lvl2","url":"/webgoogle-10txt#id-4-membentuk-matriks-kolom-stokastik","position":8},{"hierarchy":{"lvl1":"WebGoogle-10txt","lvl2":"4. Membentuk Matriks Kolom-Stokastik"},"content":"\n\n# M = Páµ€ (kolom-stokastik)\nM = P.T\ndata_node_M = pd.DataFrame(M[:30, :30], columns=range(30), index=range(30))\nprint(\"Matriks M = Páµ€ [30x30]:\")\ndisplay(data_node_M)\n\n\n\n\n\n","type":"content","url":"/webgoogle-10txt#id-4-membentuk-matriks-kolom-stokastik","position":9},{"hierarchy":{"lvl1":"WebGoogle-10txt","lvl2":"5. Menghitung PageRank Iteratif (Manual)"},"type":"lvl2","url":"/webgoogle-10txt#id-5-menghitung-pagerank-iteratif-manual","position":10},{"hierarchy":{"lvl1":"WebGoogle-10txt","lvl2":"5. Menghitung PageRank Iteratif (Manual)"},"content":"\n\ndef pagerank_iteratif(M, nodelist, d=0.85, max_iter=100, tol=1e-6):\n    n = M.shape[0]\n    r = np.ones(n) / n\n    teleport = (1 - d) / n\n\n    for i in range(max_iter):\n        r_new = d * M @ r + teleport\n        # debugging aja\n        indeks_top_node = np.argmax(r_new)  # indeks node dengan PageRank tertinggi\n        top_node = nodelist[indeks_top_node] # node dengan score pagerank tertinggi\n        print(f\"Iterasi {i+1}: Node {top_node} dengan PageRank {r_new[indeks_top_node]:.6f}\")\n        if np.linalg.norm(r_new - r, 1) < tol:\n            # debugging aja\n            print(f\"Konvergen setelah {i+1} iterasi\")\n            break\n        r = r_new\n    return r\n\nprint(\"Menghitung PageRank (butuh waktu karena 10.000 node):\")\n\nurutan_node = list(graph.nodes())\nr = pagerank_iteratif(M, nodelist=urutan_node, d=0.85, max_iter=100, tol=1e-6)\n\n\n\n","type":"content","url":"/webgoogle-10txt#id-5-menghitung-pagerank-iteratif-manual","position":11},{"hierarchy":{"lvl1":"WebGoogle-10txt","lvl2":"6. Menampilkan dan Menyimpan Hasil PageRank-semua page"},"type":"lvl2","url":"/webgoogle-10txt#id-6-menampilkan-dan-menyimpan-hasil-pagerank-semua-page","position":12},{"hierarchy":{"lvl1":"WebGoogle-10txt","lvl2":"6. Menampilkan dan Menyimpan Hasil PageRank-semua page"},"content":"\n\nscore_pagerank = pd.DataFrame({\n    'node': list(graph.nodes()),\n    'pagerank': r\n})\n\n# mengurutkan node dengan nilai PageRank tertinggi ke terendah\ndata_pagerank_node = score_pagerank.sort_values(by='pagerank', ascending=False).reset_index(drop=True)\nprint(\"Data Node dan Score PageRank:\")\ndisplay(data_pagerank_node)\n\n\n\n\n\ndata_pagerank_node.to_csv(\"/content/drive/MyDrive/PPW/PPW_Tugas6_WebGoogle10K_PageRank(Manual).csv\", index=False)\n\n\n\n","type":"content","url":"/webgoogle-10txt#id-6-menampilkan-dan-menyimpan-hasil-pagerank-semua-page","position":13},{"hierarchy":{"lvl1":"WebGoogle-10txt","lvl2":"6. Menampilkan dan Menyimpan Hasil PageRank - 5 page"},"type":"lvl2","url":"/webgoogle-10txt#id-6-menampilkan-dan-menyimpan-hasil-pagerank-5-page","position":14},{"hierarchy":{"lvl1":"WebGoogle-10txt","lvl2":"6. Menampilkan dan Menyimpan Hasil PageRank - 5 page"},"content":"\n\nlima_node_penting = data_pagerank_node.head(5)\nprint(\"5 Node dengan Nilai PageRank Tertinggi:\")\ndisplay(lima_node_penting)\n\n\n\n\n\n","type":"content","url":"/webgoogle-10txt#id-6-menampilkan-dan-menyimpan-hasil-pagerank-5-page","position":15},{"hierarchy":{"lvl1":"WebGoogle-10txt","lvl2":"7. Perbandingan: Menghitung dan Menampilkan PageRank Menggunakan NetworkX - semua node"},"type":"lvl2","url":"/webgoogle-10txt#id-7-perbandingan-menghitung-dan-menampilkan-pagerank-menggunakan-networkx-semua-node","position":16},{"hierarchy":{"lvl1":"WebGoogle-10txt","lvl2":"7. Perbandingan: Menghitung dan Menampilkan PageRank Menggunakan NetworkX - semua node"},"content":"\n\nscore_pagerank_networkx = nx.pagerank(graph, alpha=0.85)\ndata_pagerank_node_networkx = pd.DataFrame(list(score_pagerank_networkx.items()), columns=['node', 'pagerank'])\n\n# node score pagerank tertinggi ke terendah\ndata_node_pagerank_networkx_urut = data_pagerank_node_networkx.sort_values(by='pagerank', ascending=False).reset_index(drop=True)\nprint(\"Perbandingan Menggunakan NetworkX - Data Node dan Score PageRank:\")\ndisplay(data_node_pagerank_networkx_urut)\n\n\n\n\n\n# OPSIONAL\n# kalau mau simpan hasil data node dan score pagerank pembanding menggunakan NetworkX, aktifkan :\n# data_node_pagerank_networkx_urut.to_csv(\"PPW_Tugas6_WebGoogle10K_PageRank(NetworkX).csv\", index=False)\n\n\n\n","type":"content","url":"/webgoogle-10txt#id-7-perbandingan-menghitung-dan-menampilkan-pagerank-menggunakan-networkx-semua-node","position":17},{"hierarchy":{"lvl1":"WebGoogle-10txt","lvl2":"7. Perbandingan: Menghitung dan Menampilkan PageRank Menggunakan NetworkX - 5 node"},"type":"lvl2","url":"/webgoogle-10txt#id-7-perbandingan-menghitung-dan-menampilkan-pagerank-menggunakan-networkx-5-node","position":18},{"hierarchy":{"lvl1":"WebGoogle-10txt","lvl2":"7. Perbandingan: Menghitung dan Menampilkan PageRank Menggunakan NetworkX - 5 node"},"content":"\n\nlima_node_penting_networkx = data_node_pagerank_networkx_urut.head(5)\n\nprint(\"Perbandingan Menggunakan NetworkX - 5 Node dengan Nilai PageRank Tertinggi:\")\ndisplay(lima_node_penting_networkx)\n\n\n\n\n\n","type":"content","url":"/webgoogle-10txt#id-7-perbandingan-menghitung-dan-menampilkan-pagerank-menggunakan-networkx-5-node","position":19},{"hierarchy":{"lvl1":"WebGoogle-10txt","lvl2":"8. Visualisasi Hubungan Node Penting dengan Node Terhubung"},"type":"lvl2","url":"/webgoogle-10txt#id-8-visualisasi-hubungan-node-penting-dengan-node-terhubung","position":20},{"hierarchy":{"lvl1":"WebGoogle-10txt","lvl2":"8. Visualisasi Hubungan Node Penting dengan Node Terhubung"},"content":"\n\n# simpan node dengan PageRank tertinggi ke variabel\nnode_penting = int(lima_node_penting.iloc[0]['node'])\nprint(f\"Node dengan nilai PageRank tertinggi: {node_penting}\")\n\n# node yang terhubung dengan node penting (incoming dan outgoing)\nneighbors_out = list(graph.successors(node_penting))   # node yang ditaut oleh node_penting\nneighbors_in = list(graph.predecessors(node_penting))  # node yang menaut ke node_penting\n\n# menggabungkan semua node yang relevan\nnode_terhubung = set(neighbors_out + neighbors_in + [node_penting])\n\n# buat subgraph dari node-node tersebut\nsubgraph_node_terhubung = graph.subgraph(node_terhubung)\n\n\n\n# Visualisasi Graph\nplt.figure(figsize=(12, 9))\nplt.title(f\"Graph Keterhubungan Node Penting ({node_penting})\", fontsize=14, fontweight='bold')\n\npos = nx.spring_layout(subgraph_node_terhubung, seed=42, k=0.3)\n\n# warna berbeda untuk node penting\nwarna_node = ['red' if node == node_penting else 'skyblue' for node in subgraph_node_terhubung.nodes()]\n\nnx.draw_networkx_nodes(subgraph_node_terhubung, pos, node_color=warna_node, node_size=500, alpha=0.9)\nnx.draw_networkx_edges(subgraph_node_terhubung, pos, edge_color='black', arrowstyle='->', arrowsize=7, alpha=0.6)\nnx.draw_networkx_labels(subgraph_node_terhubung, pos, font_size=7, font_color='black')\n\nplt.axis('off')\nplt.show()\n\n\n\n# Data Keterhubungan Node Penting\nprint(f\"Jumlah total node yang terhubung dengan {node_penting}: {len(node_terhubung)}\")\n\n# DataFrame untuk incoming dan outgoing edges\ndf_in = pd.DataFrame({\n    'FromNodeId': neighbors_in,\n    'ToNodeId': [node_penting] * len(neighbors_in)\n})\ndf_out = pd.DataFrame({\n    'FromNodeId': [node_penting] * len(neighbors_out),\n    'ToNodeId': neighbors_out\n})\n\n# gabungkan kedua arah hubungan\ndata_node_terhubung = pd.concat([df_in, df_out], ignore_index=True)\n\nprint(f\"Daftar Node yang Terhubung dengan Node {node_penting} (Incoming dan Outgoing):\")\ndisplay(data_node_terhubung)\n\n\n\n","type":"content","url":"/webgoogle-10txt#id-8-visualisasi-hubungan-node-penting-dengan-node-terhubung","position":21},{"hierarchy":{"lvl1":"Web Mining"},"type":"lvl1","url":"/webmining","position":0},{"hierarchy":{"lvl1":"Web Mining"},"content":"","type":"content","url":"/webmining","position":1},{"hierarchy":{"lvl1":"Web Mining","lvl2":"1. Pengertian Web Mining"},"type":"lvl2","url":"/webmining#id-1-pengertian-web-mining","position":2},{"hierarchy":{"lvl1":"Web Mining","lvl2":"1. Pengertian Web Mining"},"content":"Web mining adalah cara untuk mengambil informasi atau pola penting dari data yang ada di internet. Data ini bisa berupa teks, gambar, video, audio, atau bahkan catatan bagaimana orang berinteraksi dengan situs web, misalnya halaman yang dikunjungi atau tombol yang diklik.\n\nTujuan web mining adalah mengubah data mentah menjadi informasi yang berguna, seperti:\n\nMembantu bisnis memahami perilaku pengguna.\n\nMembuat keputusan yang lebih tepat.\n\nMeningkatkan strategi pemasaran.\n\nWeb mining juga bisa membantu menemukan pola yang sulit dilihat langsung, misalnya tren yang sedang populer atau hubungan antara konten dan cara orang berinteraksi di situs web.","type":"content","url":"/webmining#id-1-pengertian-web-mining","position":3},{"hierarchy":{"lvl1":"Web Mining","lvl2":"2. Tujuan Web Mining"},"type":"lvl2","url":"/webmining#id-2-tujuan-web-mining","position":4},{"hierarchy":{"lvl1":"Web Mining","lvl2":"2. Tujuan Web Mining"},"content":"Tujuan utama dari web mining adalah untuk menghasilkan informasi yang berguna dari data web yang besar dan kompleks. Tujuan tersebut antara lain:\n\nMemahami perilaku pengguna Mengetahui bagaimana orang menelusuri situs web, halaman yang paling sering dikunjungi, konten yang disukai, atau cara mereka berinteraksi.\n\nMendukung pengambilan keputusan berbasis data Memberikan data yang akurat agar perusahaan bisa membuat strategi pemasaran, promosi, pengembangan produk, atau layanan lebih tepat.\n\nPersonalisasi pengalaman pengguna Menyajikan konten atau rekomendasi produk sesuai dengan preferensi dan kebiasaan pengguna sebelumnya.\n\nMengoptimalkan desain dan navigasi situs web Membantu membuat situs lebih mudah digunakan dan lebih efisien.","type":"content","url":"/webmining#id-2-tujuan-web-mining","position":5},{"hierarchy":{"lvl1":"Web Mining","lvl2":"3. Macam-Macam Web Mining"},"type":"lvl2","url":"/webmining#id-3-macam-macam-web-mining","position":6},{"hierarchy":{"lvl1":"Web Mining","lvl2":"3. Macam-Macam Web Mining"},"content":"","type":"content","url":"/webmining#id-3-macam-macam-web-mining","position":7},{"hierarchy":{"lvl1":"Web Mining","lvl3":"A. Web Content Mining (WCM)","lvl2":"3. Macam-Macam Web Mining"},"type":"lvl3","url":"/webmining#a-web-content-mining-wcm","position":8},{"hierarchy":{"lvl1":"Web Mining","lvl3":"A. Web Content Mining (WCM)","lvl2":"3. Macam-Macam Web Mining"},"content":"Web Content Mining berfokus pada ekstraksi informasi dari isi halaman web, termasuk teks, gambar, video, dan audio. Tujuannya adalah mencari informasi penting atau pola tersembunyi di konten web.\n\nContoh Penggunaan:\n\nAnalisis sentimen ulasan produk di situs e-commerce untuk mengetahui kepuasan pelanggan.\n\nIdentifikasi topik trending di media sosial atau portal berita online.\n\nMesin pencari menampilkan hasil yang relevan berdasarkan isi halaman.\n\nTeknik yang Digunakan:\n\nText Mining: menganalisis teks untuk menemukan kata kunci, frekuensi kata, dan pola penting.\n\nNatural Language Processing (NLP): memproses bahasa manusia untuk klasifikasi, ekstraksi informasi, atau analisis sentimen.\n\nImage & Video Analysis: mengambil informasi dari foto atau video, misalnya mengenali objek dalam foto produk.\n\nWeb Scraping: mengumpulkan data dari halaman web secara otomatis.","type":"content","url":"/webmining#a-web-content-mining-wcm","position":9},{"hierarchy":{"lvl1":"Web Mining","lvl3":"B. Web Structure Mining (WSM)","lvl2":"3. Macam-Macam Web Mining"},"type":"lvl3","url":"/webmining#b-web-structure-mining-wsm","position":10},{"hierarchy":{"lvl1":"Web Mining","lvl3":"B. Web Structure Mining (WSM)","lvl2":"3. Macam-Macam Web Mining"},"content":"Web Structure Mining mempelajari struktur hyperlink antar halaman web. Tujuannya adalah memahami struktur situs, halaman mana yang penting, dan bagaimana halaman saling terhubung.\n\nContoh Penggunaan:\n\nAlgoritma PageRank Google menentukan peringkat halaman web berdasarkan kualitas dan jumlah tautan yang masuk.\n\nOptimasi SEO dan pembuatan peta situs untuk navigasi yang lebih baik.\n\nAnalisis jaringan antar situs untuk memahami hubungan antar halaman.\n\nTeknik yang Digunakan:\n\nGraph Theory (Teori Graf): halaman web direpresentasikan sebagai node, dan tautan menjadi edge.\n\nLink Analysis: menilai pentingnya halaman berdasarkan jumlah dan kualitas tautan masuk dan keluar.\n\nNetwork Visualization Tools: visualisasi hubungan antar halaman menggunakan software seperti Gephi atau Pajek.\n\n(Kosala & Blockeel, 2000)","type":"content","url":"/webmining#b-web-structure-mining-wsm","position":11},{"hierarchy":{"lvl1":"Web Mining","lvl3":"C. Web Usage Mining (WUM)","lvl2":"3. Macam-Macam Web Mining"},"type":"lvl3","url":"/webmining#c-web-usage-mining-wum","position":12},{"hierarchy":{"lvl1":"Web Mining","lvl3":"C. Web Usage Mining (WUM)","lvl2":"3. Macam-Macam Web Mining"},"content":"Web Usage Mining menganalisis data log dan interaksi pengguna di situs web untuk memahami perilaku dan preferensi pengguna. Dengan informasi ini, perusahaan dapat menyesuaikan konten dan layanan yang ditawarkan kepada pengunjung.\n\nContoh Penggunaan:\n\nRekomendasi produk berdasarkan riwayat penelusuran dan pembelian pengguna.\n\nPersonalisasi konten untuk meningkatkan keterlibatan pengguna.\n\nAnalisis jalur navigasi untuk meningkatkan desain dan struktur situs web.\n\nTeknik yang Digunakan:\n\nData Preprocessing: membersihkan data log dari error, duplikasi, atau noise agar analisis lebih akurat.\n\nClustering & Classification: mengelompokkan pengguna dengan perilaku serupa dan memprediksi kategori perilaku baru.\n\nAssociation Rule Mining: menemukan hubungan antara item atau aktivitas yang sering terjadi bersamaan.\n\nSequential Pattern Mining: mengidentifikasi urutan perilaku umum pengguna.\n\nWeb Analytics Tools: mengumpulkan dan menganalisis data secara otomatis menggunakan software seperti Google Analytics atau Matomo.","type":"content","url":"/webmining#c-web-usage-mining-wum","position":13},{"hierarchy":{"lvl1":"Web Mining","lvl2":"4. Daftar Pustaka"},"type":"lvl2","url":"/webmining#id-4-daftar-pustaka","position":14},{"hierarchy":{"lvl1":"Web Mining","lvl2":"4. Daftar Pustaka"},"content":"J. Srivastava, P. Desikan, dan V. Kumar, â€œWeb Mining â€” Concepts, Applications, and Research Directions,â€ Proc. 10th ACM SIGKDD Int. Conf. Knowl. Discov. Data Min., 2004. [Online]. \n\nTersedia di sini\n\nA. Herrouz, â€œOverview of Web Content Mining Tools,â€ arXiv, 2013. [Online]. \n\nTersedia di sini\n\nR. Kosala dan H. Blockeel, â€œWeb Mining Research: A Survey,â€ ACM SIGKDD Explor. Newsl., 2000. [Online]. \n\nTersedia di sini","type":"content","url":"/webmining#id-4-daftar-pustaka","position":15},{"hierarchy":{"lvl1":"Web Usage"},"type":"lvl1","url":"/webusage","position":0},{"hierarchy":{"lvl1":"Web Usage"},"content":"import pandas as pd\nimport numpy as np\nimport networkx as nx\nimport matplotlib.pyplot as plt\nfrom IPython.display import display\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n\n\nnama_file = '/content/drive/MyDrive/PPW/webuage.csv'\ndf = pd.read_csv(nama_file, sep=None, engine=\"python\")\n\n\n\ndf\n\n\n\n","type":"content","url":"/webusage","position":1},{"hierarchy":{"lvl1":"Web Usage","lvl2":"1. Normalisasi nama kolom agar mudah diakses"},"type":"lvl2","url":"/webusage#id-1-normalisasi-nama-kolom-agar-mudah-diakses","position":2},{"hierarchy":{"lvl1":"Web Usage","lvl2":"1. Normalisasi nama kolom agar mudah diakses"},"content":"\n\ndf.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\nprint(\"Kolom yang tersedia:\", df.columns.tolist())\n\n\n\n\n\n","type":"content","url":"/webusage#id-1-normalisasi-nama-kolom-agar-mudah-diakses","position":3},{"hierarchy":{"lvl1":"Web Usage","lvl2":"2. Filter"},"type":"lvl2","url":"/webusage#id-2-filter","position":4},{"hierarchy":{"lvl1":"Web Usage","lvl2":"2. Filter"},"content":"\n\nfiltered = df[\n    (df[\"request_method\"] == \"GET\") &\n    (df[\"request_uri\"].isin([\"/faq.html\", \"/index.html\"])) &\n    (df[\"status\"] == 200)\n]\n\n\n\nprint(f\"Jumlah data hasil filter: {len(filtered)}\")\ndisplay(filtered.head())\n\n\n\n\n\noutput_file = \"hasil_filtered_webuage.csv\"\nfiltered.to_csv(output_file, index=False)\n\n\n\n","type":"content","url":"/webusage#id-2-filter","position":5},{"hierarchy":{"lvl1":"Web Usage","lvl2":"3. Cek Duplikasi"},"type":"lvl2","url":"/webusage#id-3-cek-duplikasi","position":6},{"hierarchy":{"lvl1":"Web Usage","lvl2":"3. Cek Duplikasi"},"content":"\n\ndf.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n\n#Tampilkan jumlah duplikasi berdasarkan IP\ndupe_count = df[\"remote_host\"].value_counts()\nprint(\"Jumlah kemunculan setiap IP:\")\ndisplay(dupe_count.head(10))  # tampilkan 10 IP teratas\n\n#Tampilkan hanya IP yang muncul lebih dari 1 kali\ndupe_ips = dupe_count[dupe_count > 1]\nprint(f\"Jumlah IP yang duplikat: {len(dupe_ips)}\")\ndisplay(dupe_ips)\n\n\n\n\n\n\n\n\n\n# === Konversi kolom waktu menjadi datetime agar bisa diurutkan ===\ndf[\"request_time\"] = pd.to_datetime(df[\"request_time\"], errors=\"coerce\")\n\n# === Urutkan berdasarkan IP dan waktu ===\ndf_sorted = df.sort_values(by=[\"remote_host\", \"request_time\"]).reset_index(drop=True)\n\n# === Filter sesuai kriteria ===\nfiltered = df_sorted[\n    (df_sorted[\"request_method\"] == \"GET\") &\n    (df_sorted[\"request_uri\"].isin([\"/faq.html\", \"/index.html\"])) &\n    (df_sorted[\"status\"] == 200)\n]\n\n# === Urutkan hasil akhir berdasarkan IP (Remote Host) ===\nfiltered = filtered.sort_values(by=\"remote_host\").reset_index(drop=True)\n\n\n\n# === Tampilkan hasil ===\nprint(f\"Jumlah data hasil filter: {len(filtered)}\")\ndisplay(filtered.head(20))  # tampilkan 20 baris teratas\n\n# === 9ï¸âƒ£ Simpan hasil ke file baru dan download ===\noutput_file = \"filtered_log_sorted_by_ip.csv\"\nfiltered.to_csv(output_file, index=False)\n\n\n\n\n\n","type":"content","url":"/webusage#id-3-cek-duplikasi","position":7},{"hierarchy":{"lvl1":"Web Usage","lvl2":"4. Data lengkap yang mengandung duplikasi"},"type":"lvl2","url":"/webusage#id-4-data-lengkap-yang-mengandung-duplikasi","position":8},{"hierarchy":{"lvl1":"Web Usage","lvl2":"4. Data lengkap yang mengandung duplikasi"},"content":"\n\ndf.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n\ndupe_count = df[\"remote_host\"].value_counts()\nprint(\"ðŸ“Š Jumlah kemunculan setiap IP:\")\ndisplay(dupe_count.head(10))  # tampilkan 10 IP teratas\n\ndupe_ips = dupe_count[dupe_count > 1]\nprint(f\"\\nðŸ” Jumlah IP yang duplikat: {len(dupe_ips)}\")\ndisplay(dupe_ips)\n\nduplicates = df[df[\"remote_host\"].isin(dupe_ips.index)]\nprint(\"\\nðŸ“‹ Data lengkap yang mengandung duplikasi:\")\ndisplay(duplicates.head(20))  # tampilkan 20 baris pertama\n\n\n\n\n\n\n\n\n\n\n\n\n\nnama_file2 = '/content/drive/MyDrive/PPW/webuageOutput /filtered_log_sorted_by_ip.csv'\ndf2 = pd.read_csv(nama_file2, sep=None, engine=\"python\")\n\n\n\n# === Normalisasi nama kolom ===\ndf.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n\n# === Konversi waktu ke datetime ===\ndf[\"request_time\"] = pd.to_datetime(df[\"request_time\"], errors=\"coerce\")\n\n# === Filter hanya method GET dan halaman index/faq ===\nfiltered = df[\n    (df[\"request_method\"] == \"GET\") &\n    (df[\"request_uri\"].isin([\"/faq.html\", \"/index.html\"])) &\n    (df[\"status\"] == 200)\n].copy()\n\n# === Ganti halaman menjadi A/B ===\nfiltered[\"url\"] = filtered[\"request_uri\"].replace({\n    \"/index.html\": \"A\",\n    \"/faq.html\": \"B\"\n})\n\n# === Urutkan berdasarkan IP dan waktu ===\nfiltered = filtered.sort_values(by=[\"remote_host\", \"request_time\"]).reset_index(drop=True)\n\n# === Hitung selisih waktu antarakses (per IP) ===\nfiltered[\"selisih_menit\"] = (\n    filtered.groupby(\"remote_host\")[\"request_time\"]\n    .diff()\n    .dt.total_seconds()\n    .div(60)\n    .fillna(0)\n    .round(2)\n)\n\n# === Buat kolom sesi per IP (gap â‰¥ 30 menit = sesi baru)\nsession_timeout = 30  # menit\nfiltered[\"sesi\"] = (\n    filtered.groupby(\"remote_host\")[\"selisih_menit\"]\n    .transform(lambda x: (x > session_timeout).cumsum() + 1)\n)\n\n# === Pilih kolom utama dan tampilkan ===\nfinal_table = filtered[[\n    \"remote_host\", \"request_time\", \"url\", \"status\",\n    \"size_of_response_(incl._headers)\", \"selisih_menit\", \"sesi\"\n]].rename(columns={\"remote_host\": \"ip\"})\n\nprint(\"Semua data ditampilkan dengan pembagian sesi per 30 menit:\")\ndisplay(final_table.head(20))\n\n# === Simpan hasil ke file CSV ===\noutput_file = \"sessionized_full_log_fixed.csv\"\nfinal_table.to_csv(output_file, index=False)\n\n\n\n\n\n# Pastikan final_table sudah berisi ip, request_time, url, selisih_menit\ndf2 = final_table.copy()\n\n# === Tentukan batas sesi (30 menit) ===\nsession_timeout = pd.Timedelta(minutes=30)\n\n# === Urutkan berdasarkan IP dan waktu ===\ndf2 = df2.sort_values(by=[\"ip\", \"request_time\"]).reset_index(drop=True)\n\n# === Buat kolom sesi berdasarkan gap waktu antar akses per IP ===\ndf2[\"sesi\"] = (\n    df2.groupby(\"ip\")[\"request_time\"]\n    .diff()\n    .gt(session_timeout)\n    .groupby(df2[\"ip\"])\n    .cumsum()\n    .add(1)\n)\n\n# === Tandai halaman A dan B dalam setiap sesi ===\ndf2[\"A\"] = (df2[\"url\"] == \"A\").astype(int)\ndf2[\"B\"] = (df2[\"url\"] == \"B\").astype(int)\n\n# === Rekap per IP dan sesi (gabungkan kunjungan dalam satu periode)\nresult = (\n    df2.groupby([\"ip\", \"sesi\"], as_index=False)[[\"A\", \"B\"]]\n    .max()  # jika pernah ke A atau B dalam sesi tersebut = 1\n)\n\n# === Hapus kolom sesi agar tampil seperti log ringkas ===\nresult = result.drop(columns=[\"sesi\"])\n\n# === Tampilkan hasil akhir ===\nprint(\"Ringkasan kunjungan per IP (setiap gap â‰¥ 30 menit buat baris baru):\")\ndisplay(result.head(20))\n\n# === Simpan ke CSV ===\nresult.to_csv(\"ip_page_session_30min.csv\", index=False)\n\n\n\n\n\n","type":"content","url":"/webusage#id-4-data-lengkap-yang-mengandung-duplikasi","position":9},{"hierarchy":{"lvl1":"Web Usage","lvl2":"5. Mencari IP yang sering muncul lebih dari sekali"},"type":"lvl2","url":"/webusage#id-5-mencari-ip-yang-sering-muncul-lebih-dari-sekali","position":10},{"hierarchy":{"lvl1":"Web Usage","lvl2":"5. Mencari IP yang sering muncul lebih dari sekali"},"content":"\n\n# === Periksa IP yang muncul lebih dari satu kali ===\nip_counts = result[\"ip\"].value_counts()\n\n# Ambil hanya IP yang duplikat (muncul > 1 kali)\ndupe_ips = ip_counts[ip_counts > 1]\nprint(f\"Jumlah IP duplikat: {len(dupe_ips)}\")\ndisplay(dupe_ips)\n\n# === Tampilkan baris lengkap dari IP yang duplikat ===\ndupe_rows = result[result[\"ip\"].isin(dupe_ips.index)]\nprint(\"Data lengkap untuk IP yang duplikat:\")\ndisplay(dupe_rows)\n\n\n\n\n\n\n\n\n","type":"content","url":"/webusage#id-5-mencari-ip-yang-sering-muncul-lebih-dari-sekali","position":11},{"hierarchy":{"lvl1":"Web Usage New"},"type":"lvl1","url":"/webusagenew","position":0},{"hierarchy":{"lvl1":"Web Usage New"},"content":"","type":"content","url":"/webusagenew","position":1},{"hierarchy":{"lvl1":"Web Usage New","lvl2":"1. Import Library"},"type":"lvl2","url":"/webusagenew#id-1-import-library","position":2},{"hierarchy":{"lvl1":"Web Usage New","lvl2":"1. Import Library"},"content":"\n\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\n\n\n\n","type":"content","url":"/webusagenew#id-1-import-library","position":3},{"hierarchy":{"lvl1":"Web Usage New","lvl2":"2. Load & Read Dataset"},"type":"lvl2","url":"/webusagenew#id-2-load-read-dataset","position":4},{"hierarchy":{"lvl1":"Web Usage New","lvl2":"2. Load & Read Dataset"},"content":"\n\nnama_file = '/content/drive/MyDrive/PPW/webuage.csv'\ndf = pd.read_csv(nama_file)\n\n\n\ndisplay(df)\n\n\n\ndf.info()\n\n\n\n","type":"content","url":"/webusagenew#id-2-load-read-dataset","position":5},{"hierarchy":{"lvl1":"Web Usage New","lvl2":"3. Filter Data"},"type":"lvl2","url":"/webusagenew#id-3-filter-data","position":6},{"hierarchy":{"lvl1":"Web Usage New","lvl2":"3. Filter Data"},"content":"\n\nfiltered = df[\n    (df[\"request_method\"].isin([\"GET\", \"POST\"])) &\n    (df[\"request_uri\"].isin([\n        \"/faq.html\",\n        \"/index.html\",\n        \"/somefile.zip\",\n        \"/contribute.txt\",\n        \"/robots.txt\"\n    ])) &\n    (df[\"status\"].isin([200, 301, 404, 206]))\n]\n\n\n\ndisplay(df)\n\n\n\n","type":"content","url":"/webusagenew#id-3-filter-data","position":7},{"hierarchy":{"lvl1":"Web Usage New","lvl2":"4. Konversi Waktu ke Format Datetime"},"type":"lvl2","url":"/webusagenew#id-4-konversi-waktu-ke-format-datetime","position":8},{"hierarchy":{"lvl1":"Web Usage New","lvl2":"4. Konversi Waktu ke Format Datetime"},"content":"\n\nfiltered['request_time'] = pd.to_datetime(\n    filtered['request_time'],\n    errors='coerce',\n    utc=True,\n    infer_datetime_format=True\n)\n\n# Hapus data yang gagal dikonversi (NaT)\ndf_bersih = filtered.dropna(subset=['request_time'])\n\n\n\n# Urutkan data berdasarkan IP dan waktu\ndf_sorted = df_bersih.sort_values(by=['remote_host', 'request_time']).reset_index(drop=True)\n\n\n\ndisplay(df_sorted)\n\n\n\n","type":"content","url":"/webusagenew#id-4-konversi-waktu-ke-format-datetime","position":9},{"hierarchy":{"lvl1":"Web Usage New","lvl2":"5. Identifikasi Sesi User"},"type":"lvl2","url":"/webusagenew#id-5-identifikasi-sesi-user","position":10},{"hierarchy":{"lvl1":"Web Usage New","lvl2":"5. Identifikasi Sesi User"},"content":"\n\n","type":"content","url":"/webusagenew#id-5-identifikasi-sesi-user","position":11},{"hierarchy":{"lvl1":"Web Usage New","lvl3":"1. Setiap Sesi","lvl2":"5. Identifikasi Sesi User"},"type":"lvl3","url":"/webusagenew#id-1-setiap-sesi","position":12},{"hierarchy":{"lvl1":"Web Usage New","lvl3":"1. Setiap Sesi","lvl2":"5. Identifikasi Sesi User"},"content":"\n\nsession_timeout = timedelta(minutes=30)\ndf_sorted['Session Number'] = 0\ndf_sorted['Session ID'] = \"\"\n\n# Hitung sesi per IP berdasarkan selisih waktu antar akses\nfor ip, group in df_sorted.groupby('remote_host'):\n    group = group.sort_values('request_time').copy()\n    waktu_akses = group['request_time'].diff().fillna(pd.Timedelta(seconds=0))\n\n    # Jika selisih waktu antar akses > 30 menit â†’ sesi baru\n    jumlah_sesi = (waktu_akses > session_timeout).cumsum() + 1\n\n    # Simpan hasil sesi ke dataframe utama\n    df_sorted.loc[group.index, 'Session Number'] = jumlah_sesi\n    df_sorted.loc[group.index, 'Session ID'] = ip + \"_S\" + jumlah_sesi.astype(str)\n\n# jumlah sesi = integer\ndf_sorted['Session Number'] = df_sorted['Session Number'].astype(int)\n\n\n\ndisplay(df_sorted)\n\n\n\n","type":"content","url":"/webusagenew#id-1-setiap-sesi","position":13},{"hierarchy":{"lvl1":"Web Usage New","lvl3":"2. Total Sesi Setiap IP","lvl2":"5. Identifikasi Sesi User"},"type":"lvl3","url":"/webusagenew#id-2-total-sesi-setiap-ip","position":14},{"hierarchy":{"lvl1":"Web Usage New","lvl3":"2. Total Sesi Setiap IP","lvl2":"5. Identifikasi Sesi User"},"content":"\n\ntotal_sesi = (\n    df_sorted.groupby('remote_host')['Session ID']\n    .nunique()\n    .reset_index()\n    .rename(columns={'Session ID': 'Total Sessions'})\n)\n\ndf_baru = pd.merge(df_sorted, total_sesi, on='remote_host', how='left')\n\n\n\n# Urutkan berdasarkan jumlah sesi terbanyak\ndf_baru = df_baru.sort_values(\n    by=['Total Sessions', 'remote_host', 'request_time'],\n    ascending=[False, True, True]\n).reset_index(drop=True)\n\n\n\ndisplay(df_baru)\n\n\n\n","type":"content","url":"/webusagenew#id-2-total-sesi-setiap-ip","position":15},{"hierarchy":{"lvl1":"Web Usage New","lvl2":"6. Kunjungan  Web"},"type":"lvl2","url":"/webusagenew#id-6-kunjungan-web","position":16},{"hierarchy":{"lvl1":"Web Usage New","lvl2":"6. Kunjungan  Web"},"content":"\n\nkunjungan_dengan_waktu = (\n    df_baru\n    .groupby(['remote_host', 'request_time', 'request_uri'])\n    .size()\n    .unstack(fill_value=0)\n    .reset_index()   # supaya request_time muncul sebagai kolom\n)\n\n\n\n# Durasi antar halaman dalam menit\ndf_baru['Next Time'] = df_baru.groupby(['remote_host', 'Session Number'])['request_time'].shift(-1)\ndf_baru['Next Request URI'] = df_baru.groupby(['remote_host', 'Session Number'])['request_uri'].shift(-1)\n\ndf_baru['Duration (minutes)'] = (\n    df_baru['Next Time'] - df_baru['request_time']\n).dt.total_seconds() / 60\n\n# Default durasi 30 menit jika tidak ada halaman berikutnya\ndf_baru['Duration (minutes)'] = df_baru['Duration (minutes)'].fillna(30)\n\n\n\ndurasi_kunjungan = df_baru.pivot_table(\n    index='remote_host',\n    columns='request_uri',\n    values='Duration (minutes)',\n    aggfunc='sum',\n    fill_value=0\n)\n\n\n\n","type":"content","url":"/webusagenew#id-6-kunjungan-web","position":17},{"hierarchy":{"lvl1":"Web Usage New","lvl2":"7. Datalog Filter + Durasi"},"type":"lvl2","url":"/webusagenew#id-7-datalog-filter-durasi","position":18},{"hierarchy":{"lvl1":"Web Usage New","lvl2":"7. Datalog Filter + Durasi"},"content":"\n\n# data\nkolom_df = []\nfor page in kunjungan.columns:\n    kolom_df.append(page)\n    kolom_df.append(f\"Lama waktu akses di {page} (menit)\")\n\ndata = pd.DataFrame(index=kunjungan.index)\n\nfor page in kunjungan.columns:\n    data[page] = kunjungan[page]\n    data[f\"Lama waktu akses di {page} (menit)\"] = durasi_kunjungan[page]\n\ndata = data.reset_index()\n\n\n\n# Tambahkan Total Sessions dari data utama\ntotal_sesi_ip = df_baru[['remote_host', 'Total Sessions']].drop_duplicates()\ndata = data.merge(total_sesi_ip, on='remote_host', how='left')\n\n\n\n# Urutkan berdasar Total Sessions\ndata = data.sort_values(by='Total Sessions', ascending=False).reset_index(drop=True)\n\n\n\ndisplay(data)\n\n\n\n# Datalog web setelah pengelompokkan user\n\n# Buat mapping IP unik ke User ID\nip_user = {ip: f\"User {i+1}\" for i, ip in enumerate(data['remote_host'].unique())}\n\n# Tambahkan kolom 'User' berdasarkan mapping\ndata['User'] = data['remote_host'].map(ip_user)\n\n# Urutkan supaya kolom 'User' muncul di paling kiri\nkolom = ['User'] + [col for col in data.columns if col != 'User']\ndf_final = data[kolom]\n\ndisplay(df_final)\n\n\n\n# simpan ke csv\ndf_final.to_csv('PPW_Tugas7_LogFile(WebUsageMining).csv', index=False)\n\n\n\n# Mulai dari df_final, ambil remote_host & request_uri\nsummary_data = df_sorted[['remote_host', 'request_time','request_uri']].copy()\n\n# Buat kolom biner\nsummary_data['faq.html']       = summary_data['request_uri'].apply(lambda x: 1 if 'faq.html' in x else 0)\nsummary_data['index.html']     = summary_data['request_uri'].apply(lambda x: 1 if 'index.html' in x else 0)\nsummary_data['somefile.zip']   = summary_data['request_uri'].apply(lambda x: 1 if 'somefile.zip' in x else 0)\nsummary_data['contribute.txt'] = summary_data['request_uri'].apply(lambda x: 1 if 'contribute.txt' in x else 0)\nsummary_data['robots.txt']     = summary_data['request_uri'].apply(lambda x: 1 if 'robots.txt' in x else 0)\n\n# Hapus kolom request_uri karena sudah jadi biner\nsummary_data_clean = summary_data.drop(columns=['request_uri'])\n\n# Tampilkan hasil\ndisplay(summary_data_clean)\n\n\n\n\n# Hitung modus berdasarkan halaman yang dikunjungi\nmodus_per_user = (\n    summary_data_clean.groupby(['remote_host'])[['faq.html', 'index.html', 'somefile.zip', 'contribute.txt', 'robots.txt']]\n    .agg(lambda x: x.mode().iloc[0] if not x.mode().empty else 0)\n    .reset_index()\n)\n\ndisplay(modus_per_user)\n\n","type":"content","url":"/webusagenew#id-7-datalog-filter-durasi","position":19},{"hierarchy":{"lvl1":"Klasifikasi dengan ektraksi fitur menggunakan  Latent Dirichlet Allocation"},"type":"lvl1","url":"/ektraksi-lda","position":0},{"hierarchy":{"lvl1":"Klasifikasi dengan ektraksi fitur menggunakan  Latent Dirichlet Allocation"},"content":"!pip install gensim\n\n\n\n!pip install gensim matplotlib seaborn scikit-learn pandas numpy\n\n\n\n!pip install umap-learn\n\n\n\nimport pandas as pd\nimport numpy as np\nimport gensim\nfrom gensim import corpora, models\nfrom gensim.models.ldamodel import LdaModel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n\n\n# Baca data\ndf_path = \"/content/drive/MyDrive/PPW/output/berita_cnn_tokens.csv\"\ndf = pd.read_csv(df_path)\n\n# Cek kolom\ndf.head()\n\n\n\ndf[['kategori_berita', 'stemmed']].head()\n\n\n\n# Melihat daftar kategori unik\nprint(df['kategori_berita'].unique())\n\n# Menghitung jumlah kategori unik\nprint(\"Jumlah kategori:\", df['kategori_berita'].nunique())\n\n# Kalau mau lihat jumlah data per kategori\nprint(df['kategori_berita'].value_counts())\n\n\n\n","type":"content","url":"/ektraksi-lda","position":1},{"hierarchy":{"lvl1":"Klasifikasi dengan ektraksi fitur menggunakan  Latent Dirichlet Allocation","lvl2":"Siapkan data untuk LDA"},"type":"lvl2","url":"/ektraksi-lda#siapkan-data-untuk-lda","position":2},{"hierarchy":{"lvl1":"Klasifikasi dengan ektraksi fitur menggunakan  Latent Dirichlet Allocation","lvl2":"Siapkan data untuk LDA"},"content":"\n\n# Ubah string list token menjadi list Python\nimport ast\ndf['tokens'] = df['tokens'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n\n# Buat kamus (dictionary) dan corpus untuk Gensim\ndictionary = corpora.Dictionary(df['tokens'])\ncorpus = [dictionary.doc2bow(text) for text in df['tokens']]\n\nprint(\"Jumlah dokumen:\", len(corpus))\nprint(\"Jumlah kata unik:\", len(dictionary))\n\n\n\n","type":"content","url":"/ektraksi-lda#siapkan-data-untuk-lda","position":3},{"hierarchy":{"lvl1":"Klasifikasi dengan ektraksi fitur menggunakan  Latent Dirichlet Allocation","lvl2":"Training Model LDA"},"type":"lvl2","url":"/ektraksi-lda#training-model-lda","position":4},{"hierarchy":{"lvl1":"Klasifikasi dengan ektraksi fitur menggunakan  Latent Dirichlet Allocation","lvl2":"Training Model LDA"},"content":"\n\n# Tentukan jumlah topik (bisa kamu sesuaikan)\nnum_topics = 10\n\nlda_model = LdaModel(\n    corpus=corpus,\n    id2word=dictionary,\n    num_topics=num_topics,\n    random_state=42,\n    passes=10,\n    alpha='auto'\n)\n\nfor i, topic in lda_model.show_topics(num_topics=num_topics, num_words=10, formatted=False):\n    print(f\"\\nTopik {i+1}:\")\n    print(\", \".join([word for word, prob in topic]))\n\n\n\n\n\n","type":"content","url":"/ektraksi-lda#training-model-lda","position":5},{"hierarchy":{"lvl1":"Klasifikasi dengan ektraksi fitur menggunakan  Latent Dirichlet Allocation","lvl2":"Ubah hasil LDA menjadi fitur numerik"},"type":"lvl2","url":"/ektraksi-lda#ubah-hasil-lda-menjadi-fitur-numerik","position":6},{"hierarchy":{"lvl1":"Klasifikasi dengan ektraksi fitur menggunakan  Latent Dirichlet Allocation","lvl2":"Ubah hasil LDA menjadi fitur numerik"},"content":"\n\ndef get_topic_distribution(lda_model, corpus):\n    topic_features = []\n    for doc in corpus:\n        topic_dist = [0] * lda_model.num_topics\n        for topic_num, prob in lda_model.get_document_topics(doc):\n            topic_dist[topic_num] = prob\n        topic_features.append(topic_dist)\n    return np.array(topic_features)\n\nX_lda = get_topic_distribution(lda_model, corpus)\ny = df['kategori_berita']\n\nprint(\"Ekstraksi fitur LDA selesai.\")\nX_lda.shape\n\n\n\n\n\n# Gabungkan hasil LDA dengan kategori berita\ndf_lda = pd.DataFrame(X_lda, columns=[f\"Topik_{i+1}\" for i in range(X_lda.shape[1])])\ndf_lda['kategori_berita'] = df['kategori_berita'].values\n\n# Lihat 5 baris pertama\ndf_lda.head()\n\n\n\n\n","type":"content","url":"/ektraksi-lda#ubah-hasil-lda-menjadi-fitur-numerik","position":7},{"hierarchy":{"lvl1":"Klasifikasi dengan ektraksi fitur menggunakan  Latent Dirichlet Allocation","lvl2":"Bagi Data Train dan Test"},"type":"lvl2","url":"/ektraksi-lda#bagi-data-train-dan-test","position":8},{"hierarchy":{"lvl1":"Klasifikasi dengan ektraksi fitur menggunakan  Latent Dirichlet Allocation","lvl2":"Bagi Data Train dan Test"},"content":"\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X_lda, y, test_size=0.2, random_state=42, stratify=y\n)\nprint(f\"Jumlah data latih: {len(X_train)}, data uji: {len(X_test)}\")\n\n\n\n\n","type":"content","url":"/ektraksi-lda#bagi-data-train-dan-test","position":9},{"hierarchy":{"lvl1":"Klasifikasi dengan ektraksi fitur menggunakan  Latent Dirichlet Allocation","lvl2":"Training Model SVM"},"type":"lvl2","url":"/ektraksi-lda#training-model-svm","position":10},{"hierarchy":{"lvl1":"Klasifikasi dengan ektraksi fitur menggunakan  Latent Dirichlet Allocation","lvl2":"Training Model SVM"},"content":"\n\nsvm_model = SVC(kernel='linear', C=1, random_state=42)\nsvm_model.fit(X_train, y_train)\n\nprint(\"Model SVM berhasil dilatih dengan fitur hasil ekstraksi LDA.\")\n\n\n\n","type":"content","url":"/ektraksi-lda#training-model-svm","position":11},{"hierarchy":{"lvl1":"Klasifikasi dengan ektraksi fitur menggunakan  Latent Dirichlet Allocation","lvl2":"Evaluasi Model"},"type":"lvl2","url":"/ektraksi-lda#evaluasi-model","position":12},{"hierarchy":{"lvl1":"Klasifikasi dengan ektraksi fitur menggunakan  Latent Dirichlet Allocation","lvl2":"Evaluasi Model"},"content":"\n\ny_pred = svm_model.predict(X_test)\n\nprint(\"Akurasi Model:\", accuracy_score(y_test, y_pred))\nprint(\"\\nLaporan Klasifikasi:\\n\", classification_report(y_test, y_pred))\n\n\n\n\n\n\n","type":"content","url":"/ektraksi-lda#evaluasi-model","position":13},{"hierarchy":{"lvl1":"Klasifikasi dengan ektraksi fitur menggunakan  Latent Dirichlet Allocation","lvl2":"Visualisasi Confusion Matrix"},"type":"lvl2","url":"/ektraksi-lda#visualisasi-confusion-matrix","position":14},{"hierarchy":{"lvl1":"Klasifikasi dengan ektraksi fitur menggunakan  Latent Dirichlet Allocation","lvl2":"Visualisasi Confusion Matrix"},"content":"\n\nplt.figure(figsize=(8,6))\nsns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\nplt.title('Confusion Matrix - SVM dengan Fitur LDA')\nplt.xlabel('Predicted Label')\nplt.ylabel('True Label')\nplt.show()\n\n\n\n\n","type":"content","url":"/ektraksi-lda#visualisasi-confusion-matrix","position":15},{"hierarchy":{"lvl1":"Klasifikasi dengan ektraksi fitur menggunakan  Latent Dirichlet Allocation","lvl2":"Distribusi Topik per Kategori"},"type":"lvl2","url":"/ektraksi-lda#distribusi-topik-per-kategori","position":16},{"hierarchy":{"lvl1":"Klasifikasi dengan ektraksi fitur menggunakan  Latent Dirichlet Allocation","lvl2":"Distribusi Topik per Kategori"},"content":"\n\ntopic_df = pd.DataFrame(X_lda, columns=[f\"Topik_{i}\" for i in range(num_topics)])\ntopic_df[\"Kategori\"] = y.values\n\ntopic_by_category = topic_df.groupby(\"Kategori\").mean()\n\nplt.figure(figsize=(12,6))\nsns.heatmap(topic_by_category, cmap=\"YlGnBu\", annot=True, fmt=\".2f\")\nplt.title(\"Distribusi Rata-rata Topik per Kategori Berita (Hasil LDA)\")\nplt.xlabel(\"Topik\")\nplt.ylabel(\"Kategori Berita\")\nplt.show()\n\n\n\n\n","type":"content","url":"/ektraksi-lda#distribusi-topik-per-kategori","position":17},{"hierarchy":{"lvl1":"Klasifikasi dengan ektraksi fitur menggunakan  Latent Dirichlet Allocation","lvl2":"Topik Dominan per Dokumen"},"type":"lvl2","url":"/ektraksi-lda#topik-dominan-per-dokumen","position":18},{"hierarchy":{"lvl1":"Klasifikasi dengan ektraksi fitur menggunakan  Latent Dirichlet Allocation","lvl2":"Topik Dominan per Dokumen"},"content":"\n\ntopic_df[\"Topik_Dominan\"] = topic_df[[f\"Topik_{i}\" for i in range(num_topics)]].idxmax(axis=1)\n\nplt.figure(figsize=(10,5))\nsns.countplot(y=\"Topik_Dominan\", hue=\"Kategori\", data=topic_df, palette=\"Set2\")\nplt.title(\"Distribusi Topik Dominan per Kategori Berita\")\nplt.xlabel(\"Jumlah Dokumen\")\nplt.ylabel(\"Topik Dominan\")\nplt.legend(title=\"Kategori\")\nplt.show()\n\n\n\n\n","type":"content","url":"/ektraksi-lda#topik-dominan-per-dokumen","position":19},{"hierarchy":{"lvl1":"Klasifikasi dengan ektraksi fitur menggunakan  Latent Dirichlet Allocation","lvl2":"Proyeksi 2D Data LDA"},"type":"lvl2","url":"/ektraksi-lda#proyeksi-2d-data-lda","position":20},{"hierarchy":{"lvl1":"Klasifikasi dengan ektraksi fitur menggunakan  Latent Dirichlet Allocation","lvl2":"Proyeksi 2D Data LDA"},"content":"\n\nimport umap\n# Buat model UMAP\numap_model = umap.UMAP(n_neighbors=15, n_components=2, random_state=42)\n\n# Fit-transform untuk data training dan test\nX_train_2d = umap_model.fit_transform(X_train)\nX_test_2d = umap_model.transform(X_test)\n\n# Simpan hasil proyeksi ke DataFrame\ntrain_umap = pd.DataFrame(X_train_2d, columns=['x', 'y'])\ntrain_umap['Kategori'] = y_train.values\n\ntest_umap = pd.DataFrame(X_test_2d, columns=['x', 'y'])\ntest_umap['Kategori'] = y_test.values\n\n\n\n\n","type":"content","url":"/ektraksi-lda#proyeksi-2d-data-lda","position":21},{"hierarchy":{"lvl1":"Klasifikasi dengan ektraksi fitur menggunakan  Latent Dirichlet Allocation","lvl2":"Visualisasi"},"type":"lvl2","url":"/ektraksi-lda#visualisasi","position":22},{"hierarchy":{"lvl1":"Klasifikasi dengan ektraksi fitur menggunakan  Latent Dirichlet Allocation","lvl2":"Visualisasi"},"content":"\n\nplt.figure(figsize=(14,6))\n\n# Plot subset training\nplt.subplot(1, 2, 1)\nsns.scatterplot(\n    data=train_umap,\n    x='x', y='y',\n    hue='Kategori',\n    palette='Set2',\n    s=40,\n    alpha=0.8\n)\nplt.title('Train Subset (UMAP Projection)')\nplt.legend(loc='best')\n\n# Plot subset testing\nplt.subplot(1, 2, 2)\nsns.scatterplot(\n    data=test_umap,\n    x='x', y='y',\n    hue='Kategori',\n    palette='Set2',\n    s=40,\n    alpha=0.8\n)\nplt.title('Test Subset (UMAP Projection)')\nplt.legend(loc='best')\n\nplt.suptitle('Visualisasi Distribusi Topik LDA dengan UMAP', fontsize=14)\nplt.tight_layout()\nplt.show()\n\n\n","type":"content","url":"/ektraksi-lda#visualisasi","position":23},{"hierarchy":{"lvl1":"Facebook"},"type":"lvl1","url":"/facebook-ppw","position":0},{"hierarchy":{"lvl1":"Facebook"},"content":"","type":"content","url":"/facebook-ppw","position":1},{"hierarchy":{"lvl1":"Facebook","lvl2":"Install dan Import Library"},"type":"lvl2","url":"/facebook-ppw#install-dan-import-library","position":2},{"hierarchy":{"lvl1":"Facebook","lvl2":"Install dan Import Library"},"content":"\n\n!pip uninstall community\n!pip install python-louvain\n\n\n\nimport networkx as nx\nimport community.community_louvain as community_louvain\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n\n\nimport networkx as nx\n\nfile = \"/content/drive/MyDrive/PPW/UAS/FB/facebook_combined.txt.gz\"\n\nG_fb = nx.read_edgelist(file, nodetype=int)\n\nprint(\"Nodes :\", G_fb.number_of_nodes())\nprint(\"Edges :\", G_fb.number_of_edges())\n\n\n\n\n#Algoritma Louvain\n\nprint(\"\\nRunning Louvain algorithm...\")\nhasil = community_louvain.best_partition(G_fb)\njumlah_komunitas = len(set(hasil.values()))\nprint(\"Total Communities Detected:\", jumlah_komunitas)\n\n\n\nprint(\"\\nRunning Louvain algorithm...\")\n\nhasil = community_louvain.best_partition(G_fb)\n\njumlah_komunitas = len(set(hasil.values()))\nprint(\"Total Communities Detected:\", jumlah_komunitas)\n\n\n\n\n#Mengitung Modularity\n\nfrom community.community_louvain import modularity\n\nnilai_modularity = modularity(hasil, G_fb)\nprint(\"\\nModularity Score :\", nilai_modularity)\n\n\n\n#Menyimpan Hasil csv\n\n# 3. Save Results\n\ndf = pd.DataFrame({\n    \"node\": list(hasil.keys()),\n    \"community\": list(hasil.values())\n})\n\ndf.to_csv(\"/content/drive/MyDrive/PPW/UAS/FB/facebook_louvain_result.csv\", index=False)\nprint(\"\\nHasil komunitas disimpan ke: facebook_louvain_result.csv\")\n\n\n\nfb_louvain_result = pd.read_csv(\"/content/drive/MyDrive/PPW/UAS/FB/facebook_louvain_result.csv\")\nfb_louvain_result\n\n\n\n#Visualisasi graph\n\n# 4. (Optional) Visualisasi\nvisualize = True\n\nif visualize:\n    pos = nx.spring_layout(G_fb, seed=42)\n\n    nx.draw(\n        G_fb,\n        pos,\n        node_size=20,\n        node_color=[hasil[n] for n in G_fb.nodes()],\n        cmap=plt.get_cmap(\"viridis\"),\n        edge_color=\"gray\",\n        linewidths=0.2\n    )\n\n    plt.title(\"Community Detection using Louvain\")\n    plt.show()\n\n\n\n\ntemukan komunitas\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom IPython.display import display\n\n# 1) Load hasil Louvain\npath = \"/content/drive/MyDrive/PPW/UAS/FB/facebook_louvain_result.csv\"\ndf = pd.read_csv(path)\n\n# 2) Hitung ukuran komunitas\ncommunity_sizes = Counter(df[\"community\"])\n\n# 3) Buat DataFrame statistik\nstats_df = pd.DataFrame({\n    \"Community ID\": list(community_sizes.keys()),\n    \"Size\": list(community_sizes.values())\n}).sort_values(by=\"Size\", ascending=False).reset_index(drop=True)\n\n# 4) Statistik ringkasan\ntotal_nodes       = len(df)\ntotal_communities = len(community_sizes)\nlargest           = stats_df.iloc[0].to_dict()\nsmallest          = stats_df.iloc[-1].to_dict()\navg_size          = stats_df[\"Size\"].mean()\nmedian_size       = stats_df[\"Size\"].median()\nstd_size          = stats_df[\"Size\"].std()\n\nsummary = {\n    \"Total Nodes\": int(total_nodes),\n    \"Total Communities\": int(total_communities),\n    \"Largest Community\": largest,\n    \"Smallest Community\": smallest,\n    \"Average Size\": float(round(avg_size, 4)),\n    \"Median Size\": float(median_size),\n    \"Std Dev Size\": float(round(std_size, 4))\n}\n\n# 5) Tampilkan tabel & ringkasan\nprint(\"=== Community Statistics Summary ===\")\nfor k, v in summary.items():\n    print(f\"{k}: {v}\")\nprint(\"\\n=== Table: community sizes (sorted desc) ===\")\ndisplay(stats_df)\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\n\n# Load the uploaded file\npath = \"/content/drive/MyDrive/PPW/UAS/FB/facebook_louvain_result.csv\"\ndf = pd.read_csv(path)\n\n# Count community sizes\ncommunity_sizes = Counter(df['community'])\n\n# Prepare data\ncommunities = list(community_sizes.keys())\nsizes = list(community_sizes.values())\n\n# Plot\nplt.figure(figsize=(12, 6))\nplt.bar(communities, sizes)\nplt.xlabel(\"Community ID\")\nplt.ylabel(\"Number of Nodes\")\nplt.title(\"Community Size Distribution (Louvain)\")\nplt.xticks(communities)\nplt.tight_layout()\nplt.show()\n\n\n\n\nkeluarkan anggota setiap komunitas\n\ncommunity_groups = {}\n\nfor index, row in df.iterrows():\n    community_groups.setdefault(row[\"community\"], []).append(row[\"node\"])\n\n# tampilkan anggota komunitas\nfor cid, members in community_groups.items():\n    print(f\"\\nKomunitas {cid} | Jumlah anggota: {len(members)}\")\n    print(members)\n\n\n\n\n-+ anggota komunitas\n\nlargest_comm = max(community_groups, key=lambda c: len(community_groups[c]))\nsmallest_comm = min(community_groups, key=lambda c: len(community_groups[c]))\n\nprint(\"\\nKomunitas terbesar:\", largest_comm, \"| anggota:\", len(community_groups[largest_comm]))\nprint(\"Komunitas terkecil:\", smallest_comm, \"| anggota:\", len(community_groups[smallest_comm]))\n\n\n\n\ngmbr ulang graph dari komunitas yang paling banyak\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# Load full graph\nfile_path = \"/content/drive/MyDrive/PPW/UAS/FB/facebook_combined.txt.gz\"\nG = nx.read_edgelist(file_path, nodetype=int)\n\n# Ambil komunitas terbesar\nlargest_nodes = community_groups[largest_comm]\n\n# Buat subgraph\nsubG = G.subgraph(largest_nodes)\n\nplt.figure(figsize=(10, 10))\nnx.draw(subG, node_size=30, edge_color='gray')\nplt.title(\"Graph Komunitas Terbesar\")\nplt.show()\n\n\n\n\ndeteksi komunitas dari komunitas terbesar itu\n\nsub_partition = community_louvain.best_partition(subG)\nprint(\"\\nKomunitas dalam komunitas terbesar berhasil ditemukan!\")\nprint(\"Total sub-komunitas:\", len(set(sub_partition.values())))\n\n\n\n\nkeluarkan komunitas didalam komunitas\n\nsub_groups = defaultdict(list)\nfor node, comm in sub_partition.items():\n    sub_groups[comm].append(node)\n\nfor sub_id, members in sub_groups.items():\n    print(f\"Sub-Komunitas {sub_id} | Jumlah anggota = {len(members)}\")\nsorted_sub = sorted(sub_groups.items(), key=lambda x: len(x[1]), reverse=True)\nprint(\"\\n=== Sub-komunitas terbesar ===\")\nfor sid, mem in sorted_sub:\n    print(f\"Sub-Kom {sid} : {len(mem)} anggota\")\n\n\n","type":"content","url":"/facebook-ppw#install-dan-import-library","position":3},{"hierarchy":{"lvl1":"Tugas"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Tugas"},"content":"Tugas ini berisi materi yang harus dipelajari dan dipahami sebagai bagian dari pembelajaran mata kuliah *Penambangan Web*. Setiap tugas berisi penjelasan teori sekaligus contoh praktik yang perlu dicermati dengan baik.\n\nSilakan buka tautan berikut untuk melihat detail masing-masing tugas:\n\nPengantar â€” Berisi penjelasan dasar mengenai konsep awal penambangan web.\n\nCrawling Data dengan Python â€” Berisi contoh implementasi web crawling menggunakan Python.","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Manajemen & Berita Tf idf Dan Cbow"},"type":"lvl1","url":"/tugas5-cbow-dan-tf-idf","position":0},{"hierarchy":{"lvl1":"Manajemen & Berita Tf idf Dan Cbow"},"content":"","type":"content","url":"/tugas5-cbow-dan-tf-idf","position":1},{"hierarchy":{"lvl1":"Manajemen & Berita Tf idf Dan Cbow","lvl3":"Load Data"},"type":"lvl3","url":"/tugas5-cbow-dan-tf-idf#load-data","position":2},{"hierarchy":{"lvl1":"Manajemen & Berita Tf idf Dan Cbow","lvl3":"Load Data"},"content":"\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport re, sys, time\nfrom google.colab import drive\nimport pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\n\n\n\ndrive.mount('/content/drive')\n\n\n\npta_trunojoyo_path = \"/content/drive/MyDrive/PPW/output/pta_manajemen.csv\"\npta_trunojoyo = pd.read_csv(pta_trunojoyo_path)\n\npta_trunojoyo\n\n\n\n\n","type":"content","url":"/tugas5-cbow-dan-tf-idf#load-data","position":3},{"hierarchy":{"lvl1":"Manajemen & Berita Tf idf Dan Cbow","lvl3":"Mengecek Nilai yang null dan memebersihkannya"},"type":"lvl3","url":"/tugas5-cbow-dan-tf-idf#mengecek-nilai-yang-null-dan-memebersihkannya","position":4},{"hierarchy":{"lvl1":"Manajemen & Berita Tf idf Dan Cbow","lvl3":"Mengecek Nilai yang null dan memebersihkannya"},"content":"\n\n# Cek jumlah NaN di tiap kolom\nprint(pta_trunojoyo.isna().sum())\n\n\n\n\n# Hapus baris yang abstrak_en kosong\ndf = pta_trunojoyo.dropna(subset=['abstrak_id']).reset_index(drop=True)\n\n# Cek lagi apakah masih ada yang kosong\nprint(df['abstrak_id'].isna().sum())\n\n\n\n\ndf\n\n\n\n","type":"content","url":"/tugas5-cbow-dan-tf-idf#mengecek-nilai-yang-null-dan-memebersihkannya","position":5},{"hierarchy":{"lvl1":"Manajemen & Berita Tf idf Dan Cbow","lvl3":"Cleaning Data"},"type":"lvl3","url":"/tugas5-cbow-dan-tf-idf#cleaning-data","position":6},{"hierarchy":{"lvl1":"Manajemen & Berita Tf idf Dan Cbow","lvl3":"Cleaning Data"},"content":"Kode tersebut melakukan pembersihan teks pada kolom abstrak_indo dengan mengubah semua huruf menjadi huruf kecil agar konsisten dan tidak membedakan antara huruf besar dan kecil. Selain itu, kode ini menghapus semua angka, tanda baca, dan simbol sehingga hanya tersisa huruf dan spasi, yang bertujuan menghilangkan karakter yang tidak relevan atau mengganggu proses analisis teks. Terakhir, pembersihan juga menghapus spasi berlebihan, termasuk spasi ganda atau spasi di awal dan akhir kalimat, agar teks menjadi lebih rapi dan mudah diproses. Proses ini penting untuk menyederhanakan dan menormalkan data teks sehingga siap digunakan dalam tahap analisis lebih lanjut seperti tokenisasi, stemming, atau klasifikasi.\n\n\nimport pandas as pd\nimport re\n\n# Fungsi cleansing\ndef cleansing(text):\n    text = str(text).lower()                         # ubah ke huruf kecil\n    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)        # hilangkan angka/simbol\n    text = re.sub(r'\\s+', ' ', text).strip()        # hilangkan spasi berlebih\n    return text\n\n# Terapkan cleansing\ndf['cleaned'] = df['abstrak_id'].apply(cleansing)\n\n# Simpan ke CSV\ndf.to_csv('/content/drive/MyDrive/PPW/output/manajemen_abstrak_cleaned.csv', index=False)\nprint(\"Data berhasil disimpan ke CSV!\\n\")\n\n# Tampilkan beberapa baris pertama\ndf[['abstrak_id','cleaned']].head()\n\n\n\n\n\n!pip install pyspellchecker\n\n\n\n","type":"content","url":"/tugas5-cbow-dan-tf-idf#cleaning-data","position":7},{"hierarchy":{"lvl1":"Manajemen & Berita Tf idf Dan Cbow","lvl3":"Stopword Removal"},"type":"lvl3","url":"/tugas5-cbow-dan-tf-idf#stopword-removal","position":8},{"hierarchy":{"lvl1":"Manajemen & Berita Tf idf Dan Cbow","lvl3":"Stopword Removal"},"content":"Kode ini digunakan untuk menghapus kata-kata umum (stopwords) dalam bahasa Indonesia dari teks yang sudah dibersihkan di kolom cleaned. Dengan menggunakan library Sastrawi, dibuat objek stopword remover yang akan menghilangkan kata-kata seperti â€œdanâ€, â€œdiâ€, â€œyangâ€, dan kata umum lain yang biasanya tidak membawa makna penting dalam analisis teks.\n\nSetelah proses penghilangan stopwords selesai, hasilnya disimpan dalam kolom baru bernama no_stopwords. Kemudian, data yang sudah diproses ini disimpan kembali ke file CSV baru manajemen_abstrak_no_stopwords.csv. Langkah ini penting agar teks menjadi lebih fokus pada kata-kata bermakna dan memudahkan analisis selanjutnya seperti klasifikasi atau clustering.\n\n!pip install Sastrawi\n\n\n\nimport pandas as pd\nfrom Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n\n# Buat stopword remover\nfactory = StopWordRemoverFactory()\nstopword = factory.create_stop_word_remover()\n\n# Terapkan stopword removal pada kolom 'cleaned'\ndf['no_stopwords'] = df['cleaned'].apply(stopword.remove)\n\n# Simpan hasil ke CSV\ndf.to_csv('/content/drive/MyDrive/PPW/output/manajemen_abstrak_no_stopwords.csv', index=False)\nprint(\"Data berhasil disimpan ke CSV!\\n\")\n\n# Tampilkan beberapa baris pertama\ndf[['cleaned','no_stopwords']].head()\n\n\n\n\n\n\n","type":"content","url":"/tugas5-cbow-dan-tf-idf#stopword-removal","position":9},{"hierarchy":{"lvl1":"Manajemen & Berita Tf idf Dan Cbow","lvl3":"Stemming"},"type":"lvl3","url":"/tugas5-cbow-dan-tf-idf#stemming","position":10},{"hierarchy":{"lvl1":"Manajemen & Berita Tf idf Dan Cbow","lvl3":"Stemming"},"content":"Kode ini melakukan proses stemming pada teks di kolom no_stopwords menggunakan library Sastrawi, yaitu mengubah kata-kata yang memiliki imbuhan atau variasi menjadi bentuk dasar atau kata dasarnya. Hasil stemming ini disimpan dalam kolom baru stemmed dan kemudian disimpan ke file CSV baru untuk digunakan dalam analisis selanjutnya. Proses stemming penting untuk menyederhanakan variasi kata sehingga model atau analisis teks dapat mengenali kata-kata dengan makna yang sama secara konsisten.\n\nimport pandas as pd\nfrom Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n\n# Buat stemmer\nstemmer = StemmerFactory().create_stemmer()\n\n# Terapkan stemming pada kolom 'no_stopwords'\ndf['stemmed'] = df['no_stopwords'].apply(stemmer.stem)\n\n# Simpan hasil ke CSV\ndf.to_csv('/content/drive/MyDrive/PPW/output/manajemen_abstrak_stemmed.csv', index=False)\nprint(\"Data berhasil disimpan ke CSV!\\n\")\n\n# Tampilkan beberapa baris pertama\ndf[['no_stopwords','stemmed']].head()\n\n\n\n\n\n\n","type":"content","url":"/tugas5-cbow-dan-tf-idf#stemming","position":11},{"hierarchy":{"lvl1":"Manajemen & Berita Tf idf Dan Cbow","lvl3":"Tokenisasi"},"type":"lvl3","url":"/tugas5-cbow-dan-tf-idf#tokenisasi","position":12},{"hierarchy":{"lvl1":"Manajemen & Berita Tf idf Dan Cbow","lvl3":"Tokenisasi"},"content":"Kode ini melakukan tokenisasi pada teks yang sudah melalui proses stemming di kolom stemmed. Tokenisasi adalah proses memecah kalimat atau teks menjadi potongan-potongan kata (token) yang lebih kecil, biasanya berdasarkan spasi. Fungsi tokenize yang sederhana di sini memecah setiap kalimat menjadi daftar kata-kata dengan menggunakan metode split(). Hasil tokenisasi disimpan dalam kolom baru tokens, yang berisi daftar kata untuk tiap baris teks. Setelah itu, data lengkap dengan token disimpan ke file CSV baru manajemen_abstrak_tokens.csv\n\nimport pandas as pd\n\n# Fungsi tokenisasi sederhana\ndef tokenize(text):\n    return text.split()\n\n# Terapkan tokenisasi pada kolom 'stemmed'\ndf['tokens'] = df['stemmed'].apply(tokenize)\n\n# Simpan hasil ke CSV\ndf.to_csv('/content/drive/MyDrive/PPW/output/manajemen_abstrak_tokens.csv', index=False)\nprint(\"Data berhasil disimpan ke CSV!\\n\")\n\n# Tampilkan beberapa baris pertama\ndf[['stemmed','tokens']].head()\n\n\n\n\n\n\n","type":"content","url":"/tugas5-cbow-dan-tf-idf#tokenisasi","position":13},{"hierarchy":{"lvl1":"Manajemen & Berita Tf idf Dan Cbow","lvl3":"TF-IDF"},"type":"lvl3","url":"/tugas5-cbow-dan-tf-idf#tf-idf","position":14},{"hierarchy":{"lvl1":"Manajemen & Berita Tf idf Dan Cbow","lvl3":"TF-IDF"},"content":"\n\n","type":"content","url":"/tugas5-cbow-dan-tf-idf#tf-idf","position":15},{"hierarchy":{"lvl1":"Manajemen & Berita Tf idf Dan Cbow","lvl4":"Ambil Corpus","lvl3":"TF-IDF"},"type":"lvl4","url":"/tugas5-cbow-dan-tf-idf#ambil-corpus","position":16},{"hierarchy":{"lvl1":"Manajemen & Berita Tf idf Dan Cbow","lvl4":"Ambil Corpus","lvl3":"TF-IDF"},"content":"\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Ambil corpus dari kolom stemmed\ncorpus = df['stemmed'].astype(str).tolist()\n\nprint(\"Jumlah dokumen dalam corpus:\", len(corpus))\nprint(\"Contoh dokumen:\", corpus[0][:200])\n\n\n\n\n# Inisialisasi TF-IDF\nvectorizer = TfidfVectorizer()\n\n# Fit dan transform\nX_tfidf = vectorizer.fit_transform(corpus)\n\nprint(\"Shape TF-IDF:\", X_tfidf.shape)  # (jumlah dokumen, jumlah kata unik)\n\n\n\n\n# Ubah ke DataFrame biar mudah dibaca\ntfidf_df = pd.DataFrame(\n    X_tfidf.toarray(),\n    columns=vectorizer.get_feature_names_out()\n)\n\n# Simpan hasil ke CSV\ntfidf_df.to_csv(\"/content/drive/MyDrive/PPW/output/tfidf_hasil2.csv\", index=False)\nprint(\"Data berhasil disimpan ke CSV!\\n\")\n\n# Tampilkan 5 baris pertama\ntfidf_df.head()\n\n\n\n\n\n\n# Tampilkan 10 kata dengan bobot TF-IDF tertinggi di dokumen pertama\nprint(tfidf_df.iloc[0].sort_values(ascending=False).head(10))\n\n\n\n","type":"content","url":"/tugas5-cbow-dan-tf-idf#ambil-corpus","position":17},{"hierarchy":{"lvl1":"Manajemen & Berita Tf idf Dan Cbow","lvl3":"CBOW"},"type":"lvl3","url":"/tugas5-cbow-dan-tf-idf#cbow","position":18},{"hierarchy":{"lvl1":"Manajemen & Berita Tf idf Dan Cbow","lvl3":"CBOW"},"content":"\n\n!pip install gensim\n\n\n\nfrom gensim.models import Word2Vec\n\ncorpus = []\nfor col in df['stemmed']:\n    word_list = col.split(\" \")\n    corpus.append(word_list)\n\nprint(corpus[0][:20])\n\n\n\n# Training Word2Vec\nmodel = Word2Vec(\n    corpus,\n    vector_size=56,   # ukuran embedding\n    window=5,\n    min_count=1,\n    sg=0   # CBOW\n)\n\n\n\n\nimport numpy as np\n\nclass MeanEmbeddingVectorizer:\n    def __init__(self, model):\n        self.model = model\n        self.vector_size = model.vector_size\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return np.array([\n            np.mean([self.model.wv[word] for word in words if word in self.model.wv]\n                    or [np.zeros(self.vector_size)], axis=0)\n            for words in X\n        ])\n\n    def fit_transform(self, X, y=None):\n        return self.fit(X).transform(X)\n\n\n\n\nmean_embedding_vectorizer = MeanEmbeddingVectorizer(model)\nmean_embedded = mean_embedding_vectorizer.fit_transform(corpus)\n\n# simpan ke DataFrame\ndf['array'] = list(mean_embedded)\n\n# cek panjang embedding\ndf['embedding_length'] = df['array'].str.len()\nprint(df[['stemmed','embedding_length']].head())\n\n\n\n\nnum_features = len(df['array'].iloc[0])  # asumsi semua list punya panjang sama\ncolumns = [f'f{i+1}' for i in range(num_features)]\n\n# Inisialisasi dictionary untuk menampung data per kolom\ndata_dict = {col: [] for col in columns}\n\n# Looping setiap baris di kolom 'embedding'\nfor embedding_list in df['array']:\n    for i, value in enumerate(embedding_list):\n        data_dict[f'f{i+1}'].append(value)\n\n# Buat DataFrame dari dictionary\nembedding_df = pd.DataFrame(data_dict)\n\nprint(embedding_df)\n\n\n\nembedding_df['abstrak_id'] = df['abstrak_id'].values\n\n\n\nembedding_df\n\n\n\nembedding_df.shape\n\n\n\n","type":"content","url":"/tugas5-cbow-dan-tf-idf#cbow","position":19},{"hierarchy":{"lvl1":"Manajemen & Berita Tf idf Dan Cbow","lvl2":"BERITA TF IDF & CBOW"},"type":"lvl2","url":"/tugas5-cbow-dan-tf-idf#berita-tf-idf-cbow","position":20},{"hierarchy":{"lvl1":"Manajemen & Berita Tf idf Dan Cbow","lvl2":"BERITA TF IDF & CBOW"},"content":"\n\n","type":"content","url":"/tugas5-cbow-dan-tf-idf#berita-tf-idf-cbow","position":21},{"hierarchy":{"lvl1":"Manajemen & Berita Tf idf Dan Cbow","lvl3":"Load Data","lvl2":"BERITA TF IDF & CBOW"},"type":"lvl3","url":"/tugas5-cbow-dan-tf-idf#load-data-1","position":22},{"hierarchy":{"lvl1":"Manajemen & Berita Tf idf Dan Cbow","lvl3":"Load Data","lvl2":"BERITA TF IDF & CBOW"},"content":"\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport re, sys, time\nfrom google.colab import drive\nimport pandas as pd\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\n\n\n\ndrive.mount('/content/drive')\n\n\n\nberita_cnn_path = \"/content/drive/MyDrive/PPW/output/cnnindonesia_berita4.csv\"\nberita_cnn = pd.read_csv(berita_cnn_path)\n\nberita_cnn\n\n\n\n# Melihat daftar kategori unik\nprint(berita_cnn['kategori_berita'].unique())\n\n# Menghitung jumlah kategori unik\nprint(\"Jumlah kategori:\", berita_cnn['kategori_berita'].nunique())\n\n# Kalau mau lihat jumlah data per kategori\nprint(berita_cnn['kategori_berita'].value_counts())\n\n\n\n","type":"content","url":"/tugas5-cbow-dan-tf-idf#load-data-1","position":23},{"hierarchy":{"lvl1":"Manajemen & Berita Tf idf Dan Cbow","lvl3":"Mengecek Nilai yang null dan memebersihkannya","lvl2":"BERITA TF IDF & CBOW"},"type":"lvl3","url":"/tugas5-cbow-dan-tf-idf#mengecek-nilai-yang-null-dan-memebersihkannya-1","position":24},{"hierarchy":{"lvl1":"Manajemen & Berita Tf idf Dan Cbow","lvl3":"Mengecek Nilai yang null dan memebersihkannya","lvl2":"BERITA TF IDF & CBOW"},"content":"\n\n# Cek jumlah NaN di tiap kolom\nprint(berita_cnn.isna().sum())\n\n\n\n# Hapus baris yang abstrak_en kosong\ndata = berita_cnn.dropna(subset=['isi_berita']).reset_index(drop=True)\n\n# Cek lagi apakah masih ada yang kosong\nprint(data['isi_berita'].isna().sum())\n\n\n\n\ndata\n\n\n\n","type":"content","url":"/tugas5-cbow-dan-tf-idf#mengecek-nilai-yang-null-dan-memebersihkannya-1","position":25},{"hierarchy":{"lvl1":"Manajemen & Berita Tf idf Dan Cbow","lvl3":"Cleaning Data","lvl2":"BERITA TF IDF & CBOW"},"type":"lvl3","url":"/tugas5-cbow-dan-tf-idf#cleaning-data-1","position":26},{"hierarchy":{"lvl1":"Manajemen & Berita Tf idf Dan Cbow","lvl3":"Cleaning Data","lvl2":"BERITA TF IDF & CBOW"},"content":"\n\nimport pandas as pd\nimport re\n\n# Fungsi cleansing\ndef cleansing(text):\n    text = str(text).lower()                         # ubah ke huruf kecil\n    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)        # hilangkan angka/simbol\n    text = re.sub(r'\\s+', ' ', text).strip()        # hilangkan spasi berlebih\n    return text\n\n# Terapkan cleansing\ndata['cleaned'] = data['isi_berita'].apply(cleansing)\n\n# Simpan ke CSV\ndata.to_csv('/content/drive/MyDrive/PPW/output/isi_berita_cleaned.csv', index=False)\nprint(\"Data berhasil disimpan ke CSV!\\n\")\n\n# Tampilkan beberapa baris pertama\ndata[['isi_berita','cleaned']].head()\n\n\n\n\n\n!pip install pyspellchecker\n\n\n\n","type":"content","url":"/tugas5-cbow-dan-tf-idf#cleaning-data-1","position":27},{"hierarchy":{"lvl1":"Manajemen & Berita Tf idf Dan Cbow","lvl3":"Stopword Removal","lvl2":"BERITA TF IDF & CBOW"},"type":"lvl3","url":"/tugas5-cbow-dan-tf-idf#stopword-removal-1","position":28},{"hierarchy":{"lvl1":"Manajemen & Berita Tf idf Dan Cbow","lvl3":"Stopword Removal","lvl2":"BERITA TF IDF & CBOW"},"content":"\n\n!pip install Sastrawi\n\n\n\nimport pandas as pd\nfrom Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n\n# Buat stopword remover\nfactory = StopWordRemoverFactory()\nstopword = factory.create_stop_word_remover()\n\n# Terapkan stopword removal pada kolom 'cleaned'\ndata['no_stopwords'] = data['cleaned'].apply(stopword.remove)\n\n# Simpan hasil ke CSV\ndata.to_csv('/content/drive/MyDrive/PPW/output/berita_cnn_no_stopwords.csv', index=False)\nprint(\"Data berhasil disimpan ke CSV!\\n\")\n\n# Tampilkan beberapa baris pertama\ndata[['cleaned','no_stopwords']].head()\n\n\n\n\n\n\n","type":"content","url":"/tugas5-cbow-dan-tf-idf#stopword-removal-1","position":29},{"hierarchy":{"lvl1":"Manajemen & Berita Tf idf Dan Cbow","lvl3":"Stemming","lvl2":"BERITA TF IDF & CBOW"},"type":"lvl3","url":"/tugas5-cbow-dan-tf-idf#stemming-1","position":30},{"hierarchy":{"lvl1":"Manajemen & Berita Tf idf Dan Cbow","lvl3":"Stemming","lvl2":"BERITA TF IDF & CBOW"},"content":"\n\nimport pandas as pd\nfrom Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n\n# Buat stemmer\nstemmer = StemmerFactory().create_stemmer()\n\n# Terapkan stemming pada kolom 'no_stopwords'\ndata['stemmed'] = data['no_stopwords'].apply(stemmer.stem)\n\n# Simpan hasil ke CSV\ndata.to_csv('/content/drive/MyDrive/PPW/output/berita_cnn_stemmed.csv', index=False)\nprint(\"Data berhasil disimpan ke CSV!\\n\")\n\n# Tampilkan beberapa baris pertama\ndata[['no_stopwords','stemmed']].head()\n\n\n\n\n\n\n","type":"content","url":"/tugas5-cbow-dan-tf-idf#stemming-1","position":31},{"hierarchy":{"lvl1":"Manajemen & Berita Tf idf Dan Cbow","lvl3":"Tokenisasi","lvl2":"BERITA TF IDF & CBOW"},"type":"lvl3","url":"/tugas5-cbow-dan-tf-idf#tokenisasi-1","position":32},{"hierarchy":{"lvl1":"Manajemen & Berita Tf idf Dan Cbow","lvl3":"Tokenisasi","lvl2":"BERITA TF IDF & CBOW"},"content":"\n\nimport pandas as pd\n\n# Fungsi tokenisasi sederhana\ndef tokenize(text):\n    return text.split()\n\n# Terapkan tokenisasi pada kolom 'stemmed'\ndata['tokens'] = data['stemmed'].apply(tokenize)\n\n# Simpan hasil ke CSV\ndata.to_csv('/content/drive/MyDrive/PPW/output/berita_cnn_tokens.csv', index=False)\nprint(\"Data berhasil disimpan ke CSV!\\n\")\n\n# Tampilkan beberapa baris pertama\ndata[['stemmed','tokens']].head()\n\n\n\n\n\n\n","type":"content","url":"/tugas5-cbow-dan-tf-idf#tokenisasi-1","position":33},{"hierarchy":{"lvl1":"Manajemen & Berita Tf idf Dan Cbow","lvl3":"TF-IDF","lvl2":"BERITA TF IDF & CBOW"},"type":"lvl3","url":"/tugas5-cbow-dan-tf-idf#tf-idf-1","position":34},{"hierarchy":{"lvl1":"Manajemen & Berita Tf idf Dan Cbow","lvl3":"TF-IDF","lvl2":"BERITA TF IDF & CBOW"},"content":"\n\n","type":"content","url":"/tugas5-cbow-dan-tf-idf#tf-idf-1","position":35},{"hierarchy":{"lvl1":"Manajemen & Berita Tf idf Dan Cbow","lvl4":"Ambil Corpus","lvl3":"TF-IDF","lvl2":"BERITA TF IDF & CBOW"},"type":"lvl4","url":"/tugas5-cbow-dan-tf-idf#ambil-corpus-1","position":36},{"hierarchy":{"lvl1":"Manajemen & Berita Tf idf Dan Cbow","lvl4":"Ambil Corpus","lvl3":"TF-IDF","lvl2":"BERITA TF IDF & CBOW"},"content":"\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Ambil corpus dari kolom stemmed\ncorpus = data['stemmed'].astype(str).tolist()\n\nprint(\"Jumlah dokumen dalam corpus:\", len(corpus))\nprint(\"Contoh dokumen:\", corpus[0][:200])\n\n\n\n# Inisialisasi TF-IDF\nvectorizer = TfidfVectorizer()\n\n# Fit dan transform\nX_tfidf = vectorizer.fit_transform(corpus)\n\nprint(\"Shape TF-IDF:\", X_tfidf.shape)  # (jumlah dokumen, jumlah kata unik)\n\n\n\n\n# Ubah ke DataFrame biar mudah dibaca\ntfidf_data = pd.DataFrame(\n    X_tfidf.toarray(),\n    columns=vectorizer.get_feature_names_out()\n)\n\n# Simpan hasil ke CSV\ntfidf_data.to_csv(\"/content/drive/MyDrive/PPW/output/tfidf_hasilberita.csv\", index=False)\nprint(\"Data berhasil disimpan ke CSV!\\n\")\n\n# Tampilkan 5 baris pertama\ntfidf_data.head()\n\n\n\n\n\n\n# Tampilkan 10 kata dengan bobot TF-IDF tertinggi di dokumen pertama\nprint(tfidf_data.iloc[0].sort_values(ascending=False).head(10))\n\n\n\n","type":"content","url":"/tugas5-cbow-dan-tf-idf#ambil-corpus-1","position":37},{"hierarchy":{"lvl1":"Manajemen & Berita Tf idf Dan Cbow","lvl3":"CBOW","lvl2":"BERITA TF IDF & CBOW"},"type":"lvl3","url":"/tugas5-cbow-dan-tf-idf#cbow-1","position":38},{"hierarchy":{"lvl1":"Manajemen & Berita Tf idf Dan Cbow","lvl3":"CBOW","lvl2":"BERITA TF IDF & CBOW"},"content":"\n\n!pip install gensim\n\n\n\n\nfrom gensim.models import Word2Vec\n\ncorpus = []\nfor col in data['stemmed']:\n    word_list = col.split(\" \")\n    corpus.append(word_list)\n\n# Tampilkan contoh dokumen pertama\nprint(corpus[0][:20])\n\n\n\n\n# Training Word2Vec\nmodel = Word2Vec(\n    corpus,\n    vector_size=56,   # ukuran embedding\n    window=5,\n    min_count=1,\n    sg=0   # CBOW\n)\n\n\n\nimport numpy as np\n\nclass MeanEmbeddingVectorizer:\n    def __init__(self, model):\n        self.model = model\n        self.vector_size = model.vector_size\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return np.array([\n            np.mean([self.model.wv[word] for word in words if word in self.model.wv]\n                    or [np.zeros(self.vector_size)], axis=0)\n            for words in X\n        ])\n\n    def fit_transform(self, X, y=None):\n        return self.fit(X).transform(X)\n\n\n\n\nmean_embedding_vectorizer = MeanEmbeddingVectorizer(model)\nmean_embedded = mean_embedding_vectorizer.fit_transform(corpus)\n\n# simpan ke DataFrame\ndata['array'] = list(mean_embedded)\n\n# cek panjang embedding\ndata['embedding_length'] = data['array'].str.len()\nprint(data[['stemmed','embedding_length']].head())\n\n\n\n\nnum_features = len(data['array'].iloc[0])  # asumsi semua list punya panjang sama\ncolumns = [f'f{i+1}' for i in range(num_features)]\n\n# Inisialisasi dictionary untuk menampung data per kolom\ndata_dict = {col: [] for col in columns}\n\n# Looping setiap baris di kolom 'embedding'\nfor embedding_list in data['array']:\n    for i, value in enumerate(embedding_list):\n        data_dict[f'f{i+1}'].append(value)\n\n# Buat DataFrame dari dictionary\nembedding_data = pd.DataFrame(data_dict)\n\nprint(embedding_data)\n\n\n\nembedding_data['isi_berita'] = data['isi_berita'].values\n\n\n\nembedding_data\n\n\n\nembedding_data.shape\n\n","type":"content","url":"/tugas5-cbow-dan-tf-idf#cbow-1","position":39},{"hierarchy":{"lvl1":"Word Graph Paper:svm"},"type":"lvl1","url":"/word-graph-svm","position":0},{"hierarchy":{"lvl1":"Word Graph Paper:svm"},"content":"pip install --upgrade pymupdf\n\n\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n\n\n%cd /content/drive/MyDrive/PPW\n\n\n\nimport pymupdf\n\ndoc = pymupdf.open(\"svm.pdf\") # open a document\nout = open(\"output.txt\", \"wb\") # create a text output\nfor page in doc: # iterate the document pages\n    text = page.get_text().encode(\"utf8\") # get plain text (is in UTF-8)\n    out.write(text) # write text of page\n    out.write(bytes((12,))) # write page delimiter (form feed 0x0C)\nout.close()\n\n\n\n%%capture\n!pip install nltk\n\n\n\nimport nltk\nnltk.download('punkt')  # hanya perlu sekali\nnltk.download('punkt_tab')  # opsional, untuk versi terbaru NLTK (â‰¥3.8.2)\n\n\n\n\n\nwith open('output.txt', 'r', encoding='utf-8') as file:\n    teks = file.read()\n\nprint(teks[:200])  # tampilkan 200 karakter pertama\n\n\n\n# Install: pip install nltk\nimport nltk\n\n\n#text = \"Ini adalah kalimat pertama. Ini kalimat kedua? Ya!\"\nsentences = nltk.sent_tokenize(teks)\nprint(sentences)\n# Output: ['Ini adalah kalimat pertama.', 'Ini kalimat kedua?', 'Ya!']\n\n\n\nimport pandas as pd\ndf = pd.DataFrame(sentences, columns=['kalimat'])\nprint(df)\n\n\n\ndf.to_csv('kalimat_svm.csv', index=False, encoding='utf-8')\n\n\n\nUntuk membuat word graph\n\nLanjutkan dengan menggunakan \n\nhttps://â€‹wwwâ€‹.geeksforgeeksâ€‹.orgâ€‹/nlpâ€‹/coâ€‹-occurenceâ€‹-matrixâ€‹-inâ€‹-nlp/\n\nimport pandas as pd\n\ndf = pd.read_csv(\"/content/drive/MyDrive/PPW/kalimat_svm.csv\")\n\nprint(df.head())      # lihat isi awal\nprint(df.columns)     # lihat nama kolom\n\n\n\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom collections import defaultdict, Counter\nimport numpy as np\nimport pandas as pd\n\n# Download NLTK resources\nnltk.download('punkt')\nnltk.download('stopwords')\n\n# Load CSV\ndf = pd.read_csv(\"/content/drive/MyDrive/PPW/kalimat_svm.csv\")\n\n# Gabungkan semua teks di kolom 'kalimat' menjadi 1 teks panjang\ntext = \" \".join(df['kalimat'].astype(str).tolist())\n\n# Preprocess the text\nstop_words = set(stopwords.words('english'))\nwords = word_tokenize(text.lower())\nwords = [word for word in words if word.isalnum() and word not in stop_words]\n\n# Define the window size for co-occurrence\nwindow_size = 2\n\n# Create a list of co-occurring word pairs\nco_occurrences = defaultdict(Counter)\nfor i, word in enumerate(words):\n    for j in range(max(0, i - window_size), min(len(words), i + window_size + 1)):\n        if i != j:\n            co_occurrences[word][words[j]] += 1\n\n# Create a list of unique words\nunique_words = list(set(words))\n\n# Initialize the co-occurrence matrix\nco_matrix = np.zeros((len(unique_words), len(unique_words)), dtype=int)\n\n# Populate the co-occurrence matrix\nword_index = {word: idx for idx, word in enumerate(unique_words)}\nfor word, neighbors in co_occurrences.items():\n    for neighbor, count in neighbors.items():\n        co_matrix[word_index[word]][word_index[neighbor]] = count\n\n# Create a DataFrame for better readability\nco_matrix_df = pd.DataFrame(co_matrix, index=unique_words, columns=unique_words)\n\n# Display the co-occurrence matrix\nco_matrix_df\n\n\n\n\n\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n# Buat graph kosong\nG = nx.Graph()\n\n# Tambahkan edge berdasarkan nilai co-occurrence\nfor word, neighbors in co_occurrences.items():\n    for neighbor, count in neighbors.items():\n        if count > 0:\n            G.add_edge(word, neighbor, weight=count)\n\n# Ukuran node berdasarkan jumlah koneksi\nnode_sizes = [G.degree(node) * 200 for node in G.nodes()]\n\nplt.figure(figsize=(14, 10))\n\n# Posisi node\npos = nx.spring_layout(G, k=0.5)\n\n# Gambar nodes dan edges\nnx.draw_networkx_nodes(G, pos, node_size=node_sizes, alpha=0.7)\nnx.draw_networkx_edges(G, pos, width=1, alpha=0.5)\nnx.draw_networkx_labels(G, pos, font_size=8)\n\nplt.title(\"Word Co-Occurrence Network Graph\")\nplt.axis(\"off\")\nplt.show()\n\n\n\n\n# Hitung jumlah kata dalam graph\njumlah_kata = G.number_of_nodes()\n\njumlah_kata\n\n\n\n\nlist(G.nodes())\n\n\n\n\njumlah_edges = G.number_of_edges()\n\njumlah_kata, jumlah_edges\n\n\n\n\n# Hitung PageRank\npagerank_scores = nx.pagerank(G, alpha=0.85)\n\n# Konversi ke DataFrame biar rapi\npagerank_df = pd.DataFrame(\n    sorted(pagerank_scores.items(), key=lambda x: x[1], reverse=True),\n    columns=[\"word\", \"pagerank\"]\n)\n\n# Tampilkan 20 kata paling penting\npagerank_df.head(20)\n\n\n\n\ntop_n = 20\ntop_words = pagerank_df.head(top_n)\n\nplt.figure(figsize=(10, 6))\nplt.barh(top_words[\"word\"], top_words[\"pagerank\"])\nplt.gca().invert_yaxis()  # Supaya ranking 1 di atas\nplt.xlabel(\"PageRank Score\")\nplt.title(f\"Top {top_n} Words by PageRank\")\nplt.show()\n\n\n\n\n%%capture\npip install networkx\n\n\n\nimport networkx as nx\narr = co_matrix_df.to_numpy()\nG=nx.from_numpy_array(arr)\n\n\n\n\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(8, 6))\nnx.draw(G, with_labels=True, node_color='lightblue', node_size=2000, font_size=12)\nplt.title(\"Graf co-occurence (Dasar)\", fontsize=14)\nplt.show()\n\n","type":"content","url":"/word-graph-svm","position":1}]}