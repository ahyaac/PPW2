['See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/220785878\nSupport Vector Machine Ensemble with Bagging\nConference Paper\xa0\xa0in\xa0\xa0Lecture Notes in Computer Science · January 2002\nDOI: 10.1007/3-540-45665-1_31\xa0·\xa0Source: DBLP\nCITATIONS\n112\nREADS\n5,212\n5 authors, including:\nS. Pang\nUnitec Institute of Technology\n69 PUBLICATIONS\xa0\xa0\xa02,075 CITATIONS\xa0\xa0\xa0\nSEE PROFILE\nHong-Mo Je\nPohang University of Science and Technology\n18 PUBLICATIONS\xa0\xa0\xa0954 CITATIONS\xa0\xa0\xa0\nSEE PROFILE\nDaijin Kim\nPohang University of Science and Technology\n301 PUBLICATIONS\xa0\xa0\xa010,656 CITATIONS\xa0\xa0\xa0\nSEE PROFILE\nAll content following this page was uploaded by Hong-Mo Je on 30 May 2014.', 'The user has requested enhancement of the downloaded file.', 'Support Vector Machine Ensemble with Bagging\nHyun-Chul Kim, Shaoning Pang, Hong-Mo Je,\nDaijin Kim, and Sung-Yang Bang\nDepartment of Computer Science and Engineering\nPohang University of Science and Technology\nSan 31, Hyoja-Dong, Nam-Gu, Pohang, 790-784, Korea\n{grass,invu71,dkim,sybang}@postech.ac.kr\nAbstract.', 'Even the support vector machine (SVM) has been proposed\nto provide a good generalization performance, the classiﬁcation result\nof the practically implemented SVM is often far from the theoretically\nexpected level because their implementations are based on the approx-\nimated algorithms due to the high complexity of time and space.', 'To\nimprove the limited classiﬁcation performance of the real SVM, we pro-\npose to use the SVM ensembles with bagging (bootstrap aggregating).', 'Each individual SVM is trained independently using the randomly chosen\ntraining samples via a bootstrap technique.', 'Then, they are aggregated\ninto to make a collective decision in several ways such as the major-\nity voting, the LSE(least squares estimation)-based weighting, and the\ndouble-layer hierarchical combining.', 'Various simulation results for the\nIRIS data classiﬁcation and the hand-written digit recognitionshow that\nthe proposed SVM ensembles with bagging outperforms a single SVM in\nterms of classiﬁcation accuracy greatly.', '1\nIntroduction\nThe support vector machine is a new and promising classiﬁcation and regression\ntechnique proposed by Vapnik and his group at AT&T Bell Laboratories [1].', 'The\nSVM learns a separating hyperplane to maximize the margin and to produce a\ngood generalization ability [2].', 'Recent theoretical research work has solved the\nexisting diﬃculties of using the SVM in practical applications [3, 4].', 'By now,\nit has been successfully applied in many areas, such as the face detection, the\nhand-writing digital character recognition, and the data mining, etc.', 'However, the SVM has two drawbacks.', 'First, since it is originally a model\nfor the binary-class classiﬁcation, we should use a combination of SVMs for the\nmulti-class classiﬁcation.', 'There are methods for combining binary classiﬁers for\nthe multi-class classiﬁcation [6, 7], but when it has been applied to SVM, the\nperformance has not seemed to improve as much as in the binary classiﬁcation.', 'Second, since learning SVM is time-consuming for a large scale of data, we\nshould use some approximate algorithms [2].', 'Using the approximate algorithms\ncan reduce the computation time, but degrade the classiﬁcation performance.', 'To overcome the above drawbacks, we propose to use the SVM ensembles.', 'We expect that the SVM ensemble can improve the classiﬁcation performance\nS.-W. Lee and A. Verri (Eds.', '): SVM 2002, LNCS 2388, pp.', '397–408, 2002.\nc\n⃝Springer-Verlag Berlin Heidelberg 2002\n\x0c398\nHyun-Chul Kim et al.', 'greatly than using a single SVM by the following fact.', 'Each individual SVM has\nbeen trained independently from the randomly chosen training samples and the\ncorrectly- classiﬁed area in the space of data samples of each SVM becomes lim-\nited to a certain area.', 'We can imagine that a combination of several SVMs will\nexpand the correctly-classiﬁed area incrementally.', 'This implies the improvement\nof classiﬁcation performance by using the SVM ensemble.', 'Likewise, we also ex-\npect that the SVM ensemble will improve the classiﬁcation performance in case\nof the multi-class classiﬁcation.', 'The idea of the SVM ensemble has been proposed in [8].', 'They used the\nboosting technique to train each individual SVM and took another SVM for\ncombining several SVMs.', 'In this paper, we propose to use the SVM ensemble\nbased on the bagging technique where each individual SVM is trained over the\nrandomly chosen training samples via a bootstrap technique and the indepen-\ndently trained several SVMs are aggregated in various ways such as the majority\nvoting, the LSE-based weighting, and the double-layer hierarchical combining.', 'This paper is organized as follows.', 'Section 2 describes the theoretical back-\nground of the SVM.', 'Section 3 describes the SVM ensembles, the bagging method\nand three diﬀerent combination methods.', 'Section 4 shows the simulation results\nwhen the proposed SVM ensemble are applied to the classiﬁcation problems such\nas the IRIS data classiﬁcation and the hand-written digit recognition.Finally, a\nconclusion is drawn.', '2\nSupport Vector Machines\nThe classical emprical risk minimization approach, which determines the classi-\nﬁcation decision function by minimizing the empirical risk as\nR = 1\nl\nL\n\x01\ni=1\n|f(xi) −yi|,\n(1)\nwhere L and f are the size of examples and the classiﬁcation decision func-\ntion, respectively.', 'For SVM, determining an optimal separating hyperplane that\ngives low generalization error is the primary concern.', 'Usually, the classiﬁcation\ndecision function in the linearly separable problem is represented by\nfw,b = sign(w · x + b).', '(2)\nIn SVM, the optimal separating hyperplane is determined by giving the largest\nmargin of separation between diﬀerent classes.', 'This optimal hyperplane bisects\nthe shortest line between the convex hulls of the two classes.', 'The optimal hy-\nperplane is required to satisfy the following constrained minimization as\nMin : 1\n2wT w,\nyi(w · xi + b) ≥1.', '(3)\n\x0cSupport Vector Machine Ensemble with Bagging\n399\nFor the linearly non-separable case, the minimization problem needs to be mod-\niﬁed to allow the misclassiﬁed data points.', 'This modiﬁcation results in a soft\nmargin classiﬁer that allows but penalizes errors by introducing a new set of\nvariables ξl\ni=1 as the measurement of violation of the constraints.', 'Min : 1\n2wT w + C(\nL\n\x01\ni=1\nξi)k,\nyi(wT ϕ(xi) + b) ≥1 −ξi,\n(4)\nwhere C and k are used to weight the penalizing variables ξi, ϕ(·) is a nonlinear\nfunction which maps the input space into a higher dimensional space.', 'Minimizing\nthe ﬁrst term in Eq.', '(4) is corresponding to minimizing the VC-dimension of\nthe learning machine and minimizing the second term in Eq.', '(4) controls the\nempirical risk.', 'Therefore, in order to solve problem Eq.', '(4), we need to construct\na set of functions, and implement the classical risk minimization on the set of\nfunctions.', 'Here, a Lagrangian method is used to solve the above problem.', 'Then,\nEq.', '(4) can be written as\nMax : F(∧) = ∧· 1 −1\n2 ∧·D∧,\n∧· y = 0; ∧≤C; ∧≥0,\n(5)\nwhere ∧= (λ1, ..., λL),D = yiyjxi · xj.', 'For the binary classiﬁcation, the decision\nfunction Eq.', '(2) can be rewritten as\nf(x) = sign(\nl\n\x01\ni=1\nyiλ∗\ni (x) + b∗)).', '(6)\nwhere λ∗\ni (x) = λiyiK(x, xi), and K(x, xi) = φ(x)φ(xi).', '( K(x, xi) can be sim-\nplifed by the kernel trick [9]. )', 'For the multi-class classiﬁcation, we can extend the SVM in the following two\nways.', 'One method is called the “one-against-all” method [6], where we have as\nmany SVMs as the number of classes.', 'The ith SVM is trained from the training\nsamples where some examples contained in the ith class have ”+1” labels, and\nother examples contained in the other classes have ”-1” labels.', 'Then, Eq.', '(4) is\nmodiﬁed into\nMin : F(∧) = 1\n2(wi)T wi + C(\nL\n\x01\nt=1\n(ξi)t)k\nyi((wi)T ϕ(xt) + bi) ≥1 −(ξi)t, if xi ∈Ci,\nyi((wi)T ϕ(xt) + bi) ≤1 −(ξi)t, if xi ∈¯Ci,\n(ξi)t > 0, i = 1, ..., L,\n(7)\n\x0c400\nHyun-Chul Kim et al.', 'where Ci is the set of data of class i.', 'The decision function of (7) becomes\nf(x) = sign( Max\nj∈1,2,...,C((wj)T · ϕ(xj) + bj)),\n(8)\nwhere C is the number of the classes.', 'Another method is called the one-against-one method [7].', 'When the number\nof classes is C, this method constructs C(C −1)/2 SVM classiﬁers.', 'The ijth\nSVM is trained from the training samples where some examples contained in\nthe ith class have ”+1” labels and other examples contained in the jth class\nhave ”-1” labels.', 'Then, Eq.', '(4) is modiﬁed into\nMin : F(∧) = 1\n2(wij)T wij + C(\nL\n\x01\nt=1\n(ξij)t)k\nyt((wij)T ϕ(xt) + bij) ≥1 −(ξij)t, if xt ∈Ci,\nyt((wij)T ϕ(xt) + bij) ≤1 −(ξij)t, if xt ∈Cj,\n(ξij)t > 0, t = 1, ..., L.\n(9)\nThe class decision in this type of multi-class classiﬁer can be performed in the\nfollowing two ways.', 'The ﬁrst decision is based on the “Max Wins” voting strat-\negy, in which C(C −1)/2 binary SVM classiﬁers will vote for each class, and the\nwinner class having the maximum votes is the last classiﬁcation decision.', 'The\nsecond method uses the tournament match, which reduces the classiﬁcation time\nto the log scale.', '3\nSupport Vector Machine Ensemble\nAn ensemble of classiﬁers is a collection of several classiﬁers whose individual\ndecisions are combined in some way to classify the test examples [10].', 'It is known\nthat an ensemble often shows much better performance than the individual clas-\nsiﬁers that make it up.', 'Hansen et.', 'al.', '[11] shows why the ensemble shows better\nperformance than individual classiﬁers as follows.', 'Assume that there are an en-\nsemble of n classiﬁers: {f1, f2, .', '.', '.', ', fn} and consider a test data x.', 'If all the classi-\nﬁers are identical, they are wrong at the same data, where an ensemble will show\nthe same performance as individual classiﬁers.', 'However, if classiﬁers are diﬀerent\nand their errors are uncorrelated, then when fi(x) is wrong, most of other classi-\nﬁers except for fi(x) may be correct.', 'Then, the result of majority voting can be\ncorrect.', 'More precisely, if the error of individual classiﬁer is p < 1/2 and the er-\nrors are independent, then the probability pE that the result of majority voting is\nincorrect is \x02n\nk=⌈n/2⌉pk(1−p)(n−k) (< \x02n\nk=⌈n/2⌉( 1\n2)k( 1\n2)(n−k) = \x02n\nk=⌈n/2⌉( 1\n2)n).', 'When the size of classiﬁers n is large, the probability pE becomes very small.', 'The SVM has been known to show a good generalization performance and\nis easy to learn exact parameters for the global optimum[2].', 'Because of these\n\x0cSupport Vector Machine Ensemble with Bagging\n401\nadvantages, their ensemble may not be considered as a method for improving\nthe classiﬁcation performance greatly.', 'However, since the practical SVM has\nbeen implemented using the approximated algorithms in order to reduce the\ncomputation complexity of time and space, a single SVM may not learn exact\nparameters for the global optimum.', 'Sometimes, the support vectors obtained\nfrom the learning is not suﬃcient to classify all unknown test examples com-\npletely.', 'So, we can not guarantee that a single SVM always provides the global\noptimal classiﬁcation performance over all test examples.', 'To overcome this limitation, we propose to use an ensemble of support vec-\ntor machines.', 'Similar arguments mentioned above about the general ensemble\nof classiﬁers can also be applied to the ensemble of support vector machines.', 'Figure 1 shows a general architecture of the proposed SVM ensemble.', 'During\nthe training phase, each individual SVM is trained independently by its own\nreplicated training data set via a bootstrap method explained in the Section\n3.1.', 'All constituent SVMs will be aggregated by various combination strategies\nexplained in the Section 3.2.', 'During the testing phase, a test example is applied\nto all SVMs simultaneously and a collective decision is obtained based on the\naggregation strategy.', 'On the other hand, the advantage of using the SVM ensemble over a single\nSVM can be achieved equally in the case of multi-class classiﬁcation.', 'Since the\nSVM is originally a binary classiﬁer, many SVMs should be combined for the\nmulti-class classiﬁcation as mentioned in 2.2.', 'The SVM classiﬁer for the multi-\nFig.', '1.', 'A general architecture of the SVM ensemble\n\x0c402\nHyun-Chul Kim et al.', 'class classiﬁcation does not show as good performance as that for the binary-\nclass classiﬁcation.', 'So, we can also improve the classiﬁcation performance in the\nmulti-class classiﬁcation by taking the SVM ensemble where each SVM classiﬁer\nis designed for the multi-class classiﬁcation.', '3.1\nConstructing the SVM Ensembles Using Bagging\nIn this work, we adopt a bagging technique [12] to construct the SVM ensemble.', 'In bagging, several SVMs are trained independently via a bootstrap method and\nthen they are aggregated via an appropriate combination technique.', 'Usually, we have a single training set Usually, we have a single training set\nT R = {(xi; yi)|i = 1, 2, .', '.', '.', ', l}.', 'But we need K training samples sets to construct\nthe SVM ensemble with K independent SVMs.', 'From the statistical fact, we need\nto make the training sample sets diﬀerent as much as possible in order to obtain\nhigher improvement of the aggregation result.', 'For doing this, we often use the\nbootstrap technique as follows.', 'Bootstrapping builds K replicate training data sets {T RB\nk |k = 1, 2, .', '.', '., K}\nby randomly resampling, but with replacement, from the given training data\nset T R repeatedly.', 'Each example xi in the given training set T R may appear\nrepeated times or not at all in any particular replicate training data set.', 'Each\nreplicate training set will be used to train a certain SVM.', '3.2\nAggregating Support Vector Machines\nAfter training, we need to aggregate several independently trained SVMs in an\nappropriate combination manner.', 'We consider two types of combination tech-\nniques such as the linear and nonlinear combination method.', 'The linear combi-\nnation method, as a linear combination of several SVMs, includes the majority\nvoting and the LSE-based weighting.', 'The majority voting and the LSE-based\nweighting are often used for the bagging and the boosting, respectively.', 'A non-\nlinear method, as a nonlinear combination of several SVMs, includes the double-\nlayer hierarchical combining that use another upper-layer SVM to combine sev-\neral lower-layer SVMs.', 'Majority Voting Majority voting is the simplest method for combining several\nSVMs.', 'Let fk(k = 1, 2, .', '.', '.', ', K) be a decision function of the kth SVM in the\nSVM ensemble and Cj(j = 1, 2, .', '.', '., C) denote a label of the j-th class.', 'Then,\nlet Nj = #{k|fk(x) = Cj}, i.e.', 'the number of SVMs whose decisions are known\nto the jth class.', 'Then, the ﬁnal decision of the SVM ensemble fmv(x) for a given\ntest vector x due to the majority voting is determined by\nfmv(x) = argmax\nj\nNj.', '(10)\n\x0cSupport Vector Machine Ensemble with Bagging\n403\nThe LSE-Based Weighting The LSE-based weighting treats several SVMs\nin the SVM ensemble with diﬀerent weights.', 'Often, the weights of several SVMs\nare determined in proportional to their accuracies of classiﬁcations [13].', 'Here,\nwe propose to learn the weights using the LSE method as follows.', 'Let fk(k = 1, 2, .', '.', '., K) be a decision function of the kth SVM in the SVM\nensemble that is trained by a replicate training data set T hauB\nk = {(x′\ni; y′\ni)|i =\n1, 2, .', '.', '.', ', L}.', 'The weight vector w can be obtained by wE = A−1y, where\nA = (fi(xj))K×L, and y = (yj)1×L.', 'Then, the ﬁnal decision of the SVM ensem-\nble fmv(x) for a given test vector x due to the LSE-based weighting is determined\nby\nfLSE(x) = sign(w · [(fi(x))K×1])\n(11)\nThe Double-Layer Hierarchical Combining We can use another SVM to\naggregate the outputs of several SVMs in the SVM ensemble.', 'So, this combi-\nnation consists of a double-layer of SVMs hierarchically where the outputs of\nseveral SVMs in the lower layer feed into a super SVM in the upper layer.', 'This\ntype of combination looks similar of the mixture of experts introduced by M.\nJordan et.', 'al.', '[14].', 'Let fk(k = 1, 2, .', '.', '., K) be a decision function of the kth SVM in the SVM\nensemble and F be a decision function of the super SVM in the upper layer.', 'Then, the ﬁnal decision of the SVM ensemble fSV M(x) for a given test vector x\ndue to the double-layer hierarchical combining is determined by\nfSV M(x) = F((f1(x), f2(x), .', '.', '.', ', fK(x))),\n(12)\nwhere K is the number of SVMs in the SVM ensemble.', '3.3\nExtension of the SVM Ensemlbe to the Multi-class\nClassiﬁcation\nIn section 2, we explained two kinds of extension methods such as ”one-against-\nall and one-against-one methods” in order to apply the SVM to the multi-class\nclassiﬁcation problem.', 'We can use these extension methods equally in the case of\nthe SVM ensemble for the multi-class classiﬁcation.', 'For the C-class classiﬁcation\nproblem, we can have two types of extensions according to the insertion level\nof the SVM ensemble: (1) the binary-classiﬁer-level SVM ensemble and (2) the\nmulti-classiﬁer-level SVM ensemble.', 'The binary-classiﬁer-level SVM ensemble consists of C SVM ensembles in the\ncase of “one-against-all” method or C(C −1)/2 SVM ensembles in the case of\n“one-against-one” method.', 'And, each SVM ensemble consists of K independent\nSVMs.', 'So, the SVM ensemble is built in the level of binary classiﬁers.', 'We obtain\nthe ﬁnal decision from the decision results of many SVM ensembles via either\nthe “Max Wins” voting strategy or the tournament match.', 'The multi-classiﬁer-level SVM ensemble consists of K multi-class classiﬁers.', 'And each multi-class classiﬁer consists of C binary classiﬁers in the case of\n\x0c404\nHyun-Chul Kim et al.', '“one-against-all” method or for C(C −1)/2 binary classiﬁers in the case of “one-\nagainst-one” method.', 'So, the SVM ensemble is built in the level of multi-class\nclassiﬁers.', 'We obtain the ﬁnal decision from the decision results of many multi-\nclass classiﬁers via an appropriate aggregating strategy of the SVM ensemble.', '4\nSimulation Results\nTo evaluate the eﬃcacy of the proposed SVM ensemble using the bagging tech-\nnique, we have performed three diﬀerent classiﬁcation problems such as the IRIS\ndata classiﬁcation, the UCI hand-written digit recognition.We used four diﬀerent\nclassiﬁcation methods such as a single SVM, and three diﬀerent SVM ensem-\nbles with diﬀerent aggregating strategies like the majority voting, the LSE-based\nweighting, and the double-layer hierarchical combining.', '4.1\nIRIS Data Classiﬁcation\nThe IRIS data set[15, 16] is one of the best known databases to be found in\nthe pattern recognition literature.', 'The IRIS data set contains 3 classes where\neach class consists of 50 instances.', 'Each class refers to a type of IRIS planet.', 'One class is linearly separable from the other classes but they are not linearly\nseparable from each other.', 'We used the one-against-one method for the multi-class extension and took\nthe binary-classiﬁer-level SVM ensemble for the classiﬁcation.', 'So, for the 3-class\nclassiﬁcation problem and one-against-one extension method, there are three\nbinary-classiﬁer-level\nSVM\nensembles\n(SV MC1,C1, SV MC1,C2, SV MC1,C3)\nwhere each SVM ensemble consists of 5 independent SVMs.', 'The ﬁnal decision\nare obtained from the decision results of three SVM ensembles via a tournament\nmatching scheme.', 'Each SVM used 2-d polynomial kernel function.', 'We selected randomly 90 data samples for the training set.', 'For bootstrap-\nping, we re-sampled randomly 60 data samples with replacement from the train-\ning data set.', 'We trained each SVM independently over the replicated training\ndata set and aggregated several trained SVMs via three diﬀerent combination\nmethods.', 'We test four diﬀerent classiﬁcation methods using the test data set con-\nsisting of 60 IRIS data.', 'Table 1 shows the classiﬁcation results of four diﬀerent\nclassiﬁcation methods for the IRIS data classiﬁcation.', 'Table 1.', 'The classiﬁcation results of IRIS data classiﬁcation\nMethod\nClassiﬁcation rate\nSingle SVM\n96.73%\nMajoirity voting\n98.0%\nLSE-based weighting\n98.66%\nHierarchical SVM\n98.66%\n\x0cSupport Vector Machine Ensemble with Bagging\n405\n4.2\nUCI Hand-Written Digit Recognition\nWe used the UCI hand-written digit data [16].', 'Some digits in the database are\nshown in Figure 2.', 'Among the UCI hand-written digits, we chose randomly 3,828\ndigits as a training data set and the remaining 1,797 digits as a test data set.', 'The original image of each digit has the size of 32 × 32 pixels.', 'It is reduced to\nthe size of 8×8 pixels where each pixel is obtained from the average of the block\nof 4 × 4 pixels in the original image.', 'So each digit is represented by a feature\nvector with the size of 64 × 1.', 'Fig.', '2.', 'Examples of hand-written digits in UCI database\nWe used one-against-one methods.', 'We constructed a SVM ensemble, con-\nsiting of 11 SVMs, for one-against-one classiﬁcation, and used the tournament\nscheme for decision.', 'Each SVM used 2-d polynomial kernel function.', 'For bag-\nging, we sampled 2000 data randomly for an individual SVM.', 'We trained each\nSVM and combined three methods, such as unweighted voting, weighted voting\n(using LSE), and a combining SVM.', 'We applied a single SVM for the compar-\nison.', 'Table 2 shows the performance of a single SVM and bagging SVMs with\nthree combining methods for the IRIS data classiﬁcation.', 'We used the one-against-one method for the multi-class extension and took\nthe binary-classiﬁer-level SVM ensemble for the classiﬁcation.', 'So, for the 10-\nclass classiﬁcation problem and one-against-one extension method, there are 45\nbinary-classiﬁer-level SVM ensembles (SV MC0,C1, SV MC0,C2, .', '.', '.', ', SV MC8,C9)\nwhere each SVM ensemble consists of 11 independent SVMs.', 'The ﬁnal decision\nare obtained from the decision results of three SVM ensembles via a tournament\nmatching scheme.', 'Each SVM used 2-d polynomial kernel function.', 'For bootstrapping, we re-sampled randomly 2,000 digits samples with re-\nplacement from the training data set consisting of 3,828 digit samples.', 'We trained\neach SVM independently over the replicated training data set and aggregated\nseveral trained SVMs via three diﬀerent combination methods.', 'We test four dif-\nferent classiﬁcation methods using the test data set consisting of 1,797 digits.', '406\nHyun-Chul Kim et al.', 'Table 2 shows the classiﬁcation results of four diﬀerent classiﬁcation methods\nfor the hand-written digit data classiﬁcation.', 'Table 2.', 'The classiﬁcation results of UCI hand-written digit recognition\nMethod\nClassiﬁcation rate\nSingle SVM\n96.99%\nMajority voting\n97.55%\nLSE-based weighting\n97.82%\nHierarchical SVM\n98.01%\n5\nConclusion\nUsually, the practical SVM has been implemented based on the approximation\nalgorithm to reduce the cost of time and space.', 'So, the obtained classiﬁcation\nperformance is far from the theoretically expected level of it.', 'To overcome this\nlimitation, we addressed the SVM ensemble that consists of several indepen-\ndently trained SVMs.', 'For training each SVM, we generated many replicated\ntraining sample sets via the bootstrapping technique.', 'Then, all independently\ntrained SVMs over the replicated training sample sets were aggregated by three\ncombination techniques such as the majority voting, the LSE-based weighting,\nand the double-layer hierarchical combining.', 'We also extended the SVM ensemble to the multi-class classiﬁcation problem:\nthe binary-classiﬁer-level SVM ensemble and the multi-classiﬁer-level SVM en-\nsemble.', 'The former did build the SVM ensemble in the level of binary classiﬁers\nand the latter did build the SVM ensemble in the level of multi-class classiﬁers.', 'The former had C SVM ensembles in the case of “one-against-all” method or\nC(C −1)/2 SVM ensembles in the case of “one-against-one” method.', 'And, each\nSVM ensemble consisted of K independent SVMs.', 'The latter consisted of K\nmulti-class classiﬁers.', 'And each multi-class classiﬁer consists of C binary classi-\nﬁers in the case of “one-against-all” method or for C(C −1)/2 binary classiﬁers\nin the case of “one-against-one” method.', 'We evaluated the classiﬁcation performance of the proposed SVM ensemble\nover three diﬀerent multi-class classiﬁcation problems such as the IRIS data clas-\nsiﬁcation, the hand-written digit recognition.', 'The SVM ensembles outperform\na single SVM for all applications in terms of classiﬁcation accuracy.', 'For three\ndiﬀerent aggregation methods, the classiﬁcation performance is superior in the\norder of the double-layer hierarchical combining, the LSE-based weighting, and\nthe majority voting.', 'In the future, we will consider other aggregation scheme\nlike boosting for constructing the SVM ensemble.', 'Support Vector Machine Ensemble with Bagging\n407\nAcknowledgements\nThe authors would like to thank the Ministry of Education of Korea for its ﬁ-\nnancial support toward the Electrical and Computer Engineering Division at\nPOSTECH through its BK21 program.', 'This research was also partially sup-\nported by the Brain Science and Engineering Research Program of the Ministry\nof Science and Technology, Korea, and by the Basic Research Institute Support\nProgram of the Korea Research Foundation.', 'References\n[1] Cortes, C., Vapnik, V.: Support vector network.', 'Machine Learning.', '20 (1995)\n273–297\n397\n[2] Burges, C.: A Tutorial on Support Vector Machines for Pattern Recognition.', 'Data\nMining and Knowledge Discovery.', '2(2) (1998) 121–167\n397, 400\n[3] Joachims, T.: Making large-scale support vector machine learning practical.', 'Ad-\nvances in Kernel Methods: Support Vector Machines, MIT Press, Cambridge, MA\n(1999)\n397\n[4] Platt, J.: Fast training of support vector machines using sequential minimal op-\ntimization.', 'Advances in Kernel Methods: Support Vector Machines, MIT Press,\nCambridge, MA (1999)\n397\n[5] Weston, J., Watkins, C.: Support Vector Machines for Multi-Class Pattern Recog-\nnition.', 'Proceedings of the 7th European Symposium on Artiﬁcial Neural Networks\n(1999)\n[6] Bottou, L., Cortes, C., Denker, J., Drucker, H., Guyon, I., Jackel, Lawrence D.,\nLeCun, Y., M¨uller U., S¨ackinger E., Simard, P., Vapnik, V.: Comparison of clas-\nsiﬁer methods: a case study in handwriting digit recognition.', 'Proceedings of the\n13th International Conference on Pattern Recognition.', 'IEEE Computer Society\nPress (1994) 77–87\n397, 399\n[7] Knerrm, S., Personnaz, L., Dreyfus, G.: Single-layer learning revisited: a stepwise\nprocedure for building and training a neural network.', 'In: Fogelman, J.(eds.', '): Neu-\nrocomputing: Algorithms, Architechtures and Application, Springer-Verlag (1990)\n397, 400\n[8] Vapnik, V.: The Nature of Statistical Learning Theory.', 'Springer, New York, 1999\n398\n[9] Sch¨olkopf, B., Smola A., and Muller K.: Nonlinear component analysis as a kernel\neigenvalue problem.', 'Neural Computation, 10(5) (1998) 1299-1319\n399\n[10] Dietterich, T.: Machine Learning Research: Four Current Directions.', 'The AI Mag-\nazine, 18(4) (1998) 97–136\n400\n[11] Hansen, L., Salamon, P.: Neural network ensembles.', 'IEEE Transactions on Pat-\ntern Analysis and Machine Intelligence, 12 (1990) 993–1001\n400\n[12] Breiman, L.: Bagging predictors.', 'Machine Learning, 24(2) (1996) 123–140, 1996\n402\n[13] Kim, D., Kim, C.: Forecasting time series with genetic fuzzy predictor ensemble.', 'IEEE Transaction on Fuzzy Systems, 5(4) (1997) 523–535\n403\n[14] Jordan, M., Jacobs, R., Hierarchical mixtures of experts and the EM algorithm.', 'Neural Computation 6(5) (1994) 181–214\n403\n[15] Fisher, R.: The use of multiple measurements in taxonomic problems.', 'Annual\nEugenics, 7, Part II (1936) 179–188\n404\n\x0c408\nHyun-Chul Kim et al.', '[16] Bay, B.: The UCI KDD Archive [http://kdd.ics.uci.edu].', 'Irvine, CA: University\nof California, Department of Information and Computer Science (1999) 404, 405\nView publication stats']
